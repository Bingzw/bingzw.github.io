<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Bingz Learning Blog</title>
    <link>/posts/</link>
    <description>Bingz Learning Blog (Posts)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <managingEditor>bingwang8878@gamil.com (Bing Wang)</managingEditor>
    <webMaster>bingwang8878@gamil.com (Bing Wang)</webMaster>
    <lastBuildDate>Mon, 15 Jul 2024 21:40:55 -0700</lastBuildDate>
    
    <atom:link href="/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>A Journey to Reinforcement Learning - Deep RL</title>
      <link>/posts/2024-07-15-rf-learning-deep/</link>
      <pubDate>Mon, 15 Jul 2024 21:40:55 -0700</pubDate>
      <author>bingwang8878@gamil.com (Bing Wang)</author>
      <guid>/posts/2024-07-15-rf-learning-deep/</guid>
      <description>&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://bingzw.github.io/rf/deeprl.png&#34; width=&#34;600&#34; height=&#34;400&#34;&gt;&lt;br&gt;
&lt;p&gt;
&lt;p&gt;&lt;em&gt;Image cited from &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This blog is a continuation of the last one: &lt;strong&gt;A Journey to Reinforcement Learning - Tabular Methods&lt;/strong&gt;. The
table of content structure and notations follow the same framework. Let&amp;rsquo;s continue the journey from the last value based
algorithm in model free setting.&lt;/p&gt;
&lt;h2 id=&#34;model-free---when-the-environment-is-unknown&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#model-free---when-the-environment-is-unknown&#34;&gt;
        
    &lt;/a&gt;
    Model Free - when the environment is unknown
&lt;/div&gt;
&lt;/h2&gt;
&lt;h3 id=&#34;value-based-continuing-&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#value-based-continuing-&#34;&gt;
        
    &lt;/a&gt;
    Value Based (Continuing &amp;hellip;)
&lt;/div&gt;
&lt;/h3&gt;
&lt;h4 id=&#34;deep-q-network-dqn&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#deep-q-network-dqn&#34;&gt;
        
    &lt;/a&gt;
    Deep Q Network (DQN)
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;Like what have been discussed in SARSA and $Q$ learning, they may not be a good fit when state or action space is too large
or even continuous. Maintaining a $Q$ table is not feasible in such cases. Instead, we can apply function approximation to
the $Q$ value. So far, the model powerful approach that can approximate almost any shape of function is &lt;strong&gt;Neural Network&lt;/strong&gt;.
Welcome to the realm of &lt;strong&gt;deep reinforcement learning&lt;/strong&gt; (short for &amp;ldquo;deep learning&amp;rdquo; + &amp;ldquo;reinforcement learning&amp;rdquo;).&lt;/p&gt;
&lt;p&gt;The idea seems to be natural. Let&amp;rsquo;s refine the DQN framework step by step. As an extension from $Q$ learning. Let&amp;rsquo;s frame the
problem with &lt;strong&gt;continuous states&lt;/strong&gt; and &lt;strong&gt;discrete actions&lt;/strong&gt; (the continuous actions case will be discussed later). The $Q$ value can
be approximated by fitting a neural network where the state is fed into the network and get output of $Q$ value at each action.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://bingzw.github.io/rf/q_dqn.png&#34; width=&#34;600&#34; height=&#34;400&#34;&gt;&lt;br&gt;
&lt;em&gt;Figure 1: Q learning vs DQN&lt;/em&gt;
&lt;p&gt;
&lt;p&gt;&lt;em&gt;Image cited from &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Given the network, what&amp;rsquo;s the loss function? From the basic $Q$ learning, the $Q$ value is updated
via $Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a&amp;rsquo;}Q(s&amp;rsquo;, a&amp;rsquo;) - Q(s, a)]$. This indicates that we are actually
&amp;ldquo;learning&amp;rdquo; the target $r + \gamma \max_{a&amp;rsquo;}Q(s&amp;rsquo;, a&amp;rsquo;)$. Therefore, the loss function can be defined as
$$
L(\theta, x) = \frac{1}{N}\sum_{i=1}^N[Q_{\theta}(s_i, a_i) - (r_i + \arg \max_{a&amp;rsquo;}Q_{\theta}(s&amp;rsquo;, a&amp;rsquo;))]^2 \tag{3.4}
$$
given a sample $x = (s_i, a_i, r_i, s&amp;rsquo;_i)$.&lt;/p&gt;
&lt;p&gt;When applying the backpropagation on (3.4), the target value is changing since it also depends on the parameter $\theta$
to be optimized. This differs from the typical deep learning where the label is given and fixed. Learning from a changing label
problem may be very unstable and loss may fluctuate. To tackle this, the target $Q$ network is introduced.&lt;/p&gt;
&lt;p&gt;The idea is to maintain two $Q$ neural nets: $Q_\theta$ and $Q_{\theta ^ *}$. The target net $Q_{\theta ^ *}$ is originally cloned from
the net $Q_\theta$ and the (3.4) is reformatted as&lt;/p&gt;
&lt;p&gt;$$
L(\theta, x) = \frac{1}{N}\sum_{i=1}^N[Q_{\theta}(s_i, a_i) - (r_i + \arg \max_{a&amp;rsquo;}Q_{\theta ^ *}(s&amp;rsquo;, a&amp;rsquo;))]^2 \tag{3.5}
$$&lt;/p&gt;
&lt;p&gt;The parameter $\theta ^ *$ is fixed in (3.5) and only copied from $Q_\theta$ periodically after a few gradient descent runs. In
this way, the loss function is firstly optimized towards a fixed label (only $\theta$ is updated), then labels are updated (copy
$\theta$ to $\theta ^ *$) and optimization continues.&lt;/p&gt;
&lt;p&gt;Gathering the training samples purely from interacting with the system results in dependent samples due to the Markov property. In DQN,
An &lt;strong&gt;experience replay&lt;/strong&gt; (ER) component introduce a &lt;em&gt;replay buffer&lt;/em&gt; that is sampling independent samples $x = (s_i, a_i, r_i, s&amp;rsquo;_i)$,
thus also improving the training efficiency.&lt;/p&gt;
&lt;p&gt;A general structure of DQN is&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://bingzw.github.io/rf/dqn_flow.png&#34; width=&#34;600&#34; height=&#34;400&#34;&gt;&lt;br&gt;
&lt;em&gt;Figure 2: DQN flow&lt;/em&gt;
&lt;p&gt;
&lt;p&gt;&lt;em&gt;Image cited from &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initialize the $Q_{\theta}$&lt;/li&gt;
&lt;li&gt;Initialize the target network $Q_{\theta ^ *}$ by cloning $\theta ^ *$ from $\theta$&lt;/li&gt;
&lt;li&gt;Initialize the replay buffer&lt;/li&gt;
&lt;li&gt;for episode $e$ from 1 to $K$, do:
&lt;ul&gt;
&lt;li&gt;Initialize the state $s$&lt;/li&gt;
&lt;li&gt;for $t$ from 1 to $T$, do:
&lt;ul&gt;
&lt;li&gt;apply the $\epsilon$-greedy strategy to $Q_{\theta}$ to choose the action $a$ based on $s$&lt;/li&gt;
&lt;li&gt;take the action and interact with the environment to get reward $r$ and $s&#39;$&lt;/li&gt;
&lt;li&gt;save the sample (s, a, r, s&amp;rsquo;) to the replay buffer&lt;/li&gt;
&lt;li&gt;if there are enough samples in the replay buffer, sample N data points $\{(s_i, s_i, r_i, s_{i+1})\}_{i=1,\dots,N}$&lt;/li&gt;
&lt;li&gt;for each sample, calculate the target $y_i = r_i + \gamma \arg\max_{a} Q_{\theta ^ *}(s_{i+1}, a)$&lt;/li&gt;
&lt;li&gt;minimize the loss function $L(\theta, x) = \frac{1}{N}\sum_{i=1}^N[y_i - Q_{\theta}(s_i, a_i)]^2$, update $Q_\theta$&lt;/li&gt;
&lt;li&gt;Update $Q_{\theta ^ *}$ every $M$ steps&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;end for&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;end for&lt;/li&gt;
&lt;li&gt;return $Q_{\theta}$ and derive the optimal policy&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;advanced-dqn&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#advanced-dqn&#34;&gt;
        
    &lt;/a&gt;
    Advanced DQN
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;Though DQN extends the problem solving capability from finite states to infinite state space, original DQN still suffers
from issues like overestimation. Below lists the efforts to further refine the DQN.&lt;/p&gt;
&lt;h5 id=&#34;double-dqn&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#double-dqn&#34;&gt;
        
    &lt;/a&gt;
    Double DQN
&lt;/div&gt;
&lt;/h5&gt;
&lt;p&gt;&lt;em&gt;Problem Addressed&lt;/em&gt;: DQN suffers from overestimation bias when updating Q-values because it uses the same network to
select and evaluate actions.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Approach&lt;/em&gt;: Double DQN separates action selection and action evaluation by using the training network to select actions and
the target network to evaluate them. This reduces overestimation bias and leads to more accurate Q-value predictions.&lt;/p&gt;
&lt;h5 id=&#34;duel-dqn&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#duel-dqn&#34;&gt;
        
    &lt;/a&gt;
    Duel DQN
&lt;/div&gt;
&lt;/h5&gt;
&lt;p&gt;&lt;em&gt;Problem Addressed&lt;/em&gt;: Traditional DQNs struggle to efficiently learn the value of states without requiring specific
action-value pairs.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Approach&lt;/em&gt;: Introduces a dueling network architecture that separates the estimation of state values and advantage functions.
This allows the model to learn which states are valuable without needing to learn the effect of each action in those states,
leading to improved learning efficiency.&lt;/p&gt;
&lt;h5 id=&#34;prioritized-experience-replay&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#prioritized-experience-replay&#34;&gt;
        
    &lt;/a&gt;
    Prioritized Experience Replay
&lt;/div&gt;
&lt;/h5&gt;
&lt;p&gt;&lt;em&gt;Problem Addressed&lt;/em&gt;: In standard experience replay, all transitions are sampled with equal probability, which can be
inefficient as some experiences are more valuable for learning.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Approach&lt;/em&gt;: Prioritizes sampling of experiences that have higher expected learning progress, meaning transitions with
a higher TD error are sampled more frequently. This focuses learning on more informative samples, speeding up
the learning process.&lt;/p&gt;
&lt;h5 id=&#34;noisy-nets&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#noisy-nets&#34;&gt;
        
    &lt;/a&gt;
    Noisy Nets
&lt;/div&gt;
&lt;/h5&gt;
&lt;p&gt;&lt;em&gt;Problem Addressed&lt;/em&gt;: Standard exploration methods (like epsilon-greedy) can be suboptimal or inefficient.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Approach&lt;/em&gt;: Replaces deterministic parameters in the neural network with parameterized noise, allowing the network to explore more
effectively by adding noise directly to the weights. This leads to more effective exploration strategies that adapt over time.&lt;/p&gt;
&lt;h5 id=&#34;distributional-dqn&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#distributional-dqn&#34;&gt;
        
    &lt;/a&gt;
    Distributional DQN
&lt;/div&gt;
&lt;/h5&gt;
&lt;p&gt;&lt;em&gt;Problem Addressed&lt;/em&gt;: Traditional DQNs focus on learning the expected return, which can overlook valuable distributional
information about the returns.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Approach&lt;/em&gt;: Instead of estimating the expected value, estimate the distribution of the $Q$ function. This is to capture
the uncertainty of the $Q$ function. The $Q$ function is then calculated as the expected value of the distribution.&lt;/p&gt;
&lt;h5 id=&#34;rainbow-dqn-4&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#rainbow-dqn-4&#34;&gt;
        
    &lt;/a&gt;
    Rainbow DQN &lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h5&gt;
&lt;p&gt;&lt;em&gt;Problem Addressed&lt;/em&gt;: Individual improvements to DQN (like those above) each address specific limitations, but combining
them could lead to even more robust performance.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Approach&lt;/em&gt;: Integrates several enhancements to DQN into a single framework: Double DQN, Duel DQN, Prioritized Experience Replay,
Noisy Nets, Distributional Q-learning, and a few others. By combining these techniques, Rainbow DQN leverages the strengths of
each approach to achieve superior performance in a unified model.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://bingzw.github.io/rf/rainbow.png&#34; width=&#34;500&#34; height=&#34;100&#34;&gt;&lt;br&gt;
&lt;em&gt;Figure 3: Rainbow DQN&lt;/em&gt;
&lt;p&gt;
&lt;p&gt;&lt;em&gt;Image cited from &lt;sup id=&#34;fnref1:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;policy-based&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#policy-based&#34;&gt;
        
    &lt;/a&gt;
    Policy Based
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;Value-based methods (like Q-learning and DQN) focus on estimating the value of state-action pairs to derive a policy.
However, these methods can struggle with high-dimensional or continuous &lt;strong&gt;action spaces&lt;/strong&gt;. Policy-based methods address
these limitations by directly learning the policy itself. In policy based approaches, policies are often represented
using parameterized functions, such as neural networks. These functions map states directly to probabilities over actions,
allowing the agent to choose actions based on the learned distribution.&lt;/p&gt;
&lt;h4 id=&#34;policy-gradient&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#policy-gradient&#34;&gt;
        
    &lt;/a&gt;
    Policy Gradient
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;Suppose $\pi_{\theta}$ is the parameterized policy that is to be optimized, our objective is to maximize the expected
cumulative reward from any starting state&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
$$
J(\theta) = \mathbb{E}_{s_0}[V_{\pi_{\theta}}(s_0)] \tag{4.1}
$$
&lt;/p&gt;
&lt;p&gt;where the expectation is taken over state. Let&amp;rsquo;s derive the derivative of (4.1).&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
\begin{aligned}
\nabla_{\theta} V_{\pi_{\theta}}(s) &amp;= \nabla_{\theta}  \sum_{a \in A} \pi_{\theta}(a \mid s)Q_{\pi_{\theta}}(s, a)\\
&amp;= \sum_{a \in A} (\nabla_{\theta} \pi_{\theta}(a \mid s)Q_{\pi_{\theta}}(s, a) + \pi_{\theta}(a \mid s) \nabla_{\theta} Q_{\pi_{\theta}}(s, a)) \\
&amp;= \sum_{a \in A} (\nabla_{\theta} \pi_{\theta}(a \mid s)Q_{\pi_{\theta}}(s, a) + \pi_{\theta}(a \mid s) \nabla_{\theta} \sum_{s&#39;, r} p(s&#39;, r \mid s, a)(r + \gamma V_{\pi_{\theta}}(s&#39;)) \\
&amp;= \sum_{a \in A} (\nabla_{\theta} \pi_{\theta}(a \mid s)Q_{\pi_{\theta}}(s, a) + \gamma\pi_{\theta}(a \mid s) \sum_{s&#39;, r} p(s&#39;, r \mid s, a)\nabla_{\theta} V_{\pi_{\theta}}(s&#39;) \\
&amp;= \sum_{a \in A} (\nabla_{\theta} \pi_{\theta}(a \mid s)Q_{\pi_{\theta}}(s, a) + \gamma\pi_{\theta}(a \mid s) \sum_{s&#39;} p(s&#39; \mid s, a)\nabla_{\theta} V_{\pi_{\theta}}(s&#39;) \hspace{2.5em} \text{(4.2)}
\end{aligned}
&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s define $\phi(s) = \sum_{a \in A} \nabla_{\theta} \pi_{\theta}(a \mid s)Q_{\pi_{\theta}}(s, a)$ and denote the probability
of getting state $s&amp;rsquo;$ after $k$ steps from state $s$ under the policy $\pi_{\theta}$ as $d_{\pi_{\theta}}(s \rightarrow s&amp;rsquo;, k)$, then continue
with equation (4.2) as&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
\begin{aligned}
\nabla_{\theta} V_{\pi_{\theta}}(s) &amp;= \phi(s) + \gamma\sum_{a}\pi_{\theta}(a \mid s) \sum_{s&#39;} p(s&#39; \mid s, a)\nabla_{\theta} V_{\pi_{\theta}}(s&#39;) \\
&amp;= \phi(s) + \gamma\sum_{a}\sum_{s&#39;}\pi_{\theta}(a \mid s)  p(s&#39; \mid s, a)\nabla_{\theta} V_{\pi_{\theta}}(s&#39;) \\
&amp;= \phi(s) + \gamma\sum_{s&#39;}d_{\pi_{\theta}}(s \rightarrow s&#39;, 1)\nabla_{\theta} V_{\pi_{\theta}}(s&#39;) \\
&amp;= \phi(s) + \gamma\sum_{s&#39;}d_{\pi_{\theta}}(s \rightarrow s&#39;, 1) [\phi(s&#39;) + \gamma\sum_{s&#39;&#39;}d_{\pi_{\theta}}(s&#39; \rightarrow s&#39;&#39;, 1)\nabla_{\theta} V_{\pi_{\theta}}(s&#39;&#39;)] \\
&amp;= \phi(s) + \gamma\sum_{s&#39;}d_{\pi_{\theta}}(s \rightarrow s&#39;, 1) \phi(s&#39;) + \gamma^2\sum_{s&#39;&#39;}d_{\pi_{\theta}}(s \rightarrow s&#39;&#39;, 2)\nabla_{\theta} V_{\pi_{\theta}}(s&#39;&#39;) \\
&amp;= \phi(s) + \gamma\sum_{s&#39;}d_{\pi_{\theta}}(s \rightarrow s&#39;, 1) \phi(s&#39;) + \gamma^2\sum_{s&#39;&#39;}d_{\pi_{\theta}}(s \rightarrow s&#39;&#39;, 2)\nabla_{\theta} V_{\pi_{\theta}}(s&#39;&#39;) + \gamma^3\sum_{s&#39;&#39;&#39;}d_{\pi_{\theta}}(s \rightarrow s&#39;&#39;&#39;, 3)\nabla_{\theta} V_{\pi_{\theta}}(s&#39;&#39;&#39;)] \\
&amp;= \dots \\
&amp;= \sum_{x \in S} \sum_{k=0}^{\infty} \gamma^k d_{\pi_{\theta}}(s \rightarrow x, k)\phi(x) \hspace{17 em} \text{(4.3)}
\end{aligned}
&lt;/p&gt;
&lt;p&gt;Now the gradient of the expected cumulative reward function is&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
\begin{aligned}
\nabla_{\theta} J_{\pi_{\theta}}(s) &amp;= \nabla_{\theta} \mathbb{E}_{s_0} [V_{\pi_{\theta}}(s_0)] \\
&amp;= \sum_{s \in S} \mathbb{E}_{s_0} [\sum_{k=0}^{\infty} \gamma^k d_{\pi_{\theta}}(s_0 \rightarrow s, k)\phi(s)] \\
&amp;= \sum_{s \in S} \eta(s)\phi(s), \hspace{1 em} (\eta(s) = \mathbb{E}_{s_0} [\sum_{k=0}^{\infty} \gamma^k d_{\pi_{\theta}}(s_0 \rightarrow s, k)]) \\
&amp;= (\sum_{s \in S} \eta(s)) \sum_{s \in S} \frac{\eta(s)}{\sum_{s \in S} \eta(s)} \phi(s) \\
&amp; \propto \sum_{s \in S} \frac{\eta(s)}{\sum_{s \in S} \eta(s)} \phi(s) \\
&amp;= \sum_{s \in S} \nu(s) \phi(s), \hspace{1 em} (\nu(s) = \frac{\eta(s)}{\sum_{s \in S} \eta(s)}) \\
&amp;= \sum_{s \in S} \nu(s) \sum_{a \in A} Q_{\pi_{\theta}}(s, a) \nabla_{\theta} \pi_{\theta}(a \mid s) \\
&amp;= \sum_{s \in S} \nu(s) \sum_{a \in A} \pi_{\theta}(a \mid s) Q_{\pi_{\theta}}(s, a) \frac{\nabla_{\theta} \pi_{\theta}(a \mid s)}{\pi_{\theta}(a \mid s)} \\
&amp;= \mathbb{E}_{\pi_{\theta}} [Q_{\pi_{\theta}}(s, a) \nabla_{\theta} \log(\pi_{\theta}(a \mid s))] \hspace{16 em} \text{(4.4)}
\end{aligned}
&lt;/p&gt;
&lt;p&gt;Thus, the expected cumulative reward function can be optimized by taking the gradient ascent from (4.4). Note that we may need to estimate
the $Q_{\pi_{\theta}}(s, a)$ when calculating the gradient, a simple way is to apply Monte Carlo here and thus results in &amp;ldquo;REINFORCE&amp;rdquo; algorithm.&lt;/p&gt;
&lt;h4 id=&#34;reinforce&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#reinforce&#34;&gt;
        
    &lt;/a&gt;
    REINFORCE
&lt;/div&gt;
&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Initialize the policy parameter $\theta$&lt;/li&gt;
&lt;li&gt;for episode $e$ from 1 to $K$, do:
&lt;ul&gt;
&lt;li&gt;Initialize the state $s$&lt;/li&gt;
&lt;li&gt;for $t$ from 1 to $T$, do:
&lt;ul&gt;
&lt;li&gt;sample the trajectories $\{s_1, a_1, r_1, \dots, s_T, a_T, r_T\}$ under the policy $\pi_{\theta}$&lt;/li&gt;
&lt;li&gt;For any $t \in [1, T]$, calculate the total reward since time $t$, $\psi_t = \sum_{t&amp;rsquo;=t}^T \gamma^{t&amp;rsquo;-t}r_{t&amp;rsquo;}$&lt;/li&gt;
&lt;li&gt;apply gradient ascent to update $\theta$, i.e. $\theta = \theta + \alpha \sum_t^T \psi_t \nabla_{\theta} \pi_{\theta}(a_t \mid s_t)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;end for&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;end for&lt;/li&gt;
&lt;li&gt;return the policy $\pi_\theta$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is a &amp;ldquo;policy version&amp;rdquo; monte carlo approach. Like the Monte Carlo algorithm, it&amp;rsquo;s a simple online algorithm but may suffer from issues like
high variance, delayed rewards, inefficient sampling, noisy updates and non-stationary returns. To tackle these issues, more dedicated tools
are needed.&lt;/p&gt;
&lt;h4 id=&#34;trust-region-policy-optimization-trpo&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#trust-region-policy-optimization-trpo&#34;&gt;
        
    &lt;/a&gt;
    Trust Region Policy Optimization (TRPO)
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;When using a deep neural network to fit the policy network, the policy gradient updates can be noisy with high variance. Thus resulting in
worse policy updates due to the fluctuation parameters. Is there any way to guarantee the monotonicity of the parameter updates?&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s rephrase the problem as something like this: Suppose the current policy is $\pi_{\theta}$ with parameter $\theta$, we would like
to search for a new parameter $\theta&amp;rsquo;$ such that $J(\theta&amp;rsquo;) \geq J(\theta)$.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
\begin{aligned}
J(\theta&#39;) - J(\theta) &amp;= \mathbb{E}_{s_0}[V_{\pi_{\theta&#39;}}(s_0)] - \mathbb{E}_{s_0}[V_{\pi_{\theta}}(s_0)] \\
&amp;= \mathbb{E}_{\pi_{\theta&#39;}}[\sum_{t=0}^{\infty} \gamma^{t} r_t] - \mathbb{E}_{\pi_{\theta&#39;}}[\sum_{t=0}^{\infty} \gamma^{t} V_{\pi_{\theta}}(s_t) - \sum_{t=1}^{\infty} \gamma^{t} V_{\pi_{\theta}}(s_t)] \hspace{7 em} \text{(4.5)}
\end{aligned}
&lt;/p&gt;
&lt;p&gt;The equation (4.5) holds because the start state $s_0$ does not depend on policy $\pi_{\theta&amp;rsquo;}$, thus the expectation can be
rewritten under the policy $\pi_{\theta&amp;rsquo;}$.&lt;/p&gt;
&lt;h4 id=&#34;ppo&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#ppo&#34;&gt;
        
    &lt;/a&gt;
    PPO
&lt;/div&gt;
&lt;/h4&gt;
&lt;h4 id=&#34;cross-entropy-method-gradient-free&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#cross-entropy-method-gradient-free&#34;&gt;
        
    &lt;/a&gt;
    Cross-Entropy Method [Gradient Free]
&lt;/div&gt;
&lt;/h4&gt;
&lt;h4 id=&#34;evolution-strategy-gradient-free&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#evolution-strategy-gradient-free&#34;&gt;
        
    &lt;/a&gt;
    Evolution Strategy [Gradient Free]
&lt;/div&gt;
&lt;/h4&gt;
&lt;h3 id=&#34;hybrid&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#hybrid&#34;&gt;
        
    &lt;/a&gt;
    Hybrid
&lt;/div&gt;
&lt;/h3&gt;
&lt;h4 id=&#34;ddpq&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#ddpq&#34;&gt;
        
    &lt;/a&gt;
    DDPQ
&lt;/div&gt;
&lt;/h4&gt;
&lt;h4 id=&#34;actor-critic-ac&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#actor-critic-ac&#34;&gt;
        
    &lt;/a&gt;
    Actor Critic (AC)
&lt;/div&gt;
&lt;/h4&gt;
&lt;h4 id=&#34;sac&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#sac&#34;&gt;
        
    &lt;/a&gt;
    SAC
&lt;/div&gt;
&lt;/h4&gt;
&lt;h2 id=&#34;reference&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#reference&#34;&gt;
        
    &lt;/a&gt;
    Reference
&lt;/div&gt;
&lt;/h2&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;Mao, Hongzi, et al. &amp;ldquo;Resource management with deep reinforcement learning.&amp;rdquo; Proceedings of the 15th ACM workshop on hot topics in networks. 2016&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;Sebastianelli, Alessandro, et al. &amp;ldquo;A Deep Q-Learning based approach applied to the Snake game.&amp;rdquo; 2021 29th Mediterranean Conference on Control and Automation (MED). IEEE, 2021&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;Muteba, K. F., Karim Djouani, and Thomas O. Olwal. &amp;ldquo;Deep reinforcement learning based resource allocation for narrowband cognitive radio-IoT systems.&amp;rdquo; Procedia Computer Science 175 (2020): 315-324&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;Hessel, Matteo, et al. &amp;ldquo;Rainbow: Combining improvements in deep reinforcement learning.&amp;rdquo; Proceedings of the AAAI conference on artificial intelligence. Vol. 32. No. 1. 2018.&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref1:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>A Journey to Reinforcement Learning - Tabular Methods</title>
      <link>/posts/2024-07-05-rf-learning-tabular/</link>
      <pubDate>Fri, 05 Jul 2024 20:59:39 -0700</pubDate>
      <author>bingwang8878@gamil.com (Bing Wang)</author>
      <guid>/posts/2024-07-05-rf-learning-tabular/</guid>
      <description>&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://bingzw.github.io/rf/rf.png&#34; width=&#34;600&#34; height=&#34;400&#34;&gt;&lt;br&gt;
&lt;p&gt;
&lt;p&gt;&lt;em&gt;Image cited from &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;basic-problem-statement-of-reinforcement-learning&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#basic-problem-statement-of-reinforcement-learning&#34;&gt;
        
    &lt;/a&gt;
    Basic Problem Statement of Reinforcement Learning
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in
an environment to maximize cumulative reward. Unlike supervised learning, where the model is trained on a fixed dataset,
RL involves learning through interaction with the environment. Let&amp;rsquo;s define some basic elements in RL domain.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Agent&lt;/strong&gt;: The learner or decision maker.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Environment&lt;/strong&gt;: The external system the agent interacts with.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;State&lt;/strong&gt; $S$: A representation of the current situation of the agent.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Action&lt;/strong&gt; $A$: The set of all possible moves the agent can make.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reward&lt;/strong&gt; $R$: A feedback signal from the environment to evaluate the agent&amp;rsquo;s action.
&lt;ul&gt;
&lt;li&gt;A factor $\gamma$ between 0 and 1 that represents the difference in importance between future rewards and immediate rewards is
usually used to prioritize short-term rewards over long-term rewards&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Policy $\pi$&lt;/strong&gt;: A strategy used by the agent to determine the next action based on the current state. It&amp;rsquo;s usually
a probability function $\pi: S\times A$ -&amp;gt; $[0, 1]$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Value Function $V_{\pi}(s)$&lt;/strong&gt;: A function that estimates the expected cumulative reward from a given state with the policy $\pi$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Q-Function $Q_{\pi}(s, a)$&lt;/strong&gt;: A function that estimates the expected cumulative reward from a given state-action pair with the policy $\pi$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As it can be seen in the above figure, the general workflow of RL involves&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Observe the state $s_t$ (and reward $r_t$), the state can be any representation of the current situation or context in which the agent operates.
Note that the state space $S$ can be either &lt;strong&gt;finite&lt;/strong&gt; or &lt;strong&gt;infinite&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Based on the current policy $\pi$, the agent selects an action $a_t \in A$ to perform. The selection can be &lt;strong&gt;deterministic&lt;/strong&gt;
or &lt;strong&gt;stochastic&lt;/strong&gt; depending on the policy.&lt;/li&gt;
&lt;li&gt;The agent performs the selected action $a_t$ in the environment (can also be either &lt;strong&gt;known&lt;/strong&gt; or &lt;strong&gt;unknown&lt;/strong&gt;).&lt;/li&gt;
&lt;li&gt;After performing the action, the agent receives a reward $r_{t+1}$ from the environment and observes the next state $s_{t+1}$.
The mechanism of moving from one state to another given a specific action is modeled by a probability function, denoted as $P(s_{t+1} \mid s_t, a_t)$.&lt;/li&gt;
&lt;li&gt;The agent updates its &lt;strong&gt;policy&lt;/strong&gt; $\pi(a_t \mid s_t)$ and &lt;strong&gt;value functions&lt;/strong&gt; $V(s_t)$ or $Q(s_t, a_t)$ based on the observed reward $r_t$
and next state $s_{t+1}$. The update rule varies depending on the RL algorithm used. (Note that the policy learned in step 5
can be either the &lt;strong&gt;same (on policy)&lt;/strong&gt; or &lt;strong&gt;different (off policy)&lt;/strong&gt; with the ones in step 2)&lt;/li&gt;
&lt;li&gt;The agent repeats again from step 1 and continues this iterative process until the policy converges, meaning it has
learned an optimal or near-optimal policy that maximizes cumulative rewards over time.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Reinforcement Learning (RL) derives its name from the concept of &amp;ldquo;reinforcement&amp;rdquo; in behavioral psychology, where learning
occurs through rewards and punishments. The agent learns to make decisions by receiving feedback in the form of rewards or
penalties. Positive outcomes reinforce the actions that led to them, strengthening the behavior. The learning process is kind
of a process of &lt;strong&gt;&amp;ldquo;Trial and Error&amp;rdquo;&lt;/strong&gt;, where the agent explores different actions to discover which ones yield the highest rewards.
Long-term beneficial actions are reinforced through repeated positive outcomes.&lt;/p&gt;
&lt;p&gt;Now let&amp;rsquo;s start with the most influential and fundamental RL model - Markov Decision Process (MDP). Our fantastic journey begins here.
Note that the below algorithms and equations are cited from &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;, &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; and &lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h3 id=&#34;markov-decision-process-mdp&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#markov-decision-process-mdp&#34;&gt;
        
    &lt;/a&gt;
    Markov Decision Process (MDP)
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s be more specific about the above settings to realize the MDP. Assume the state space $S$ and action space $A$ are finite,
the process is equipped with the markov property, that is, $P(s_{t+1} \mid h_t) = P(s_{t+1} \mid s_t, a_t)$, where $h_t = \{ s_1, a_1, &amp;hellip; , s_t, a_t \}$
denotes the history of states and actions.&lt;/p&gt;
&lt;p&gt;Suppose the agent is interacting with the environment for a total $T$ steps (horizon). Let&amp;rsquo;s define the total reward after time $t$ as
$$
G_t = r_{t+1} + \gamma r_{t+2} + &amp;hellip; + \gamma^{T-t-1} r_T \tag{1.1}
$$
The value function is defined as
$$
V_{\pi}(s) = \mathbb{E}_{\pi}[G_t \mid s_t=s] \tag{1.2}
$$&lt;/p&gt;
&lt;p&gt;The value action function $Q$ is defined as&lt;/p&gt;
&lt;p&gt;$$
Q_{\pi}(s, a) = \mathbb{E}_{\pi}[G_t \mid s_t=s, a_t=a] \tag{1.3}
$$&lt;/p&gt;
&lt;p&gt;From the above definition, it&amp;rsquo;s easy to derive the connection between $V$ and $Q$. In particular, when marginalizing the action,
we are able to convert $Q$ function to $V$ value function.&lt;/p&gt;
&lt;p&gt;$$
V_{\pi}(s) = \sum_{a \in A} \pi(a \mid s)Q_{\pi}(s, a) \tag{1.4}
$$&lt;/p&gt;
&lt;p&gt;when converting $V$ to $Q$, we can see that&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
\begin{aligned}
Q_{\pi}(s, a) &amp;= \mathbb{E}_{\pi}[G_t \mid s_t=s, a_t=a] \\
&amp;= \mathbb{E}_{\pi}[r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3}+ ... \mid s_t=s, a_t=a] \\
&amp;= \mathbb{E}_{\pi}[r_{t+1} \mid s_t=s, a_t=a] + \gamma \mathbb{E}_{\pi}[r_{t+2} + \gamma r_{t+3} + ... \mid s_t=s, a_t=a] \\
&amp;= R(s, a) + \gamma \mathbb{E}_{\pi}[G_{t+1} \mid s_t=s, a_t=a] \\
&amp;= R(s, a) + \gamma \mathbb{E}_{\pi}[\mathbb{E}_{\pi}[G_{t+1} \mid s_{t+1}] \mid s_t=s, a_t=a] \\
&amp;= R(s, a) + \gamma \mathbb{E}_{\pi}[V_{\pi}(s_{t+1}) \mid s_t=s, a_t=a] \\
&amp;= R(s, a) + \gamma \sum_{s&#39; \in S} p(s&#39; \mid s, a) V_{\pi}(s&#39;) \hspace{15.5em} \text{(1.5)}
\end{aligned}
&lt;/p&gt;
&lt;h4 id=&#34;bellman-expectation-equation&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#bellman-expectation-equation&#34;&gt;
        
    &lt;/a&gt;
    Bellman Expectation Equation
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;From (1.2), we can express the value function in an recursive way.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
\begin{aligned}
V_{\pi}(s) &amp;= \mathbb{E}_{\pi}[G_t \mid s_t=s] \\
&amp;= \mathbb{E}_{\pi}[r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + ... \mid s_t=s] \\
&amp;= \mathbb{E}_{\pi}[r_{t+1} \mid s_t=s] + \gamma\mathbb{E}_{\pi}[r_{t+2} + \gamma r_{t+3} + ... \mid s_t=s] \\
&amp;= \mathbb{E}_{\pi}[r_{t+1} \mid s_t=s] + \gamma\mathbb{E}_{\pi}[G_{t+1} \mid s_t=s] \\
&amp;= \mathbb{E}_{\pi}[r_{t+1} \mid s_t=s] + \gamma\mathbb{E}_{\pi}[\mathbb{E}_{\pi}[G_{t+1} \mid s_{t+1}] \mid s_t=s] \\
&amp;= \mathbb{E}_{\pi}[r_{t+1} \mid s_t=s] + \gamma\mathbb{E}_{\pi}[V_{\pi}(s_{t+1}) \mid s_t=s] \\
&amp;= \mathbb{E}_{\pi}[r_{t+1} + \gamma V_{\pi}(s_{t+1}) \mid s_t=s] \hspace{19em} \text{(1.6)}
\end{aligned}
&lt;/p&gt;
Similarly, the state action function can also be written recursively
$$
Q_{\pi}(s, a) = \mathbb{E}_{\pi}[r_{t+1} + \gamma Q_{\pi}(s_{t+1}, a_{t+1}) \mid s_t=s, a_t=a] \tag{1.7}
$$
Furthermore, equation (1.5) also expressed the current-future connection between $V$ and $Q$. So if we plug equation 
(1.5) in (1.4), then we would get 
$$
V_{\pi}(s) = \sum_{a \in A} \pi(a \mid s)(R(s, a) + \gamma \sum_{s&#39; \in S} p(s&#39; \mid s, a) V_{\pi}(s&#39;)) \tag{1.8}
$$
which denotes the connection of value function at current state and future state.
On the other hand, if (1.4) is plugged in (1.5), we can see
$$
Q_{\pi}(s, a) = R(s, a) + \gamma \sum_{s&#39; \in S} p(s&#39; \mid s, a) \sum_{a&#39; \in A} \pi(a&#39; \mid s&#39;)Q_{\pi}(s&#39;, a&#39;) \tag{1.9}
$$
which builds the connection of the action value function between the current and future state action pairs. By comparing
(1.6) and (1.8), (1.7) and (1.9), it&#39;s easy to observe that (1.8) and (1.9) actually implements the expectation expression explicitly. 
&lt;p&gt;A visualized interpretation of (1.8) and (1.9) are shown in the following backup diagram.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
    &lt;img src=&#34;https://bingzw.github.io/rf/backup.png&#34;&gt;&lt;br&gt;
    &lt;em&gt;Figure 1: Backup Diagram&lt;/em&gt;
&lt;/p&gt;
&lt;h4 id=&#34;bellman-optimal-equation&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#bellman-optimal-equation&#34;&gt;
        
    &lt;/a&gt;
    Bellman Optimal Equation
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;The goal of reinforcement learning is to find the optimal policy $\pi^*$ such that the value function is maximized.&lt;/p&gt;
&lt;p&gt;$$
\pi^*(s) = \arg \max_{\pi} V_{\pi}(s)
$$&lt;/p&gt;
&lt;p&gt;when this is achieved, the optimal value function is $V^*(s) = max_{\pi}V_{\pi}(s), \forall s \in S$. At this time,
the optimal value function can also be achieved by selecting the best action under the optimal policy&lt;/p&gt;
&lt;p&gt;$$
V^* (s) = \max_{a} Q^* (s, a) \tag{1.10}
$$&lt;/p&gt;
&lt;p&gt;where $Q^* (s, a) = \arg \max_{\pi} Q_{\pi}(s, a)$, $\forall s \in S, a \in A$.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s apply the optimal policy in (1.5) and we have&lt;/p&gt;
&lt;p&gt;$$
Q^* (s, a) = R(s, a) + \gamma \sum_{s&amp;rsquo; \in S} p(s&amp;rsquo; \mid s, a) V^*(s&amp;rsquo;) \tag{1.11}
$$&lt;/p&gt;
&lt;p&gt;When plugging (1.10) in (1.11), we get &lt;strong&gt;Bellman optimal equation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
Q^* (s, a) = R(s, a) + \gamma \sum_{s&amp;rsquo; \in S} p(s&amp;rsquo; \mid s, a) \max_{a&amp;rsquo;} Q^* (s&amp;rsquo;, a&amp;rsquo;) \tag{1.12}
$$&lt;/p&gt;
&lt;p&gt;Similarly, plugging (1.11) in (1.10), the Bellman optimal value equation is&lt;/p&gt;
&lt;p&gt;$$
V^* (s) = \max_{a} \left( R(s, a) + \gamma \sum_{s&amp;rsquo; \in S} p(s&amp;rsquo; \mid s, a) V^* (s&amp;rsquo;) \right) \tag{1.13}
$$&lt;/p&gt;
&lt;p&gt;So far, we have introduced the main math modeling framework in RL. How shall we fit the real-life cases into this framework
and get the best policy? What trade-offs have to be made in each scenario? Let&amp;rsquo;s take a deep dive.&lt;/p&gt;
&lt;h2 id=&#34;model-based---when-the-environment-is-given&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#model-based---when-the-environment-is-given&#34;&gt;
        
    &lt;/a&gt;
    Model Based - when the environment is given
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;Our journey begins with a typical MDP where the transition mechanism and the reward function are fully known. As seen in
the above Bellman expectation and optimal equations, the problem can be resolved by tackling a similar subproblem recursively.
This is usually known as dynamic programming.&lt;/p&gt;
&lt;h3 id=&#34;dynamic-programming&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#dynamic-programming&#34;&gt;
        
    &lt;/a&gt;
    Dynamic Programming
&lt;/div&gt;
&lt;/h3&gt;
&lt;h4 id=&#34;policy-iteration&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#policy-iteration&#34;&gt;
        
    &lt;/a&gt;
    policy iteration
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;The key idea is to iteratively process two alternative steps: policy evaluation and policy improvement. Policy evaluation
computes the value function $V^\pi$ for the current policy $\pi$. While policy improvement updates the policy $\pi$ using
the greedy approach.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;initialize the policy $\pi(s)$ and value function $V(s)$&lt;/li&gt;
&lt;li&gt;while not stop
&lt;ul&gt;
&lt;li&gt;while $\delta &amp;gt; \theta$, do
&lt;ul&gt;
&lt;li&gt;$\delta \leftarrow 0$&lt;/li&gt;
&lt;li&gt;for each $s \in S$
&lt;ul&gt;
&lt;li&gt;$v \leftarrow V(s)$&lt;/li&gt;
&lt;li&gt;$V(s) \leftarrow \sum_{a \in A} \pi(a \mid s)(R(s, a) + \gamma \sum_{s&amp;rsquo; \in S} p(s&amp;rsquo; \mid s, a) V_{\pi}(s&amp;rsquo;)) $ &lt;strong&gt;(policy evaluation)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;$\delta \leftarrow \max(\delta, |v - V(s)|)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;end while&lt;/li&gt;
&lt;li&gt;$\pi_{old} \leftarrow \pi$&lt;/li&gt;
&lt;li&gt;for each $s \in S$
&lt;ul&gt;
&lt;li&gt;$\pi(s) \leftarrow \arg \max_{a} R(s, a) + \gamma \sum_{s&amp;rsquo;} p(s&amp;rsquo; \mid s, a)V(s&amp;rsquo;)$ &lt;strong&gt;(policy improvement)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;if $\pi_{old} = \pi$
&lt;ul&gt;
&lt;li&gt;stop and return $\pi$ and $V$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;value-iteration&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#value-iteration&#34;&gt;
        
    &lt;/a&gt;
    value iteration
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;It usually takes quite a significant amount of time to run policy evaluation, especially when the state and action space are
large enough. Is there a way to avoid too many policy evaluation process? The answer is value iteration. It&amp;rsquo;s an iterative process
to update the Bellman optimal equation (1.13).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;initialize the value function $V(s)$&lt;/li&gt;
&lt;li&gt;while $\delta &amp;gt; \theta$, do
&lt;ul&gt;
&lt;li&gt;$\delta \leftarrow 0$&lt;/li&gt;
&lt;li&gt;for each $s \in S$
&lt;ul&gt;
&lt;li&gt;$v \leftarrow V(s)$&lt;/li&gt;
&lt;li&gt;$V(s) \leftarrow \max_{a \in A} (R(s, a) + \gamma \sum_{s&amp;rsquo; \in S} p(s&amp;rsquo; \mid s, a) V_{\pi}(s&amp;rsquo;)) $&lt;/li&gt;
&lt;li&gt;$\delta \leftarrow \max(\delta, |v - V(s)|)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;end while&lt;/li&gt;
&lt;li&gt;$\pi(s) \leftarrow \arg \max_{a} R(s, a) + \gamma \sum_{s&amp;rsquo;} p(s&amp;rsquo; \mid s, a)V(s&amp;rsquo;)$&lt;/li&gt;
&lt;li&gt;return $V$ and $\pi$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It can be seen that value iteration doesn&amp;rsquo;t own policy updates, it generates the optimal policy when the value function converges.&lt;/p&gt;
&lt;h2 id=&#34;model-free---when-the-environment-is-unknown&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#model-free---when-the-environment-is-unknown&#34;&gt;
        
    &lt;/a&gt;
    Model Free - when the environment is unknown
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;In practical, the environment is hardly fully known or it&amp;rsquo;s simply a blackbox most of the time. Thus dynamic programming (policy
iteration &amp;amp; value iteration) might not helpful. In this section, we will introduce solutions originated from various kinds ideas.&lt;/p&gt;
&lt;h3 id=&#34;value-based&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#value-based&#34;&gt;
        
    &lt;/a&gt;
    Value Based
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;This collection of algorithms aims optimizing the the value function $V$ or $Q$, which can then be used to derive the optimal
policy. The policy is typically derived indirectly by selecting actions that maximize the estimated value. Value based methods
are usually simple to implement and understand. They are effective in environments with discrete and finite action spaces.
Deriving optimal policies is clear and straightforward. However, they are usually struggling with high-dimensional or continuous
action spaces. Trying function approximation (e.g., neural networks) may not work well due to unstable and divergence. Besides,
value based methods usually require extensive exploration to accurately estimate value functions.&lt;/p&gt;
&lt;h4 id=&#34;model-free-policy-evaluation-monte-carlo--temporal-difference-td&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#model-free-policy-evaluation-monte-carlo--temporal-difference-td&#34;&gt;
        
    &lt;/a&gt;
    Model Free Policy Evaluation: Monte Carlo &amp;amp; Temporal Difference (TD)
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;Value based approaches inherits the idea from dynamic programming. The difference now is that the environment is unknown such that
both policy iteration and value iteration are not feasible (transition probably unavailable now). Let&amp;rsquo;s start with a simple question:
&lt;strong&gt;how can we estimate the value function given a policy when the environment is unknown&lt;/strong&gt;. The idea is to &lt;strong&gt;interact with the environment&lt;/strong&gt;
and update the value function/policy based on the returned rewards. Depending on how heavily we rely on interacting with the
environment, we have the Monte Carlo and Temporal difference method.&lt;/p&gt;
&lt;h5 id=&#34;monte-carlo&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#monte-carlo&#34;&gt;
        
    &lt;/a&gt;
    Monte Carlo
&lt;/div&gt;
&lt;/h5&gt;
&lt;p&gt;Monte Carlo methods rely on averaging returns of sampled episodes to estimate the expected value of states or state-action pairs.
Unlike temporal difference (TD) methods, Monte Carlo methods do not bootstrap and instead use complete episodes to update value estimates.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initialize $V_{\pi}(s)$ arbitrarily for all states $s \in S$.&lt;/li&gt;
&lt;li&gt;Initialize the total reward $S(s)$ and total visits $N(s)$&lt;/li&gt;
&lt;li&gt;for episode from 1 to $N$, do:
&lt;ul&gt;
&lt;li&gt;Generate an episode trajectory $(s_0, a_0, r_1, s_1, a_1, r_2, \ldots, s_T)$ following policy $\pi$.&lt;/li&gt;
&lt;li&gt;For each state $s$ that first appearing in the episode trajectory:
&lt;ul&gt;
&lt;li&gt;Compute the return $G_t$  from state $s$: $G_t = r_{t+1} + \gamma r_{t+2} + \dots + \gamma^{T-t-1} r_T$&lt;/li&gt;
&lt;li&gt;total reward at $s$ is $S(s) \leftarrow S(s) + G_t$&lt;/li&gt;
&lt;li&gt;total count visiting $s$ is $N(s) \leftarrow N(s) + 1$&lt;/li&gt;
&lt;li&gt;Update the value estimate $V_{\pi}(s)$ as the average of all observed returns for state $s$: $V_{\pi}(s) \leftarrow \frac{S(s)} {N(s)}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;end for&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;end for&lt;/li&gt;
&lt;li&gt;return the value function under policy $\pi$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It&amp;rsquo;s worth noting that the monte carlo update can also be reformated in an incremental way, that is,
$$
V_{\pi}(s) \leftarrow V_{\pi}(s) + \frac{1}{N(s)} (G_t - V_{\pi}(s)) \tag{3.1}
$$
It can be viewed to update the value function with an error term
$G_t - V_{\pi}(s)$. As $V_{\pi}(s) \rightarrow G_t$, then the error term does not bring too much value and as a result $V(s)$ converges to
$G_t$. So $G_t$ can somehow be interpreted as the target of the update. In Monte Carlo, the updates does not start until the
entire episode completes. This may not feasible in some cases where the interactive game never ends or more frequent updates
are expected. To tackle this, we are happy to introduce temporal difference.&lt;/p&gt;
&lt;h5 id=&#34;temporal-difference&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#temporal-difference&#34;&gt;
        
    &lt;/a&gt;
    Temporal Difference
&lt;/div&gt;
&lt;/h5&gt;
&lt;p&gt;From (1.2), we can see that the Monte Carlo is approximating the target $G_t$ using (3.1). If we only interact with the environment
one step instead of completing the full episode trajectory. It&amp;rsquo;s equivalent to reformat the (1.2) as (1.6), where the target is
thus $r_{t} + \gamma V_{\pi}(s_{t+1})$. So the formula (3.1) can be written as
$$
V_{\pi}(s) \leftarrow V_{\pi}(s) + \alpha (r_{t+1} + \gamma V_{\pi}(s_{t+1}) - V_{\pi}(s)) \tag{3.2}
$$
where the $r_{t+1} + \gamma V_{\pi}(s_{t+1}) - V_{\pi}(s)$ is called temporal difference error. Since only one step reward is retrieved
from the system interaction, it&amp;rsquo;s also noted as 1-step temporal difference, i.e. TD(1). What if we interact a few more steps with the environment?
The target $G_t$ would include more future steps rewards. A general representation of TD(k) is shown below.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
\begin{aligned}
&amp;TD(1) \hspace{1em} \rightarrow \hspace{1em} G^{1}_t = r_{t+1} + \gamma V(s_{t+1}) \\
&amp;TD(2) \hspace{1em} \rightarrow \hspace{1em} G^{2}_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 V(s_{t+2}) \\
&amp;TD(k) \hspace{1em} \rightarrow \hspace{1em} G^{k}_t = r_{t+1} + \gamma r_{t+2} + \dots + \gamma^{k-1} r_{t+k} + \gamma^k V(s_{t+k}) \\
&amp;TD(\infty) / MC \hspace{1em} \rightarrow \hspace{1em} G^{\infty}_t = r_{t+1} + \gamma r_{t+2}+ \dots + \gamma^{T-t-1} r_{T})
\end{aligned}
&lt;/p&gt;
&lt;p&gt;Compared with Monte Carlo, it&amp;rsquo;s possible to updating the value function in an online fashion, meaning that update happens after
every step of interaction. It&amp;rsquo;s more efficient than updating after completing an episode. This also indicates that TD learning can be
applied to any piece of episode, which is more flexible. The estimation variance is lower but bias can be higher due to bootstrapping
(updates based on estimated value of next state)&lt;/p&gt;
&lt;p&gt;Below compares the computation graph among MC, TD and DP. In MC, only the states on the episode trajectory are updated. On the
other hand, DP computes all the related next states when updating. TD compromise both MC and DP, it sampled only one or finite steps
for updating.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://bingzw.github.io/rf/rf_dp_mc_td.png&#34; width=&#34;900&#34; height=&#34;600&#34;&gt;&lt;br&gt;
&lt;em&gt;Figure 2: Visual Interpretation of DP, TD and MC&lt;/em&gt;
&lt;p&gt;
&lt;p&gt;&lt;em&gt;Image cited from &lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h4 id=&#34;sarsa&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#sarsa&#34;&gt;
        
    &lt;/a&gt;
    SARSA
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;Now given a policy $\pi$, we have learned TD and MC can help evaluate the value function when the environment is unknown.
The next natural question is How can we find the optimal policy? We can borrow the policy improvement idea from policy
iteration in DP. Get the next policy by using the greedy search, i.e. $\pi_{i+1}(s) = \arg \max_{a} Q_{\pi_i}(s, a)$.
Combining the generalized policy evaluation and the greedy policy improvement would allow us to build the generalized policy
iteration.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://bingzw.github.io/rf/policy_iteration.png&#34; width=&#34;250&#34; height=&#34;150&#34;&gt;&lt;br&gt;
&lt;em&gt;Figure 3: Generalized Policy Iteration&lt;/em&gt;
&lt;p&gt;
&lt;p&gt;Another difference from the policy iteration in DP is that we may never update some state-action pairs using the pure greedy
policy improvement strategy. This is because we are now interacting with the environment instead of updating all the states action
explicitly with known transition probabilities. So we may need to allow some extent of exploration to mitigate such issue. One
solution is to apply the $\epsilon$-greedy strategy.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
\begin{aligned}
\pi(a \mid s) &amp;= 
\begin{cases} 
      1 - \epsilon &amp; \text{if } a = \arg \max_a Q(s, a) \\
      \epsilon &amp;  \text{other actions in A}
\end{cases}
\end{aligned}
&lt;/p&gt;
&lt;p&gt;Thus the complete algorithm (TD policy evaluation on $Q$ + $\epsilon$-greedy policy improvement) is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initialize the $Q(s, a)$&lt;/li&gt;
&lt;li&gt;for iteration from 1 to $N$, do:
&lt;ul&gt;
&lt;li&gt;get initial state $s$&lt;/li&gt;
&lt;li&gt;apply the $\epsilon$-greedy strategy to choose the action $a$ based on $s$&lt;/li&gt;
&lt;li&gt;for $t$ from 1 to $T$, do:
&lt;ul&gt;
&lt;li&gt;interact with the environment and get reward $r$ and $s&#39;$&lt;/li&gt;
&lt;li&gt;apply the $\epsilon$-greedy strategy to choose the next action $a&amp;rsquo;$ based on $s&#39;$&lt;/li&gt;
&lt;li&gt;$Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma Q(s&amp;rsquo;, a&amp;rsquo;) - Q(s, a)]$ &lt;strong&gt;(TD policy evaluation)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;$s \leftarrow s&amp;rsquo;, a \leftarrow a&amp;rsquo;$ &lt;strong&gt;($\epsilon$-greedy policy improvement)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;end for&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;end for&lt;/li&gt;
&lt;li&gt;return the value action $Q$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It&amp;rsquo;s worth noting that once the state, action, reward, next state, next action $(s, a, r, s&amp;rsquo;, a&amp;rsquo;)$ is generated, the update is
conducted once and then repeats the iterations. Therefore, is called &lt;strong&gt;SARSA&lt;/strong&gt;. In a more general case, if we interact more steps
and update the $Q$ value using $Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha [r_t + \gamma r_{t+1} + \dots + \gamma^n Q(s_{t+n}, a_{t+n}) - Q(s_t, a_t)]$,
then it&amp;rsquo;s &lt;strong&gt;$n$ step SARSA&lt;/strong&gt;.&lt;/p&gt;
&lt;h4 id=&#34;q-learning&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#q-learning&#34;&gt;
        
    &lt;/a&gt;
    Q-Learning
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;If the way of updating the TD policy evaluation in SARSA is changed to
$$
Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a&amp;rsquo;} Q(s&amp;rsquo;, a&amp;rsquo;) - Q(s, a)] \tag{3.3}
$$
Then we would get the Q learning algorithm.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initialize the $Q(s, a)$&lt;/li&gt;
&lt;li&gt;for iteration from 1 to $N$, do:
&lt;ul&gt;
&lt;li&gt;get initial state $s$&lt;/li&gt;
&lt;li&gt;for $t$ from 1 to $T$, do:
&lt;ul&gt;
&lt;li&gt;apply the $\epsilon$-greedy strategy to choose the action $a$ based on $s$&lt;/li&gt;
&lt;li&gt;interact with the environment and get reward $r$ and $s&#39;$&lt;/li&gt;
&lt;li&gt;$Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a&amp;rsquo;} Q(s&amp;rsquo;, a&amp;rsquo;) - Q(s, a)]$&lt;/li&gt;
&lt;li&gt;$s \leftarrow s&#39;$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;end for&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;end for&lt;/li&gt;
&lt;li&gt;return the value action $Q$ and derive the optimal policy&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The major difference between Q learning and SARSA is about the way to update the $Q$ value. In (3.3), we are actually
approximating the target value $r + \gamma \max_{a&amp;rsquo;} Q(s&amp;rsquo;, a&amp;rsquo;)$. So the goal here is to learn
the optimal policy $\pi(a \mid s) = \max_{a&amp;rsquo;} Q(s, a&amp;rsquo;)$. Let&amp;rsquo;s call the policy used in (3.3) as the target policy.
The data used to learn the target policy $(s, a, r, s&amp;rsquo;)$ is however generated from another policy that interacts with the
environment, called the behavior policy ($\epsilon$-greedy). It&amp;rsquo;s easy to see that the target policy is different from the
behavior policy. This is also called &lt;strong&gt;off-policy&lt;/strong&gt; learning.&lt;/p&gt;
&lt;p&gt;On the other hand, the behavior policy fully aligns with the target policy in SARSA, which means $a&amp;rsquo;$ and $a$ are generated
from the same policy. This kind of learning is called &lt;strong&gt;on-policy&lt;/strong&gt; learning.&lt;/p&gt;
&lt;p&gt;One interesting common area among DP, SARSA, Q-learning is that they all assume finite or discrete state and action space.
Optimizing such problem can be seen as updating a table with an entry for each possible state or state-action pair. Thus,
these algorithms are also called &lt;strong&gt;tabular methods&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;To be continued &amp;hellip;&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;summary&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#summary&#34;&gt;
        
    &lt;/a&gt;
    Summary
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;So far, we have introduced the basic RL setting and basic tabular methods that are suitable to simple cases. However, most
real problems involve large quantity or infinite state or action space. More powerful tools are thus needed. In next blog,
we will start the journey to deep reinforcement learning, an area that utilize the fancy deep learning techniques to tackle
more complicated problems.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#citation&#34;&gt;
        
    &lt;/a&gt;
    Citation
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;If you find this post helpful, please consider citing it as:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bibtex&#34; data-lang=&#34;bibtex&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f3f99d&#34;&gt;@article&lt;/span&gt;{&lt;span style=&#34;color:#ff5c57&#34;&gt;wang2024rflearningtabular&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;author&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;Bing Wang&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;title&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;A Journey to Reinforcement Learning - Tabular Methods&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;journal&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;bingzw.github.io&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;year&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;2024&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;month&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;July&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;url&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;https://bingzw.github.io/posts/2024-07-05-rf-learning-tabular/&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;or&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Bing Wang. (2024, June). A Journey to Reinforcement Learning - Tabular Methods. 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;https://bingzw.github.io/posts/2024-07-05-rf-learning-tabular/
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;reference&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#reference&#34;&gt;
        
    &lt;/a&gt;
    Reference
&lt;/div&gt;
&lt;/h2&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.mdpi.com/2076-3417/13/4/2443&#34;&gt;Souchleris, Konstantinos, George K. Sidiropoulos, and George A. Papakostas. &amp;ldquo;Reinforcement learning in game industry—review, prospects and challenges.&amp;rdquo; Applied Sciences 13.4 (2023): 2443&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://datawhalechina.github.io/easy-rl/&#34;&gt;Easy RL&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://hrl.boyuai.com/&#34;&gt;Hands On RL&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf&#34;&gt;Reinforcement Learning: An Introduction&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://roboticseabass.com/2020/08/02/an-intuitive-guide-to-reinforcement-learning/&#34;&gt;An intuitive guide to reinforcement learning&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Recommendation Models</title>
      <link>/posts/2024-06-06-recommendation-model/</link>
      <pubDate>Thu, 06 Jun 2024 22:04:53 -0700</pubDate>
      <author>bingwang8878@gamil.com (Bing Wang)</author>
      <guid>/posts/2024-06-06-recommendation-model/</guid>
      <description>&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://bingzw.github.io/recommendation_model/reco.jpeg&#34; width=&#34;600&#34; height=&#34;400&#34;&gt;&lt;br&gt;
&lt;p&gt;
&lt;p&gt;&lt;em&gt;Image cited from &lt;a href=&#34;https://www.vecteezy.com/&#34;&gt;vecteezy&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction-to-recommendation-models&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#introduction-to-recommendation-models&#34;&gt;
        
    &lt;/a&gt;
    Introduction to Recommendation Models
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;In the digital age, the sheer volume of available content and products can be overwhelming for users. Recommendation
systems play a crucial role in helping users discover relevant items by filtering through vast amounts of data. These
systems are integral to various industries, including e-commerce, streaming services, social media, and online
advertising.&lt;/p&gt;
&lt;p&gt;Recommendation models are algorithms designed to suggest items to users based on various factors such as past behavior,
preferences, and item characteristics. The goal is to enhance user experience by providing personalized recommendations,
thereby increasing engagement and satisfaction.&lt;/p&gt;
&lt;p&gt;In most cases, the goal is trying to find the most appealing item for the user given all user demographic, behavior and
interest information. A well-designed recommendation system usually results in better personalization, user engagement,
and revenue generation. Let&amp;rsquo;s take a look at the most popular modeling approaches.&lt;/p&gt;
&lt;h2 id=&#34;collaborative-filtering-cf&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#collaborative-filtering-cf&#34;&gt;
        
    &lt;/a&gt;
    Collaborative Filtering (CF)
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;Collaborative Filtering is one of the most famous recommendation models so far. It&amp;rsquo;s about to guess the user&amp;rsquo;s interest
based on the behaviors and preferences of other users with similar tastes/characteristics. This basic approach is
straightforward and only requires user-item interaction history.&lt;/p&gt;
&lt;h3 id=&#34;memory-based-cf&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#memory-based-cf&#34;&gt;
        
    &lt;/a&gt;
    Memory Based CF
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;Suppose the recommendation is purely derived from historical records and it does not involve any predictive modelings.
Let&amp;rsquo;s assume a table of all ratings of users on items. Let $r_{u,i}$ denotes the missing rating of user $u$ on item $i$.
$S_u$ be a set of users that share similar characteristics with user $u$. $P_j$ denotes a set of items that are close to
item $j$. The goal is to guess the missing rating $r_{u,i}$. Let&amp;rsquo;s start expressing the &amp;ldquo;predicted&amp;rdquo; rating from either user-user
filtering or item-item filtering.&lt;/p&gt;
&lt;h4 id=&#34;user-user-filtering&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#user-user-filtering&#34;&gt;
        
    &lt;/a&gt;
    User User Filtering
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;Assume that users with a similar profile share a similar taste. We are predicting the missing rating as the weighted
average from ratings of similar users:&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
$r_{u,i} = \bar{r}_u + \frac{\sum_{u&#39;\in S_u}sim(u, u&#39;) * (r_{u&#39;,i} - \bar{r}_{u&#39;})}{\sum_{u&#39;\in S_u}sim(u, u&#39;)}$
&lt;p&gt;
&lt;p&gt;where $\bar{r}_u$ is the average rating of user $u$ and $sim(u, u&amp;rsquo;)$ represents the similarity score between user $u$
and $u&amp;rsquo;$. A common choice is to calculate the cosine similarity between two user rating vectors.&lt;/p&gt;
&lt;h4 id=&#34;item-item-filtering&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#item-item-filtering&#34;&gt;
        
    &lt;/a&gt;
    Item Item Filtering
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;Assume that the customers will prefer products that share a high similarity with those already well appreciated. The
missing rating is thus predicted as the weighted average of a set of similar products:&lt;/p&gt;
&lt;p&gt;$$r_{u,i} = \frac{\sum_{j \in P_i}sim(j, i) * r_{u,j}}{\sum_{j \in P_i}sim(j, i)}$$&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s straightforward to observe that user-based filtering is to check what other users think of the same product, while
item-based filtering is aggregating what the user thinks of other items. Essentially, both are weighted linear
combination of observed ratings. Can we do better than weighted average?&lt;/p&gt;
&lt;h3 id=&#34;model-based-cf&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#model-based-cf&#34;&gt;
        
    &lt;/a&gt;
    Model Based CF
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;Unlike the memory based CF which is trying to fill in the missing cells in the rating matrix, model-based collaborative
filtering is to predict user preferences for items based on past interactions. It often relies on the concept of
latent factors. These are hidden features that influence user preferences and item characteristics. By uncovering these
latent factors, the model can predict the likelihood of a user liking an item.&lt;/p&gt;
&lt;p&gt;Let $R$ denotes the user item interaction rating matrix, $R \in \mathbb{R}^{m*n}$, where $m$ is the dimension of user
space and $n$ denotes the dimension of item space.&lt;/p&gt;
&lt;h4 id=&#34;clustering&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#clustering&#34;&gt;
        
    &lt;/a&gt;
    Clustering
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;Given user or item embeddings, a simple approach is to apply the K nearest neighbours to find the K closest users or
items depending on the similarity metrics used.&lt;/p&gt;
&lt;h4 id=&#34;matrix-factorization&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#matrix-factorization&#34;&gt;
        
    &lt;/a&gt;
    Matrix Factorization
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;The basic idea is to decompose the user-item interaction matrix into two lower-dimensional matrices:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;User Matrix (U)&lt;/strong&gt;: Represents users in terms of latent factors, $U \in \mathbb{R}^{m * p}$, where $p$ is the dimension
of the latent space.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Item Matrix (V)&lt;/strong&gt;: Represents items in terms of latent factors, $V \in \mathbb{R}^{n * p}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The interaction matrix $R$ is approximated as the product of these two matrices: $R \approx U \cdot V^T$. In practice, we
are usually optimizing for a weighted matrix factorization objective&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;$$L(\theta) = \sum_{(i, j)\in obs} \omega_{i, j} (R_{i, j} - U_i \cdot V_{j}^{T})^2 + \omega_{0} \sum_{(i, j) \notin obs}
(U_i \cdot V_{j}^{T})^2$$&lt;/p&gt;
&lt;p&gt;where $\omega_0$ is a hyper-parameter that weights the two terms so that the objective is not
dominated by one or the other, and $\omega_{i, j}$ is a function of the frequency of user $i$ and item $j$.&lt;/p&gt;
&lt;p&gt;Common optimization algorithms to minimize the above objective function includes &lt;strong&gt;stochastic gradient descent&lt;/strong&gt; and
&lt;strong&gt;weighted alternating least squares (WALS)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Recently, a few efforts on deep learning have also been proposed on matrix factorization. For example,&lt;/p&gt;
&lt;h5 id=&#34;deep-auto-encoders2&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#deep-auto-encoders2&#34;&gt;
        
    &lt;/a&gt;
    Deep Auto-Encoders&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h5&gt;
&lt;p&gt;It proposed a model called Collaborative Denoising Auto-Encoder (CDAE) that extends the traditional denoising autoencoder
architecture to the collaborative filtering domain. CDAE is designed to handle implicit feedback, where user preferences
are inferred from user behavior rather than explicit ratings. The model incorporates both user-specific and item-specific
factors, leveraging the rich user interaction data to learn better representations for recommendation tasks.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
    &lt;img src=&#34;https://bingzw.github.io/recommendation_model/autoencoder_mf.png&#34;&gt;&lt;br&gt;
    &lt;em&gt;Figure 1: deep auto-encoder&lt;/em&gt;
&lt;/p&gt;
&lt;h5 id=&#34;neural-collaborative-filtering3&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#neural-collaborative-filtering3&#34;&gt;
        
    &lt;/a&gt;
    Neural Collaborative Filtering&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h5&gt;
&lt;p&gt;NCF replaces these linear latent factor models with non-linear neural networks, allowing for a more expressive
representation of user-item interactions. By doing so, NCF can capture more complex patterns in the data that traditional
methods might miss. The core idea is to use multi-layer perceptrons (MLPs) to model the interaction function between
users and items, providing a more flexible and powerful framework for learning user preferences.&lt;/p&gt;
&lt;p&gt;The general architecture of NCF includes embedding layers for users and items, followed by one or more hidden layers
that learn the interaction between these embeddings. The final output layer predicts the user&amp;rsquo;s preference for a given
item. This approach not only improves the accuracy of recommendations but also enables the integration of additional
features, such as user demographics or item attributes, into the model. Refer to the DLRM or DeepFM for more details.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
    &lt;img src=&#34;https://bingzw.github.io/recommendation_model/ncf_mf.png&#34;&gt;&lt;br&gt;
    &lt;em&gt;Figure 2: neural collaborative filtering&lt;/em&gt;
&lt;/p&gt;
&lt;h2 id=&#34;content-based-models-ranking-as-recommendation&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#content-based-models-ranking-as-recommendation&#34;&gt;
        
    &lt;/a&gt;
    Content Based Models (Ranking As Recommendation)
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;Unlike the collaborative filtering that purely relies on user-item interaction data. Content-based recommendation models
focus on the attributes of items to suggest similar items to users based on their past interactions. These models analyze
the content (such as text, keywords, categories, or features) associated with the items and create a profile for each
user based on the features of the items they have shown interest in. Compared with collaborative filtering, it&amp;rsquo;s easier
to handle the cold start issue when item features are known. However, its recommendation may strictly adheres to the
user&amp;rsquo;s profile, potentially limiting diversity. There are quite a few popular model frameworks in this area and we would
focus on the models based on deep neural architecture.&lt;/p&gt;
&lt;h3 id=&#34;wide--deep4&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#wide--deep4&#34;&gt;
        
    &lt;/a&gt;
    Wide &amp;amp; Deep&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;Wide &amp;amp; Deep learning combined wide linear models and deep neural networks to achieve both memorization and generalization.
The model consists of two components:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Wide Component&lt;/strong&gt;: This part is a generalized linear model (GLM) that excels at memorization by capturing feature
interactions using cross-product feature transformations. The wide component is effective at handling sparse features
and explicitly memorizing frequent co-occurrence patterns. It can be represented as:&lt;/p&gt;
&lt;p&gt;$$y_{\text{wide}} = \mathbf{w}^T \mathbf{x} + b $$&lt;/p&gt;
&lt;p&gt;where $\mathbf{x}$ represents the input features, $\mathbf{w}$ represents the learned weights, and $b$ is
the bias term.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Component&lt;/strong&gt;: This part is a feed-forward neural network that excels at generalization by capturing high-level
and non-linear feature interactions. The deep component uses dense embeddings to represent categorical features, and it
learns implicit interactions through multiple hidden layers. The output of the deep component can be represented as:&lt;/p&gt;
&lt;p&gt;$$\mathbf{h}^L = f^L(\mathbf{W}^L \mathbf{h}^{L-1} + \mathbf{b}^L)$$&lt;/p&gt;
&lt;p&gt;where $\mathbf{h}^L$ is the activation of the $L$-th layer, $\mathbf{W}^L$ and $\mathbf{b}^L$ are the weights and biases
of the $L$-th layer, respectively, and $f^L$ is the activation function.
The final output of the deep component, $ y_{\text{deep}} $, is the output of the last layer of the deep neural network. It is given by:&lt;/p&gt;
&lt;p&gt;$$
y_{\text{deep}} = \mathbf{W}^O \mathbf{h}^{L} + \mathbf{b}^O
$$&lt;/p&gt;
&lt;p&gt;where $ \mathbf{W}^O $ and $ \mathbf{b}^O $ are the weights and bias of the output layer, and $ \mathbf{h}^L $ is the activation of the last hidden layer.&lt;/p&gt;
&lt;p&gt;The final prediction is a weighted sum of the outputs from the wide and deep components:&lt;/p&gt;
&lt;p&gt;$$\hat{y} = \sigma(y_{\text{wide}} + y_{\text{deep}})$$&lt;/p&gt;
&lt;p&gt;where $\sigma$ is the sigmoid function used to squash the output to a probability score.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p align=&#34;center&#34;&gt;
   &lt;img src=&#34;https://bingzw.github.io/recommendation_model/wide_deep.png&#34; width=&#34;800&#34; height=&#34;600&#34;&gt;&lt;br&gt;
   &lt;em&gt;Figure 3: wide &amp; deep model&lt;/em&gt;
&lt;/p&gt;
&lt;h3 id=&#34;deepfm5&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#deepfm5&#34;&gt;
        
    &lt;/a&gt;
    DeepFM&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;DeepFM addresses the challenge of capturing both low-order and high-order feature interactions in recommendation systems.
The model consists of two interconnected components:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;FM Component&lt;/strong&gt;: This part captures low-order feature interactions using Factorization Machines, which are effective
for handling sparse data and modeling pairwise feature interactions without manual feature engineering. The FM component
can be represented as:&lt;/p&gt;
&lt;p&gt;$$
y_{\text{FM}} = w_0 + \sum_{i=1}^n w_i x_i + \sum_{i=1}^n \sum_{j=i+1}^n \langle \mathbf{v}_i, \mathbf{v}_j \rangle x_i x_j
$$&lt;/p&gt;
&lt;p&gt;where $w_0$ is the global bias, $w_i$ is the weight of the $i$-th feature, $\mathbf{v}_i$ and $\mathbf{v}_j$ are
latent vectors for the $i$-th and $j$-th features, respectively, and $x_i$ and $x_j$ are input feature values.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Component&lt;/strong&gt;: This part captures high-order feature interactions through a deep neural network. The deep
component uses embeddings to represent input features and learns complex interactions through multiple hidden layers.
The output of the deep component can be represented as:&lt;/p&gt;
&lt;p&gt;$$\mathbf{h}^L = f^L(\mathbf{W}^L \mathbf{h}^{L-1} + \mathbf{b}^L)$$&lt;/p&gt;
&lt;p&gt;where $\mathbf{h}^L$ is the activation of the $L$-th layer, $\mathbf{W}^L$ and $\mathbf{b}^L$ are the weights and
biases of the $L$-th layer, respectively, and $f^L$ is the activation function.
The final output of the deep component, $ y_{\text{deep}} $, is the output of the last layer of the deep neural network. It is given by:&lt;/p&gt;
&lt;p&gt;$$
y_{\text{deep}} = \mathbf{W}^O \mathbf{h}^{L} + \mathbf{b}^O
$$&lt;/p&gt;
&lt;p&gt;where $ \mathbf{W}^O $ and $ \mathbf{b}^O $ are the weights and bias of the output layer, and $ \mathbf{h}^L $ is the
activation of the last hidden layer.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The final prediction is a combination of the outputs from the FM and deep components:&lt;/p&gt;
&lt;p&gt;$$\hat{y} = \sigma(y_{\text{FM}} + y_{\text{deep}})$$&lt;/p&gt;
&lt;p&gt;where $\sigma$ is the sigmoid function used to squash the output to a probability score.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
   &lt;img src=&#34;https://bingzw.github.io/recommendation_model/deepfm.png&#34; width=&#34;600&#34; height=&#34;400&#34;&gt;&lt;br&gt;
   &lt;em&gt;Figure 4: deepfm model&lt;/em&gt;
&lt;/p&gt;
&lt;h3 id=&#34;deep-learning-recommendation-model-dlrm6&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#deep-learning-recommendation-model-dlrm6&#34;&gt;
        
    &lt;/a&gt;
    Deep Learning Recommendation Model (DLRM)&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;Deep Learning Recommendation Model (DLRM) is an advanced machine learning framework designed by Facebook AI to tackle
the complex challenge of personalized recommendations at scale. It is particularly suited for large-scale recommendation
systems in environments such as social media platforms, e-commerce, and online advertising. The DLRM combines the
strengths of collaborative filtering and content-based methods by utilizing both dense and sparse features to provide
highly accurate and scalable recommendations.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
   &lt;img src=&#34;https://bingzw.github.io/recommendation_model/dlrm.png&#34; width=&#34;600&#34; height=&#34;400&#34;&gt;&lt;br&gt;
   &lt;em&gt;Figure 5: deep learning recommendation model&lt;/em&gt;
&lt;/p&gt;
&lt;p&gt;Key components of DLRM includes&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Sparse Features&lt;/strong&gt;: These are categorical variables (e.g., user ID, item ID) which are typically represented using
embeddings. Embeddings transform sparse categorical data into dense vectors of continuous numbers, making them suitable
for neural network processing, denoted as $\mathbf{x}_s[i]$.
The raw sparse features are transformed to sparse embeddings as follows. Let
$$
\mathbf{e}_i = \text{Embedding}(\mathbf{x}_s[i])
$$
where $\mathbf{e}_i$ is the embedding vector for the $i$-th sparse feature, $\text{Embedding}$ denotes an embedding lookup table&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dense Features&lt;/strong&gt;: These are numerical variables (e.g., user age, item price) that are used directly in their raw
form or normalized form, denoted as $\mathbf{x}_d$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Bottom MLP (Multilayer Perceptron)&lt;/strong&gt;: Processes the dense features to capture high-level representations.
$$
\mathbf{h}_d = \text{BottomMLP}(\mathbf{x}_d)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Interaction Layer&lt;/strong&gt;: This layer captures the interactions between different features (both sparse and dense). It
uses a dot product to compute the pairwise interactions among features.
$$
\mathbf{z} = \left[ \mathbf{h}_d, \mathbf{e}_1, \mathbf{e}_2, \ldots, \mathbf{e}_n \right]
$$
where $\mathbf{z}$ is the concatenated vector of dense feature representation and embeddings.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Top MLP&lt;/strong&gt;: Combines the processed dense features and interactions from the interaction layer to make the final prediction.
$$
\hat{y} = \sigma(\text{TopMLP}(\mathbf{z}))
$$
where $\sigma$ is an activation function, typically a sigmoid function for binary classification tasks.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;deep--cross-network-v2-dcn-v27&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#deep--cross-network-v2-dcn-v27&#34;&gt;
        
    &lt;/a&gt;
    Deep &amp;amp; Cross Network v2 (DCN v2)&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;DCN-V2 starts with an embedding layer, followed by a cross network containing multiple cross layers that models explicit
feature interactions, and then combines with a deep network that models implicit feature interactions. The function class
modeled by DCN-V2 is a strict superset of that modeled by DCN. The overall model architecture is depicted in Fig. 5, with
two ways to combine the cross network with the deep network: (1) stacked and (2) parallel.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cross Component&lt;/strong&gt;: The Cross Network V2 enhances the original cross network by introducing a more flexible mechanism
to capture feature interactions. The cross layer in DCN V2 can be represented as:&lt;/p&gt;
&lt;p&gt;$$
x_{l+1} = x_0 \odot (W_l \cdot x_l + b_l) + x_l
$$&lt;/p&gt;
&lt;p&gt;where $x_l$ is the input to the l-th cross layer. $W_l$ and $b_l$ are the weight matrix and bias vector of the l-th
cross layer. $x_{l+1}$ is the output of the $l+1$-th cross layer. $\odot$ is the Hadamard product.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Component&lt;/strong&gt;: The deep network in DCN V2 captures high-order feature interactions through a series of dense layers.
This part of the network learns abstract representations of the input features, enabling the model to generalize well.&lt;/p&gt;
&lt;p&gt;$$
h_{l} = f_{l}(W_{l} h_{l-1} + b_{l})
$$&lt;/p&gt;
&lt;p&gt;where $h_{l}$ is the activation of the $l$-th layer, $W_{l}$ and $b_{l}$ are the weights and
biases of the $l$-th layer, respectively, and $f_{l}$ is the activation function.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The final prediction is a combination of the outputs from either the hidden layer (stacked structure) or the  concatenation
of cross and deep network outputs.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
   &lt;img src=&#34;https://bingzw.github.io/recommendation_model/dcn_v2.png&#34; width=&#34;600&#34; height=&#34;400&#34;&gt;&lt;br&gt;
   &lt;em&gt;Figure 6: deep cross network v2&lt;/em&gt;
&lt;/p&gt;
&lt;h3 id=&#34;deep-interest-network-din8&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#deep-interest-network-din8&#34;&gt;
        
    &lt;/a&gt;
    Deep Interest Network (DIN)&lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;Deep Interest Network (DIN) is a neural network-based model designed for personalized recommendation systems, particularly
in the context of e-commerce and advertising. Unlike traditional recommendation models that primarily focus on user-item interactions,
DIN leverages a user&amp;rsquo;s historical behavior to make more accurate and contextually relevant recommendations. The key
innovation in DIN is its ability to capture user interests dynamically and use this information to influence the recommendation
process.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
   &lt;img src=&#34;https://bingzw.github.io/recommendation_model/din.png&#34; width=&#34;800&#34; height=&#34;600&#34;&gt;&lt;br&gt;
   &lt;em&gt;Figure 7: deep interest network&lt;/em&gt;
&lt;/p&gt;
&lt;p&gt;In particular, the DIN contains the following key components:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Embedding Layer&lt;/strong&gt;: User behaviors and items are represented as dense vectors through an embedding layer. Let
$({e_{1}, e_{2}, \ldots, e_{n}})$ be the sequence of embeddings for user behaviors, and $e_{target}$ be the embedding of the target item.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Local Activation Unit&lt;/strong&gt;: This unit applies the attention mechanism to compute the relevance of each user behavior
in the sequence with respect to the target item.
Let
$$
\alpha_{i} = \frac{\exp(\text{score}(e_{i}, e_{target}))}{\sum_{j=1}^{n} \exp(\text{score}(e_{j}, e_{target}))}
$$
where $\text{score}(e_{i}, e_{target})$ is the activation weight output from a feed-forward
network measuring the relevance of behavior $e_{i}$ to the target item.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Interest Extractor Layer&lt;/strong&gt;: Combines the weighted behavior embeddings to form a user interest representation.
$$
u = \sum_{i=1}^{n} \alpha_{i} e_{i}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Prediction Layer&lt;/strong&gt;: The user interest representation is concatenated with the target item embedding and fed into a neural network to predict the user&amp;rsquo;s interaction with the target item.
$$
\hat{y} = \sigma(W[u, e_{target}, e_{profile}, e_{context}] + b)
$$
where $[u, e_{target}, e_{profile}, e_{context}]$ denotes the concatenation of the user interest representation, profile
embedding, context embedding and the target item embedding, $W$ is a weight matrix, $b$ is a bias term, and $\sigma$ is an activation function (e.g., sigmoid).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;multi-gate-mixture-of-experts-mmoe9&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#multi-gate-mixture-of-experts-mmoe9&#34;&gt;
        
    &lt;/a&gt;
    Multi-gate Mixture of Experts (MMOE)&lt;sup id=&#34;fnref:9&#34;&gt;&lt;a href=&#34;#fn:9&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;Multi-gate Mixture-of-Experts (MMoE) is an advanced multi-task learning (MTL) framework designed to model and leverage
task relationships for improved performance across multiple tasks. The MMoE architecture combines the benefits of
mixture-of-experts models with the flexibility of multi-gate mechanisms, allowing the model to dynamically allocate
computational resources based on the specific needs of each task. This approach is particularly useful in scenarios
where tasks are interrelated and can benefit from shared learning while maintaining task-specific adaptations.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
   &lt;img src=&#34;https://bingzw.github.io/recommendation_model/mmoe.png&#34; width=&#34;800&#34; height=&#34;600&#34;&gt;&lt;br&gt;
   &lt;em&gt;Figure 8: multi-gate mixture of experts&lt;/em&gt;
&lt;/p&gt;
&lt;p&gt;Key components of the model architecture are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Experts&lt;/strong&gt;: A set of neural network sub-models that serve as specialized units for feature extraction. Each expert
is trained to capture different aspects of the input data. Let $\mathbf{x}$ represent the input data, $E$ represent
the number of experts, and $T$ represent the number of tasks. The output from expert is represented as
$$
\mathbf{h}^i = f_e^i(\mathbf{x}), \quad \text{for } i = 1, 2, \ldots, E
$$
where $f_e^i$ is the function of the $i$-th expert, and $\mathbf{h}^i$ is the output of the $i$-th expert.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Multi-gate Mechanism&lt;/strong&gt;: Separate gating networks for each task that dynamically select and weight the contributions
of different experts based on the input data and task requirements.
$$
\mathbf{g}^j = \sigma(\mathbf{W}^j \mathbf{x}), \quad \text{for } j = 1, 2, \ldots, T
$$
where $\mathbf{W}^j$ are the parameters of the gating network for task $j$, and $g^j$ is the gating output distribution
of gating weights to each expert assigned by task $j$. To combine the experts output and gating weights, we have
$$\mathbf{t}^j = \sum_{i=1}^{E} g_i^j \cdot \mathbf{h}^i$$
where $\mathbf{t}^j$ is the combined output for task $j$, and $g_i^j$ is the weight for the $i$-th expert assigned by
the gating network of task $j$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Task-specific Layers&lt;/strong&gt;: Layers that process the combined outputs from the experts, tailored to the specific requirements of each task.
$$
\hat{y}^j = f_o^j(\mathbf{t}^j)
$$
where $f_o^j$ is the task-specific output layer for task $j$, and $\hat{y}^j$ is the predicted output for task $j$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;behavior-sequence-transformer-bst10&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#behavior-sequence-transformer-bst10&#34;&gt;
        
    &lt;/a&gt;
    Behavior Sequence Transformer (BST)&lt;sup id=&#34;fnref:10&#34;&gt;&lt;a href=&#34;#fn:10&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;10&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;The Behavior Sequence Transformer (BST) is a novel neural network architecture designed for modeling user behavior
sequences in recommendation systems. BST leverages the power of Transformer models, which have achieved significant
success in natural language processing (NLP), to capture the sequential patterns and contextual dependencies in user
interactions over time. This approach enhances the ability to predict user preferences and improve recommendation accuracy.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
   &lt;img src=&#34;https://bingzw.github.io/recommendation_model/bst.png&#34; width=&#34;800&#34; height=&#34;600&#34;&gt;&lt;br&gt;
   &lt;em&gt;Figure 9: behavior sequence transformer&lt;/em&gt;
&lt;/p&gt;
&lt;p&gt;Key components of BST include:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Input Layer&lt;/strong&gt;:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;User Behavior Sequence&lt;/strong&gt;: A sequence of items or actions representing user interactions over time.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Contextual Features&lt;/strong&gt;: Additional information such as timestamps, device types, and other relevant context.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Embedding Layer&lt;/strong&gt;:
$$
\mathbf{E}_x = \mathbf{W}_e \cdot \mathbf{x}
$$
where $\mathbf{W}_e$ is the embedding matrix and $\mathbf{x}$ is the input feature vector.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Positional Encoding&lt;/strong&gt;:
$$
\mathbf{E}_p = \text{PE}(pos)
$$
where $\text{PE}(pos)$ is the positional encoding function that adds position-specific information to the embeddings.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Transformer Encoder&lt;/strong&gt;:
$$
\mathbf{H}_i = \text{LayerNorm}(\text{MultiHeadAttention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) + \mathbf{E}_i)
$$
where $\mathbf{Q}$, $\mathbf{K}$, and $\mathbf{V}$ are the query, key, and value matrices, and $\mathbf{E}_i$ is the
input embedding at layer $i$. A Feed-Forward Networks ($\text{FFN}$) is added on top of the self attention layer to further enhance the
model with non-linearity.
$$
\mathbf{O}_i = \text{LayerNorm}(\text{FFN}(\mathbf{H}_i) + \mathbf{H}_i)
$$
In practice, we usually stack multiple transformer layers. So the output of last layer is thus fed as the input of the
next layer.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Output Layer&lt;/strong&gt;:
$$
\hat{y} = \sigma(\mathbf{W}_o \cdot \mathbf{O}_L)
$$
where $\mathbf{W}_o$ is the weight matrix of the output layer, $\mathbf{O}_L$ is the output of the last Transformer encoder layer, and $\sigma$ is the activation function.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#summary&#34;&gt;
        
    &lt;/a&gt;
    Summary
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;So far, we have summarized the main popular recommendation models. These summaries highlight the unique strengths and
specific applications of each recommendation model, reflecting their advancements and contributions to the field. The
modern recommendation systems usually are built with multiple stages and are composed with both simple and complex models.
Users may choose the best model depending on the business requirements and system architecture design.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#citation&#34;&gt;
        
    &lt;/a&gt;
    Citation
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;If you find this post helpful, please consider citing it as:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bibtex&#34; data-lang=&#34;bibtex&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f3f99d&#34;&gt;@article&lt;/span&gt;{&lt;span style=&#34;color:#ff5c57&#34;&gt;wang2024recommendationmodel&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;author&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;Bing Wang&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;title&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;Recommendation Models&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;journal&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;bingzw.github.io&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;year&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;2024&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;month&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;June&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;url&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;https://bingzw.github.io/posts/2024-06-06-recommendation-model/&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;or&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Bing Wang. (2024, June). Recommendation Models. 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;https://bingzw.github.io/posts/2024-06-06-recommendation-model/
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;reference&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#reference&#34;&gt;
        
    &lt;/a&gt;
    Reference
&lt;/div&gt;
&lt;/h2&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://developers.google.com/machine-learning/recommendation/collaborative/matrix&#34;&gt;Collaborative Filtering and Matrix Factorization&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1708.01715&#34;&gt;Kuchaiev, Oleksii, and Boris Ginsburg. &amp;ldquo;Training deep autoencoders for collaborative filtering.&amp;rdquo; arXiv preprint arXiv:1708.01715 (2017)&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1708.05031&#34;&gt;He, Xiangnan, et al. &amp;ldquo;Neural collaborative filtering.&amp;rdquo; Proceedings of the 26th international conference on world wide web. 2017&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1606.07792&#34;&gt;Cheng, Heng-Tze, et al. &amp;ldquo;Wide &amp;amp; deep learning for recommender systems.&amp;rdquo; Proceedings of the 1st workshop on deep learning for recommender systems. 2016&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1703.04247&#34;&gt;Guo, Huifeng, et al. &amp;ldquo;DeepFM: a factorization-machine based neural network for CTR prediction.&amp;rdquo; arXiv preprint arXiv:1703.04247 (2017)&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1906.00091&#34;&gt;Naumov, Maxim, et al. &amp;ldquo;Deep learning recommendation model for personalization and recommendation systems.&amp;rdquo; arXiv preprint arXiv:1906.00091 (2019)&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2008.13535&#34;&gt;Wang, Ruoxi, et al. &amp;ldquo;Dcn v2: Improved deep &amp;amp; cross network and practical lessons for web-scale learning to rank systems.&amp;rdquo; Proceedings of the web conference 2021. 2021&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1706.06978&#34;&gt;Zhou, Guorui, et al. &amp;ldquo;Deep interest network for click-through rate prediction.&amp;rdquo; Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp;amp; data mining. 2018&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:9&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://dl.acm.org/doi/pdf/10.1145/3219819.3220007&#34;&gt;Ma, Jiaqi, et al. &amp;ldquo;Modeling task relationships in multi-task learning with multi-gate mixture-of-experts.&amp;rdquo; Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp;amp; data mining. 2018&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:9&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:10&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1905.06874&#34;&gt;Chen, Qiwei, et al. &amp;ldquo;Behavior sequence transformer for e-commerce recommendation in alibaba.&amp;rdquo; Proceedings of the 1st international workshop on deep learning practice for high-dimensional sparse data. 2019&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:10&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Causality Introduction</title>
      <link>/posts/2024-05-28-causality/</link>
      <pubDate>Tue, 28 May 2024 16:19:16 -0700</pubDate>
      <author>bingwang8878@gamil.com (Bing Wang)</author>
      <guid>/posts/2024-05-28-causality/</guid>
      <description>&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://bingzw.github.io/causality/causality.jpeg&#34; width=&#34;600&#34; height=&#34;400&#34;&gt;&lt;br&gt;
&lt;p&gt;
&lt;p&gt;&lt;em&gt;Image cited from &lt;a href=&#34;https://depositphotos.com/vectors/causality.html&#34;&gt;depositphotos&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;what-is-causality&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#what-is-causality&#34;&gt;
        
    &lt;/a&gt;
    What Is Causality?
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;Different from common ML tasks that focus on summarizing the patterns in the data, expecting to make predictions on
similar unseen data. Causality, on the other hand, is about understanding the underlying mechanisms that drive the data.
It is about to predict how the people/agent/system would react if it were intervened in a certain way. Think about the
example of having two parallel universes with different light, temperature, and humidity conditions. What would happen
to a plant if it were moved from one universe to another? This is a causal question.&lt;/p&gt;
&lt;h3 id=&#34;causal-formulation&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#causal-formulation&#34;&gt;
        
    &lt;/a&gt;
    Causal Formulation
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;To understand how the outcome would change in different conditions, we need to define the following notations to
describe the causal relations.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$Y_i$: the outcome of interest for individual $i$. For example, the health status of a patient.&lt;/li&gt;
&lt;li&gt;$A_i$: an indicator of whether the individual $i$ received the treatment. For example, a new drug.&lt;/li&gt;
&lt;li&gt;$X_i$: a set of covariates that may affect both the treatment assignment and the outcome. For example, health status,
age, gender etc&lt;/li&gt;
&lt;li&gt;$U_i$: unobserved factors that may affect both the treatment assignment and the outcome. For example, genetic factors.&lt;/li&gt;
&lt;li&gt;$Y_i^{a}$: the potential outcome of individual $i$ if he/she received the treatment $A = a, a \in \{0, 1\}$. For
example, the health status of a patient if he/she received the new drug ($a=1$).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In practice, the data $(Y_i, A_i, X_i)$ is usually drawn i.i.d from a population and they are formulated into a causal
graph as shown below&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
    &lt;img src=&#34;https://bingzw.github.io/causality/causal_model_diagram.png&#34; width=&#34;600&#34; height=&#34;400&#34;&gt;&lt;br&gt;
    &lt;em&gt;Figure 1: causal inference elements&lt;/em&gt;
&lt;/p&gt;
&lt;p&gt;The mechanism of the causal graph is often reasoning by structural causal models (SCM). It models the underlying data
generation process with an ordered sequence of functions that map the parent nodes to the child nodes. For example, the
SCMs of the above causal graph can be written as:
$P(Y, A, X) = P(Y | A, X) P(X | A) P(A)$. This equation describes the probabilistic dependencies between the observed
variables. Now the question is how do we define the effect of the new drug $A$ on health outcome $Y$?&lt;/p&gt;
&lt;h3 id=&#34;average-treatment-effect&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#average-treatment-effect&#34;&gt;
        
    &lt;/a&gt;
    Average Treatment Effect
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s continue with the above drug trail context and notations. Consider a simple case with a binary treatment $A$
(1: treated, 0: untreated) and a binary outcome $Y$ (1: recovered, 0: not recovered).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$Y^{a=1}$ (Y under the treatment
a = 1) be the outcome variable that would have been observed if the patient had received the treatment.&lt;/li&gt;
&lt;li&gt;$Y^{a=0}$ (Y
under the treatment a = 0) be the outcome variable that would have been observed if the patient had not received the
treatment.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When estimating the causal effect on the &lt;strong&gt;population&lt;/strong&gt;, it&amp;rsquo;s usually to consider the average treatment effect (ATE),
$ATE = E[Y^{a=1}] - E[Y^{a=0}]$, denoting the average causal effect of the treatment on the outcome.&lt;/p&gt;
&lt;h2 id=&#34;causal-effect-estimation&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#causal-effect-estimation&#34;&gt;
        
    &lt;/a&gt;
    Causal Effect Estimation
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;With the causal formulation and the average treatment effect defined, the next step is to estimate the causal effect
from the observed data. Multiple methods have been developed for this purpose, each relying on different assumptions
and is suitable for different scenarios. Some of the most common methods include: &lt;em&gt;randomized controlled trials (A/B
testing)&lt;/em&gt;, &lt;em&gt;confounder adjustments methods&lt;/em&gt;, &lt;em&gt;instrumental variables&lt;/em&gt;, and &lt;em&gt;difference-in-differences&lt;/em&gt; etc.&lt;/p&gt;
&lt;h3 id=&#34;randomized-controlled-trials-ab-testing&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#randomized-controlled-trials-ab-testing&#34;&gt;
        
    &lt;/a&gt;
    Randomized Controlled Trials (A/B Testing)
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;The most common approach to estimate the causal effect is to run an experiment, usually in the following general steps.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Objective: Define the research question and the hypothesis (usually built with treatment metrics) to be tested.&lt;/li&gt;
&lt;li&gt;Design: Randomly assign the subjects into two groups, the treatment group (A) and the control group (B). The
treatment group receives the treatment, while the control group does not.&lt;/li&gt;
&lt;li&gt;Randomization: Randomly assign the subjects to the treatment and control groups to ensure the groups are comparable.&lt;/li&gt;
&lt;li&gt;Run test: Implement the A/B test and let it run for a sufficient period to gather an adequate amount of data&lt;/li&gt;
&lt;li&gt;Analysis: Analyze the data and calculate the treatment effect.&lt;/li&gt;
&lt;li&gt;Decisions: Make decisions based on the test results.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Randomized controlled trials are considered the gold standard for causal inference because they can eliminate the
confounding bias by randomizing the treatment assignment. However, they are not always feasible due to ethical,
practical, or financial reasons. Therefore, other methods are developed to estimate the causal effect from observational
data.&lt;/p&gt;
&lt;h3 id=&#34;causal-inference---confounder-adjustments-methods&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#causal-inference---confounder-adjustments-methods&#34;&gt;
        
    &lt;/a&gt;
    Causal Inference - Confounder Adjustments Methods
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;Before diving into the confounder adjustments methods, let&amp;rsquo;s first clarify the confounding bias. Consider the following
example:&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
    &lt;img src=&#34;https://bingzw.github.io/causality/confounder_selection_bias.png&#34;&gt;&lt;br&gt;
    &lt;em&gt;Figure 2: confounder example&lt;/em&gt;
&lt;/p&gt;
&lt;p&gt;A study is conducted to investigate the effect of a new drug on the health outcome of patients. The study
finds that patients who received the new drug have a higher recovery rate than those who did not. However, the study
also finds that the patients who received the new drug are younger and healthier than those who did not. Therefore, the
observed effect of the new drug on the health outcome may be &lt;strong&gt;confounded&lt;/strong&gt; by the age and health status of the patients.
To estimate the causal effect of the new drug on the health outcome, we need to &lt;em&gt;adjust&lt;/em&gt; for the confounding factors,
such as age and health status.&lt;/p&gt;
&lt;h4 id=&#34;key-assumptions&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#key-assumptions&#34;&gt;
        
    &lt;/a&gt;
    Key Assumptions
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;All the confounder adjustments methods rely on the following key assumptions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Exchangeability&lt;/strong&gt;: the treatment assignment is independent of the potential outcomes given the covariates.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Positivity&lt;/strong&gt;: the probability of receiving the treatment is positive for all levels of the covariates.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Consistency&lt;/strong&gt;: the individual&amp;rsquo;s potential outcome is the same as the observed outcome when the treatment is received.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;general-flow-of-causal-inference&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#general-flow-of-causal-inference&#34;&gt;
        
    &lt;/a&gt;
    General Flow of Causal Inference
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;In general, causal inference is to build a pseudo-population that mimics the randomized experimentation from the observed
data. The assumptions taken and the modeling approaches applied are to ensure the pseudo-population is unbiased and
consistent with the true population. The general flow of causal inference can be summarized as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Define the Problem&lt;/strong&gt;: What are the causal effect of interest? What are the treatment and outcome variables?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Acquire Data&lt;/strong&gt;: Collect the data that contains the treatment, outcome, and covariates.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Build the Causal Model&lt;/strong&gt;: Fit the causal model that describes the causal relationship&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Estimate the Causal Effect&lt;/strong&gt;: Estimate the counterfactual causal effect using the trained model&lt;/li&gt;
&lt;/ol&gt;
&lt;p align=&#34;center&#34;&gt;
    &lt;img src=&#34;https://bingzw.github.io/causality/causal_inference_flow.png&#34; width=&#34;800&#34; height=&#34;500&#34;&gt;&lt;br&gt;
    &lt;em&gt;Figure 3: Causal Inference Modeling Flow&lt;/em&gt;
&lt;/p&gt;
We follow the same modeling process in figure 3. First defining the outcome, treatment, and covariates, then building the
causal model using the sample data. Next to estimate the counterfactual performance for both treated and untreated in the
treatment and control groups. The dashed figure in the above graph represents the unobserved counterfactuals that are 
estimated from the counterfactual model. Finally, the causal effect is then estimated by comparing the observed outcome 
with the estimated counterfactual outcome.
&lt;p&gt;Let&amp;rsquo;s dive deep into how the causal model&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; can be trained from the observed data.&lt;/p&gt;
&lt;h4 id=&#34;inverse-probability-weighting&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#inverse-probability-weighting&#34;&gt;
        
    &lt;/a&gt;
    Inverse Probability Weighting
&lt;/div&gt;
&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Key Idea: Assigning weights to observations in order to create a pseudo-population that approximates the population
that would have been obtained through random assignment in a randomized control trial (RCT).&lt;/li&gt;
&lt;li&gt;Steps:
&lt;ol&gt;
&lt;li&gt;Estimate the propensity score, $g(x) = P(A = 1 | X = x)$, the probability of receiving the treatment given the
covariates.&lt;/li&gt;
&lt;li&gt;Calculate the inverse probability weights, $w_i = \frac{P(A_i)}{g(x_i)}$.&lt;/li&gt;
&lt;li&gt;Fit a marginal structure model $E[Y^a] = \beta_0 + \beta_1 a$ with the inverse probability weights $w_i$.&lt;/li&gt;
&lt;li&gt;Estimate the average treatment effect, $ATE = \hat{\beta_1}$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;standardization&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#standardization&#34;&gt;
        
    &lt;/a&gt;
    Standardization
&lt;/div&gt;
&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Key Idea: Standardize the outcome under different treatment conditions to estimate the average treatment effect.&lt;/li&gt;
&lt;li&gt;Steps:
&lt;ol&gt;
&lt;li&gt;Fit the outcome model with treatment and covariates, $P[Y^a | X=x] = P(Y | A=a, X=x)$. For example, the linear model
representation is $Y^A = \beta_0 + \beta_1 A + \beta_2 X + \epsilon$.&lt;/li&gt;
&lt;li&gt;Estimate the expectation of the outcome under different treatment conditions:
$E[Y^a] = \int_x P(Y|A=a, x)*f_X(x)dx$.&lt;/li&gt;
&lt;li&gt;Calculate the average treatment effect, $ATE = E[Y^{a=1}] - E[Y^{a=0}]$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;stratification&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#stratification&#34;&gt;
        
    &lt;/a&gt;
    Stratification
&lt;/div&gt;
&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Key Idea: Stratify the data into different strata based on the covariates and estimate the average treatment effect.&lt;/li&gt;
&lt;li&gt;Steps:
&lt;ol&gt;
&lt;li&gt;Estimate the propensity score, $g(x) = P(A = 1 | X = x)$, the probability of receiving the treatment given the
covariates.&lt;/li&gt;
&lt;li&gt;Fit the outcome model with treatment and propensity score as the covariates,
$P[Y^a | X=x] = P(Y | A=a, g(x))$. A simple linear representation is $Y^A = \beta_0 + \beta_1 A + \beta_2 * g(X) + \epsilon$.&lt;/li&gt;
&lt;li&gt;Estimate the expectation of the outcome under different treatment conditions:
$E[Y^a] = \int_x P(Y|A=a, g(x))*f_X(x)dx$&lt;/li&gt;
&lt;li&gt;Calculate the average treatment effect, $ATE = E[Y^{a=1}] - E[Y^{a=0}]$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;propensity-score-matching&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#propensity-score-matching&#34;&gt;
        
    &lt;/a&gt;
    Propensity Score Matching
&lt;/div&gt;
&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Key Idea: Match the treated and control units based on the propensity score to estimate the average treatment effect.&lt;/li&gt;
&lt;li&gt;Steps:
&lt;ol&gt;
&lt;li&gt;Estimate the propensity score, $g(x) = P(A = 1 | X = x)$, the probability of receiving the treatment given the
covariates.&lt;/li&gt;
&lt;li&gt;Match the treated and control units based on the propensity score. (Nearest Neighbour, Kernel Matching, etc)&lt;/li&gt;
&lt;li&gt;Estimate the average treatment effect, $ATE = E[Y^{a=1}] - E[Y^{a=0}]$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;double-machine-learning-dml&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#double-machine-learning-dml&#34;&gt;
        
    &lt;/a&gt;
    Double Machine Learning (DML)
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;In all above methods, we have observed that two components are needed to estimate the causal effect: the propensity score
$g(x) = P(A = 1 | X = x)$ and the outcome model $P[Y^a | X=x] = P(Y | A=a, X=x)$. However, the estimation of these
components may be biased, leading to biased estimates of the causal effect. To address this issue, double machine learning
(DML) is proposed.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Key Idea: Use machine learning algorithms to estimate the nuisance functions and statistical estimation techniques to
estimate the causal effect.&lt;/li&gt;
&lt;li&gt;Steps:
&lt;ol&gt;
&lt;li&gt;Divide the data into K-folds (cross fitting). For each fold $i$, train the propensity score model and the outcome model on
folds other than $i$, i.e. $g^{-i}(x)$ and $P^{-i}(Y^a | X)$.&lt;/li&gt;
&lt;li&gt;Estimating the augmented inverse probability of treatment weighted estimator (AIPTW) for fold $i$:
$\hat{\tau_i} = \frac{1}{n_i} \sum_{i} \hat{P}^{-i}(Y^{a=1} | x_i) - \hat{P}^{-i}(Y^{a=0} | x_i) + A_i\frac{Y_i - \hat{P}^{-i}(Y^{a=1} | x_i)}{\hat{g}^{-i}(x_i)} - (1-A_i)\frac{Y_i - \hat{P}^{-i}(Y^{a=0} | x_i)}{1-\hat{g}^{-i}(x_i)}$&lt;/li&gt;
&lt;li&gt;Aggreagate the estimates from all folds to get the final estimate of the causal effect, $\hat{\tau} = \frac{1}{N} \sum_{i} n_i \hat{\tau_i}$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;causal-inference---instrumental-variables&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#causal-inference---instrumental-variables&#34;&gt;
        
    &lt;/a&gt;
    Causal Inference - Instrumental Variables
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;All above approaches rely on the assumption of no unobserved confounding, which may not be satisfied in practice. When
there is potential endogeneity or unobserved confounding in observational data, instrumental variables can be used to
estimate the causal effect.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Key Idea: Estimating the causal effect of a treatment or intervention when there is potential endogeneity or
unobserved confounding in observational data. It relies on the use of an instrumental variable, which is a variable
that is correlated with the treatment but not directly with the outcome, except through its influence on the treatment.&lt;/li&gt;
&lt;li&gt;Assumptions:
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Relevance&lt;/strong&gt;: the instrumental variable is correlated with the treatment.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Exogeneity&lt;/strong&gt;: the instrumental variable is independent of the unobserved confounders.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Exclusion Restriction&lt;/strong&gt;: the instrumental variable affects the outcome only through the treatment.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Steps:
&lt;ol&gt;
&lt;li&gt;Estimate the first-stage regression: $A = \alpha_0 + \alpha_1 Z + \epsilon_1$.&lt;/li&gt;
&lt;li&gt;Estimate the second-stage regression: $Y = \beta_0 + \beta_1 \hat{A} + \epsilon_2$.&lt;/li&gt;
&lt;li&gt;Calculate the average treatment effect, $ATE = \hat{\beta_1}$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So far, we have discussed the major methods for estimating the causal effect from observational data. Each method has its
own assumptions and limitations, there is no clear winner and the choice of method depends on the specific context and
data characteristics. In practice, it is often recommended to use multiple methods to estimate the causal effect and
compare the results to ensure the robustness of the conclusions.&lt;/p&gt;
&lt;h2 id=&#34;root-cause-optimization&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#root-cause-optimization&#34;&gt;
        
    &lt;/a&gt;
    Root Cause Optimization
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;In the above sections, we designed a hypothetical causality problem of estimating the effect of taking a new drug. The
treatment variable $A$ can be defined as $A_i = 1$ if the patient takes the new drug and $A_i = 0$ if the patient does not.
More generally, the treatment variable can be formulated in this way $A = 1_{{f(\theta) &amp;lt; \mu}}$, where $f(\theta)$ is
the function that maps the treatment parameters $\theta$ to the treatment assignment, and $\mu$ is the threshold that
determines the treatment assignment. $f(\theta)$ is often a known metric since causal inference usually starts with some
suspected causal mechanisms. However, the threshold $\mu$ is often unknown and needs to be optimized to maximize the
causal effect. Therefore, the root cause optimization is to find the optimal threshold $\mu$ that maximizes the causal
effect.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
$argmax_{\mu} E[Y^{A=1}] - E[Y^{A=0}]$, 
where $A = 1_{\{f(\theta) &lt; \mu\}}$ with given $f(\theta)$
&lt;p&gt;
&lt;p&gt;To solve the optimization problem, we can use the causal inference methods discussed above to estimate the causal effect
under different thresholds and find the optimal threshold that maximizes the causal effect. Popular optimization algorithms
such as grid search, random search and bayesian optimization can be used to find the optimal threshold.&lt;/p&gt;
&lt;h2 id=&#34;appendix&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#appendix&#34;&gt;
        
    &lt;/a&gt;
    Appendix
&lt;/div&gt;
&lt;/h2&gt;
&lt;h3 id=&#34;do-calculus&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#do-calculus&#34;&gt;
        
    &lt;/a&gt;
    Do Calculus
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;While we formulate the causal effect from the counterfactual perspective, there is another powerful tool that called
do-calculus that estimates the causal effect by setting interventions on the treatment variable. In particular, it&amp;rsquo;s
trying to apply interventions $A=a$ to the causal graph that deletes the edges pointing to $A$, and then conditioning on
the observed variables to estimate the causal effect. The average treatment effect can be written as:&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
$ATE = E[Y^{a=1}] - E[Y^{a=0}] = E[Y|do(A=1)] - E[Y|do(A=0)]$
&lt;p&gt;
&lt;p&gt;Please refer to the do-calculus literature&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; for more details.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#citation&#34;&gt;
        
    &lt;/a&gt;
    Citation
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;If you find this post helpful, please consider citing it as:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bibtex&#34; data-lang=&#34;bibtex&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f3f99d&#34;&gt;@article&lt;/span&gt;{&lt;span style=&#34;color:#ff5c57&#34;&gt;wang2024causality&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;author&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;Bing Wang&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;title&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;Causality Introduction&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;journal&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;bingzw.github.io&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;year&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;2024&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;month&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;May&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;url&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;https://bingzw.github.io/posts/2024-05-28-causality/&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;or&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Bing Wang. (2024, May). Causality Introduction. 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;https://bingzw.github.io/posts/2024-05-28-causality/
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;reference&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#reference&#34;&gt;
        
    &lt;/a&gt;
    Reference
&lt;/div&gt;
&lt;/h2&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Od6oAz1Op2k&#34;&gt;Causal Inference: Explained&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/&#34;&gt;Hernán MA, Robins JM (2020). Causal Inference: What If. Boca Raton: Chapman &amp;amp; Hall/CRC.&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;Pearl, Judea (2000). Causality: Models, Reasoning, and Inference.&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
    </item>
    
  </channel>
</rss>

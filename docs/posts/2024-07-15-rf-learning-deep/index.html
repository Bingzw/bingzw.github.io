<!DOCTYPE html>


<html lang="en-us" data-theme="">
<head>
    
        
<meta charset="utf-8">
<meta name="HandheldFriendly" content="True">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer-when-downgrade">

<title>A Journey to Reinforcement Learning - Deep RL - Bingz Learning Blog</title>

<meta name="description" content="An summary of deep reinforcement learning algorithms">





<link rel="icon" type="image/x-icon" href="https://bingzw.github.io/favicon.ico">
<link rel="apple-touch-icon-precomposed" href="https://bingzw.github.io/favicon.png">






    



    



<style>
  body {
    visibility: hidden;
    opacity: 0;
  }
</style>

<noscript>
  <style>
    body {
      visibility: visible;
      opacity: 1;
    }
  </style>
</noscript>




    





    
    
        
    
    

    
        <link rel="stylesheet" href="/css/style.min.184a655c5ad8596648622468e6696abf0cf0a2cf8266df17b4f7a36fe9c97551.css" integrity="sha256-GEplXFrYWWZIYiRo5mlqvwzwos&#43;CZt8XtPejb&#43;nJdVE=">
    





    





    
    
        
    
    

    
        <link rel="stylesheet" href="/css/style.min.c4c04b3ef88e3d619ad4c7ee5e03048422bc55c4fefdc1f07657c1133670aa22.css" integrity="sha256-xMBLPviOPWGa1MfuXgMEhCK8VcT&#43;/cHwdlfBEzZwqiI=">
    





    





    
    
        
    
    

    
        <link rel="stylesheet" href="/css/style.min.21c5d8fe0a79d623b0adc1ce4bd4f6dd2c05cd939c9aaaa966ba7186b1464f4d.css" integrity="sha256-IcXY/gp51iOwrcHOS9T23SwFzZOcmqqpZrpxhrFGT00=">
    












    

    





    
    
        
    
    

    
        <script src="/js/script.min.08f04d96386c73c9bf4d160333f8f448c05a6e01c06770542ee0e013954ce930.js" type="text/javascript" charset="utf-8" integrity="sha256-CPBNljhsc8m/TRYDM/j0SMBabgHAZ3BULuDgE5VM6TA="></script>
    













    
  
    
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-3P52NT65ZQ"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-3P52NT65ZQ');
        }
      </script>
    
  









    
</head>
<body>
    <a class="skip-main" href="#main">Skip to main content</a>
    <div class="container">
        <header class="common-header">
            
                <div class="header-top">
    <div class="header-top-left">
        <h1 class="site-title noselect">
    <a href="/">Bingz Learning Blog</a>
</h1>

        



    



    



    
        <div class="theme-switcher">
            <span class="inline-svg">

    


    
    
    
    
    

    <svg  xmlns="http://www.w3.org/2000/svg"  width="24"  height="24"  viewBox="0 0 24 24"  fill="none"  stroke="currentColor"  stroke-width="2"  stroke-linecap="round"  stroke-linejoin="round"  class="icon icon-tabler icons-tabler-outline icon-tabler-sun-high"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M14.828 14.828a4 4 0 1 0 -5.656 -5.656a4 4 0 0 0 5.656 5.656z" /><path d="M6.343 17.657l-1.414 1.414" /><path d="M6.343 6.343l-1.414 -1.414" /><path d="M17.657 6.343l1.414 -1.414" /><path d="M17.657 17.657l1.414 1.414" /><path d="M4 12h-2" /><path d="M12 4v-2" /><path d="M20 12h2" /><path d="M12 20v2" /></svg>


</span>

        </div>
    

    <script>
        const STORAGE_KEY = 'user-color-scheme'
        const defaultTheme = "auto"

        let currentTheme
        let switchButton
        let autoDefinedScheme = window.matchMedia('(prefers-color-scheme: dark)')

        function switchTheme(e) {
            currentTheme = (currentTheme === 'dark') ? 'light' : 'dark';
            if (localStorage) localStorage.setItem(STORAGE_KEY, currentTheme);
            document.documentElement.setAttribute('data-theme', currentTheme);
            changeGiscusTheme(currentTheme);
            document.body.dispatchEvent(new CustomEvent(currentTheme + "-theme-set"));
        }

        const autoChangeScheme = e => {
            currentTheme = e.matches ? 'dark' : 'light'
            document.documentElement.setAttribute('data-theme', currentTheme);
            changeGiscusTheme(currentTheme);
            document.body.dispatchEvent(new CustomEvent(currentTheme + "-theme-set"));
        }

        document.addEventListener('DOMContentLoaded', function () {
            switchButton = document.querySelector('.theme-switcher')
            currentTheme = detectCurrentScheme()

            if (currentTheme === 'auto') {
                autoChangeScheme(autoDefinedScheme);
                autoDefinedScheme.addListener(autoChangeScheme);
            } else {
                document.documentElement.setAttribute('data-theme', currentTheme)
            }

            if (switchButton) {
                switchButton.addEventListener('click', switchTheme, false)
            }

            showContent();
        })

        function detectCurrentScheme() {
            if (localStorage !== null && localStorage.getItem(STORAGE_KEY)) {
                return localStorage.getItem(STORAGE_KEY)
            }
            if (defaultTheme) {
                return defaultTheme
            }
            return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
        }

        function showContent() {
            document.body.style.visibility = 'visible';
            document.body.style.opacity = 1;
        }

        function changeGiscusTheme (theme) {
            function sendMessage(message) {
              const iframe = document.querySelector('iframe.giscus-frame');
              if (!iframe) return;
              iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
            }

            sendMessage({
              setConfig: {
                theme: theme
              }
            });
        }
    </script>


        <ul class="social-icons noselect">


    
        
        
        <li>
            <a href="https://github.com/bingzw" title="Github" rel="me">
            <span class="inline-svg">

    


    
    
    
    
    

    <svg  xmlns="http://www.w3.org/2000/svg"  width="24"  height="24"  viewBox="0 0 24 24"  fill="none"  stroke="currentColor"  stroke-width="2"  stroke-linecap="round"  stroke-linejoin="round"  class="icon icon-tabler icons-tabler-outline icon-tabler-brand-github"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4 -4.3 -2.5 -6 -3m12 5v-3.5c0 -1 .1 -1.4 -.5 -2c2.8 -.3 5.5 -1.4 5.5 -6a4.6 4.6 0 0 0 -1.3 -3.2a4.2 4.2 0 0 0 -.1 -3.2s-1.1 -.3 -3.5 1.3a12.3 12.3 0 0 0 -6.2 0c-2.4 -1.6 -3.5 -1.3 -3.5 -1.3a4.2 4.2 0 0 0 -.1 3.2a4.6 4.6 0 0 0 -1.3 3.2c0 4.6 2.7 5.7 5.5 6c-.6 .6 -.6 1.2 -.5 2v3.5" /></svg>


</span>

            </a>
        </li>
    

    
        <li>
            <a href="https://www.linkedin.com/in/bingzw/" title="Linkedin" rel="me">
            <span class="inline-svg">

    


    
    
    
    
    

    <svg  xmlns="http://www.w3.org/2000/svg"  width="24"  height="24"  viewBox="0 0 24 24"  fill="none"  stroke="currentColor"  stroke-width="2"  stroke-linecap="round"  stroke-linejoin="round"  class="icon icon-tabler icons-tabler-outline icon-tabler-brand-linkedin"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 4m0 2a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v12a2 2 0 0 1 -2 2h-12a2 2 0 0 1 -2 -2z" /><path d="M8 11l0 5" /><path d="M8 8l0 .01" /><path d="M12 16l0 -5" /><path d="M16 16v-3a2 2 0 0 0 -4 0" /></svg>


</span>

            </a>
        </li>
    






    <li>
            <a href="/index.xml" title="RSS" rel="me">
            <span class="inline-svg">

    


    
    
    
    
    

    <svg  xmlns="http://www.w3.org/2000/svg"  width="24"  height="24"  viewBox="0 0 24 24"  fill="none"  stroke="currentColor"  stroke-width="2"  stroke-linecap="round"  stroke-linejoin="round"  class="icon icon-tabler icons-tabler-outline icon-tabler-rss"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M5 19m-1 0a1 1 0 1 0 2 0a1 1 0 1 0 -2 0" /><path d="M4 4a16 16 0 0 1 16 16" /><path d="M4 11a9 9 0 0 1 9 9" /></svg>


</span>

            </a>
        </li>
    

</ul>

    </div>
    <div class="header-top-right">

    </div>
</div>


    <nav class="noselect">
        
        
        <a class="" href="https://bingzw.github.io/" title="">Home</a>
        
        <a class="" href="https://bingzw.github.io/about/" title="">About</a>
        
        <a class="" href="https://bingzw.github.io/tags/" title="">Tags</a>
        
        <a class="" href="https://bingzw.github.io/posts/" title="">Archive</a>
        
    </nav>



<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>





            
        </header>
        <main id="main" tabindex="-1">
            
    

    <article class="post h-entry">
        <div class="post-header">
            <header>
                <h1 class="p-name post-title">A Journey to Reinforcement Learning - Deep RL</h1>
                

            </header>
            



<div class="post-info noselect">
    
        <div class="post-date dt-published">
            <time datetime="2024-07-15">2024-07-15</time>
            
        </div>
    

    <a class="post-hidden-url u-url" href="/posts/2024-07-15-rf-learning-deep/">/posts/2024-07-15-rf-learning-deep/</a>
    <a href="https://bingzw.github.io/" class="p-name p-author post-hidden-author h-card" rel="me"></a>


    <div class="post-taxonomies">
        
        
            <ul class="post-tags">
                
                    
                    <li><a href="/tags/machine-learning/">#Machine Learning</a></li>
                
                    
                    <li><a href="/tags/deep-reinforcement-learning/">#Deep Reinforcement Learning</a></li>
                
            </ul>
        
        
    </div>
</div>

        </div>
        

  
  




  
  
  
  <details class="toc noselect">
    <summary>Table of Contents</summary>
    <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#model-free---when-the-environment-is-unknown">Model Free - when the environment is unknown</a>
      <ul>
        <li><a href="#value-based-continuing-">Value Based (Continuing &hellip;)</a>
          <ul>
            <li><a href="#deep-q-network-dqn">Deep Q Network (DQN)</a></li>
            <li><a href="#advanced-dqn">Advanced DQN</a>
              <ul>
                <li><a href="#double-dqn">Double DQN</a></li>
                <li><a href="#duel-dqn">Duel DQN</a></li>
                <li><a href="#prioritized-experience-replay">Prioritized Experience Replay</a></li>
                <li><a href="#noisy-nets">Noisy Nets</a></li>
                <li><a href="#distributional-dqn">Distributional DQN</a></li>
                <li><a href="#rainbow-dqn-4">Rainbow DQN </a></li>
              </ul>
            </li>
          </ul>
        </li>
        <li><a href="#policy-based">Policy Based</a>
          <ul>
            <li><a href="#policy-gradient">Policy Gradient</a></li>
            <li><a href="#reinforce">REINFORCE</a></li>
            <li><a href="#trust-region-policy-optimization-trpo">Trust Region Policy Optimization (TRPO)</a></li>
            <li><a href="#ppo">PPO</a></li>
            <li><a href="#cross-entropy-method-gradient-free">Cross-Entropy Method [Gradient Free]</a></li>
            <li><a href="#evolution-strategy-gradient-free">Evolution Strategy [Gradient Free]</a></li>
          </ul>
        </li>
        <li><a href="#hybrid">Hybrid</a>
          <ul>
            <li><a href="#ddpq">DDPQ</a></li>
            <li><a href="#actor-critic-ac">Actor Critic (AC)</a></li>
            <li><a href="#sac">SAC</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#reference">Reference</a></li>
  </ul>
</nav></div>
  </details>
  



<script>
  var toc = document.querySelector(".toc");
  if (toc) {
    toc.addEventListener("click", function () {
      if (event.target.tagName !== "A") {
        event.preventDefault();
        if (this.open) {
          this.open = false;
          this.classList.remove("expanded");
        } else {
          this.open = true;
          this.classList.add("expanded");
        }
      }
    });
  }
</script>

        <div class="content e-content">
            <p align="center">
<img src="/rf/deeprl.png" width="600" height="400"><br>
<p>
<p><em>Image cited from <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></em></p>
<p>This blog is a continuation of the last one: <strong>A Journey to Reinforcement Learning - Tabular Methods</strong>. The
table of content structure and notations follow the same framework. Let&rsquo;s continue the journey from the last value based
algorithm in model free setting.</p>
<h2 id="model-free---when-the-environment-is-unknown" >
<div>
    <a href="#model-free---when-the-environment-is-unknown">
        
    </a>
    Model Free - when the environment is unknown
</div>
</h2>
<h3 id="value-based-continuing-" >
<div>
    <a href="#value-based-continuing-">
        
    </a>
    Value Based (Continuing &hellip;)
</div>
</h3>
<h4 id="deep-q-network-dqn" >
<div>
    <a href="#deep-q-network-dqn">
        
    </a>
    Deep Q Network (DQN)
</div>
</h4>
<p>Like what have been discussed in SARSA and $Q$ learning, they may not be a good fit when state or action space is too large
or even continuous. Maintaining a $Q$ table is not feasible in such cases. Instead, we can apply function approximation to
the $Q$ value. So far, the model powerful approach that can approximate almost any shape of function is <strong>Neural Network</strong>.
Welcome to the realm of <strong>deep reinforcement learning</strong> (short for &ldquo;deep learning&rdquo; + &ldquo;reinforcement learning&rdquo;).</p>
<p>The idea seems to be natural. Let&rsquo;s refine the DQN framework step by step. As an extension from $Q$ learning. Let&rsquo;s frame the
problem with <strong>continuous states</strong> and <strong>discrete actions</strong> (the continuous actions case will be discussed later). The $Q$ value can
be approximated by fitting a neural network where the state is fed into the network and get output of $Q$ value at each action.</p>
<p align="center">
<img src="/rf/q_dqn.png" width="600" height="400"><br>
<em>Figure 1: Q learning vs DQN</em>
<p>
<p><em>Image cited from <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></em></p>
<p>Given the network, what&rsquo;s the loss function? From the basic $Q$ learning, the $Q$ value is updated
via $Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a&rsquo;}Q(s&rsquo;, a&rsquo;) - Q(s, a)]$. This indicates that we are actually
&ldquo;learning&rdquo; the target $r + \gamma \max_{a&rsquo;}Q(s&rsquo;, a&rsquo;)$. Therefore, the loss function can be defined as
$$
L(\theta, x) = \frac{1}{N}\sum_{i=1}^N[Q_{\theta}(s_i, a_i) - (r_i + \arg \max_{a&rsquo;}Q_{\theta}(s&rsquo;, a&rsquo;))]^2 \tag{3.4}
$$
given a sample $x = (s_i, a_i, r_i, s&rsquo;_i)$.</p>
<p>When applying the backpropagation on (3.4), the target value is changing since it also depends on the parameter $\theta$
to be optimized. This differs from the typical deep learning where the label is given and fixed. Learning from a changing label
problem may be very unstable and loss may fluctuate. To tackle this, the target $Q$ network is introduced.</p>
<p>The idea is to maintain two $Q$ neural nets: $Q_\theta$ and $Q_{\theta ^ *}$. The target net $Q_{\theta ^ *}$ is originally cloned from
the net $Q_\theta$ and the (3.4) is reformatted as</p>
<p>$$
L(\theta, x) = \frac{1}{N}\sum_{i=1}^N[Q_{\theta}(s_i, a_i) - (r_i + \arg \max_{a&rsquo;}Q_{\theta ^ *}(s&rsquo;, a&rsquo;))]^2 \tag{3.5}
$$</p>
<p>The parameter $\theta ^ *$ is fixed in (3.5) and only copied from $Q_\theta$ periodically after a few gradient descent runs. In
this way, the loss function is firstly optimized towards a fixed label (only $\theta$ is updated), then labels are updated (copy
$\theta$ to $\theta ^ *$) and optimization continues.</p>
<p>Gathering the training samples purely from interacting with the system results in dependent samples due to the Markov property. In DQN,
An <strong>experience replay</strong> (ER) component introduce a <em>replay buffer</em> that is sampling independent samples $x = (s_i, a_i, r_i, s&rsquo;_i)$,
thus also improving the training efficiency.</p>
<p>A general structure of DQN is</p>
<p align="center">
<img src="/rf/dqn_flow.png" width="600" height="400"><br>
<em>Figure 2: DQN flow</em>
<p>
<p><em>Image cited from <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></em></p>
<ul>
<li>Initialize the $Q_{\theta}$</li>
<li>Initialize the target network $Q_{\theta ^ *}$ by cloning $\theta ^ *$ from $\theta$</li>
<li>Initialize the replay buffer</li>
<li>for episode $e$ from 1 to $K$, do:
<ul>
<li>Initialize the state $s$</li>
<li>for $t$ from 1 to $T$, do:
<ul>
<li>apply the $\epsilon$-greedy strategy to $Q_{\theta}$ to choose the action $a$ based on $s$</li>
<li>take the action and interact with the environment to get reward $r$ and $s'$</li>
<li>save the sample (s, a, r, s&rsquo;) to the replay buffer</li>
<li>if there are enough samples in the replay buffer, sample N data points $\{(s_i, s_i, r_i, s_{i+1})\}_{i=1,\dots,N}$</li>
<li>for each sample, calculate the target $y_i = r_i + \gamma \arg\max_{a} Q_{\theta ^ *}(s_{i+1}, a)$</li>
<li>minimize the loss function $L(\theta, x) = \frac{1}{N}\sum_{i=1}^N[y_i - Q_{\theta}(s_i, a_i)]^2$, update $Q_\theta$</li>
<li>Update $Q_{\theta ^ *}$ every $M$ steps</li>
</ul>
</li>
<li>end for</li>
</ul>
</li>
<li>end for</li>
<li>return $Q_{\theta}$ and derive the optimal policy</li>
</ul>
<h4 id="advanced-dqn" >
<div>
    <a href="#advanced-dqn">
        
    </a>
    Advanced DQN
</div>
</h4>
<p>Though DQN extends the problem solving capability from finite states to infinite state space, original DQN still suffers
from issues like overestimation. Below lists the efforts to further refine the DQN.</p>
<h5 id="double-dqn" >
<div>
    <a href="#double-dqn">
        
    </a>
    Double DQN
</div>
</h5>
<p><em>Problem Addressed</em>: DQN suffers from overestimation bias when updating Q-values because it uses the same network to
select and evaluate actions.</p>
<p><em>Approach</em>: Double DQN separates action selection and action evaluation by using the training network to select actions and
the target network to evaluate them. This reduces overestimation bias and leads to more accurate Q-value predictions.</p>
<h5 id="duel-dqn" >
<div>
    <a href="#duel-dqn">
        
    </a>
    Duel DQN
</div>
</h5>
<p><em>Problem Addressed</em>: Traditional DQNs struggle to efficiently learn the value of states without requiring specific
action-value pairs.</p>
<p><em>Approach</em>: Introduces a dueling network architecture that separates the estimation of state values and advantage functions.
This allows the model to learn which states are valuable without needing to learn the effect of each action in those states,
leading to improved learning efficiency.</p>
<h5 id="prioritized-experience-replay" >
<div>
    <a href="#prioritized-experience-replay">
        
    </a>
    Prioritized Experience Replay
</div>
</h5>
<p><em>Problem Addressed</em>: In standard experience replay, all transitions are sampled with equal probability, which can be
inefficient as some experiences are more valuable for learning.</p>
<p><em>Approach</em>: Prioritizes sampling of experiences that have higher expected learning progress, meaning transitions with
a higher TD error are sampled more frequently. This focuses learning on more informative samples, speeding up
the learning process.</p>
<h5 id="noisy-nets" >
<div>
    <a href="#noisy-nets">
        
    </a>
    Noisy Nets
</div>
</h5>
<p><em>Problem Addressed</em>: Standard exploration methods (like epsilon-greedy) can be suboptimal or inefficient.</p>
<p><em>Approach</em>: Replaces deterministic parameters in the neural network with parameterized noise, allowing the network to explore more
effectively by adding noise directly to the weights. This leads to more effective exploration strategies that adapt over time.</p>
<h5 id="distributional-dqn" >
<div>
    <a href="#distributional-dqn">
        
    </a>
    Distributional DQN
</div>
</h5>
<p><em>Problem Addressed</em>: Traditional DQNs focus on learning the expected return, which can overlook valuable distributional
information about the returns.</p>
<p><em>Approach</em>: Instead of estimating the expected value, estimate the distribution of the $Q$ function. This is to capture
the uncertainty of the $Q$ function. The $Q$ function is then calculated as the expected value of the distribution.</p>
<h5 id="rainbow-dqn-4" >
<div>
    <a href="#rainbow-dqn-4">
        
    </a>
    Rainbow DQN <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>
</div>
</h5>
<p><em>Problem Addressed</em>: Individual improvements to DQN (like those above) each address specific limitations, but combining
them could lead to even more robust performance.</p>
<p><em>Approach</em>: Integrates several enhancements to DQN into a single framework: Double DQN, Duel DQN, Prioritized Experience Replay,
Noisy Nets, Distributional Q-learning, and a few others. By combining these techniques, Rainbow DQN leverages the strengths of
each approach to achieve superior performance in a unified model.</p>
<p align="center">
<img src="/rf/rainbow.png" width="500" height="100"><br>
<em>Figure 3: Rainbow DQN</em>
<p>
<p><em>Image cited from <sup id="fnref1:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup></em></p>
<h3 id="policy-based" >
<div>
    <a href="#policy-based">
        
    </a>
    Policy Based
</div>
</h3>
<p>Value-based methods (like Q-learning and DQN) focus on estimating the value of state-action pairs to derive a policy.
However, these methods can struggle with high-dimensional or continuous <strong>action spaces</strong>. Policy-based methods address
these limitations by directly learning the policy itself. In policy based approaches, policies are often represented
using parameterized functions, such as neural networks. These functions map states directly to probabilities over actions,
allowing the agent to choose actions based on the learned distribution.</p>
<h4 id="policy-gradient" >
<div>
    <a href="#policy-gradient">
        
    </a>
    Policy Gradient
</div>
</h4>
<p>Suppose $\pi_{\theta}$ is the parameterized policy that is to be optimized, our objective is to maximize the expected
cumulative reward from any starting state</p>
<p align="center">
$$
J(\theta) = \mathbb{E}_{s_0}[V_{\pi_{\theta}}(s_0)] \tag{4.1}
$$
</p>
<p>where the expectation is taken over state. Let&rsquo;s derive the derivative of (4.1).</p>
<p align="center">
\begin{aligned}
\nabla_{\theta} V_{\pi_{\theta}}(s) &= \nabla_{\theta}  \sum_{a \in A} \pi_{\theta}(a \mid s)Q_{\pi_{\theta}}(s, a)\\
&= \sum_{a \in A} (\nabla_{\theta} \pi_{\theta}(a \mid s)Q_{\pi_{\theta}}(s, a) + \pi_{\theta}(a \mid s) \nabla_{\theta} Q_{\pi_{\theta}}(s, a)) \\
&= \sum_{a \in A} (\nabla_{\theta} \pi_{\theta}(a \mid s)Q_{\pi_{\theta}}(s, a) + \pi_{\theta}(a \mid s) \nabla_{\theta} \sum_{s', r} p(s', r \mid s, a)(r + \gamma V_{\pi_{\theta}}(s')) \\
&= \sum_{a \in A} (\nabla_{\theta} \pi_{\theta}(a \mid s)Q_{\pi_{\theta}}(s, a) + \gamma\pi_{\theta}(a \mid s) \sum_{s', r} p(s', r \mid s, a)\nabla_{\theta} V_{\pi_{\theta}}(s') \\
&= \sum_{a \in A} (\nabla_{\theta} \pi_{\theta}(a \mid s)Q_{\pi_{\theta}}(s, a) + \gamma\pi_{\theta}(a \mid s) \sum_{s'} p(s' \mid s, a)\nabla_{\theta} V_{\pi_{\theta}}(s') \hspace{2.5em} \text{(4.2)}
\end{aligned}
</p>
<p>Let&rsquo;s define $\phi(s) = \sum_{a \in A} \nabla_{\theta} \pi_{\theta}(a \mid s)Q_{\pi_{\theta}}(s, a)$ and denote the probability
of getting state $s&rsquo;$ after $k$ steps from state $s$ under the policy $\pi_{\theta}$ as $d_{\pi_{\theta}}(s \rightarrow s&rsquo;, k)$, then continue
with equation (4.2) as</p>
<p align="center">
\begin{aligned}
\nabla_{\theta} V_{\pi_{\theta}}(s) &= \phi(s) + \gamma\sum_{a}\pi_{\theta}(a \mid s) \sum_{s'} p(s' \mid s, a)\nabla_{\theta} V_{\pi_{\theta}}(s') \\
&= \phi(s) + \gamma\sum_{a}\sum_{s'}\pi_{\theta}(a \mid s)  p(s' \mid s, a)\nabla_{\theta} V_{\pi_{\theta}}(s') \\
&= \phi(s) + \gamma\sum_{s'}d_{\pi_{\theta}}(s \rightarrow s', 1)\nabla_{\theta} V_{\pi_{\theta}}(s') \\
&= \phi(s) + \gamma\sum_{s'}d_{\pi_{\theta}}(s \rightarrow s', 1) [\phi(s') + \gamma\sum_{s''}d_{\pi_{\theta}}(s' \rightarrow s'', 1)\nabla_{\theta} V_{\pi_{\theta}}(s'')] \\
&= \phi(s) + \gamma\sum_{s'}d_{\pi_{\theta}}(s \rightarrow s', 1) \phi(s') + \gamma^2\sum_{s''}d_{\pi_{\theta}}(s \rightarrow s'', 2)\nabla_{\theta} V_{\pi_{\theta}}(s'') \\
&= \phi(s) + \gamma\sum_{s'}d_{\pi_{\theta}}(s \rightarrow s', 1) \phi(s') + \gamma^2\sum_{s''}d_{\pi_{\theta}}(s \rightarrow s'', 2)\nabla_{\theta} V_{\pi_{\theta}}(s'') + \gamma^3\sum_{s'''}d_{\pi_{\theta}}(s \rightarrow s''', 3)\nabla_{\theta} V_{\pi_{\theta}}(s''')] \\
&= \dots \\
&= \sum_{x \in S} \sum_{k=0}^{\infty} \gamma^k d_{\pi_{\theta}}(s \rightarrow x, k)\phi(x) \hspace{17 em} \text{(4.3)}
\end{aligned}
</p>
<p>Now the gradient of the expected cumulative reward function is</p>
<p align="center">
\begin{aligned}
\nabla_{\theta} J_{\pi_{\theta}}(s) &= \nabla_{\theta} \mathbb{E}_{s_0} [V_{\pi_{\theta}}(s_0)] \\
&= \sum_{s \in S} \mathbb{E}_{s_0} [\sum_{k=0}^{\infty} \gamma^k d_{\pi_{\theta}}(s_0 \rightarrow s, k)\phi(s)] \\
&= \sum_{s \in S} \eta(s)\phi(s), \hspace{1 em} (\eta(s) = \mathbb{E}_{s_0} [\sum_{k=0}^{\infty} \gamma^k d_{\pi_{\theta}}(s_0 \rightarrow s, k)]) \\
&= (\sum_{s \in S} \eta(s)) \sum_{s \in S} \frac{\eta(s)}{\sum_{s \in S} \eta(s)} \phi(s) \\
& \propto \sum_{s \in S} \frac{\eta(s)}{\sum_{s \in S} \eta(s)} \phi(s) \\
&= \sum_{s \in S} \nu(s) \phi(s), \hspace{1 em} (\nu(s) = \frac{\eta(s)}{\sum_{s \in S} \eta(s)}) \\
&= \sum_{s \in S} \nu(s) \sum_{a \in A} Q_{\pi_{\theta}}(s, a) \nabla_{\theta} \pi_{\theta}(a \mid s) \\
&= \sum_{s \in S} \nu(s) \sum_{a \in A} \pi_{\theta}(a \mid s) Q_{\pi_{\theta}}(s, a) \frac{\nabla_{\theta} \pi_{\theta}(a \mid s)}{\pi_{\theta}(a \mid s)} \\
&= \mathbb{E}_{\pi_{\theta}} [Q_{\pi_{\theta}}(s, a) \nabla_{\theta} \log(\pi_{\theta}(a \mid s))] \hspace{16 em} \text{(4.4)}
\end{aligned}
</p>
<p>Thus, the expected cumulative reward function can be optimized by taking the gradient ascent from (4.4). Note that we may need to estimate
the $Q_{\pi_{\theta}}(s, a)$ when calculating the gradient, a simple way is to apply Monte Carlo here and thus results in &ldquo;REINFORCE&rdquo; algorithm.</p>
<h4 id="reinforce" >
<div>
    <a href="#reinforce">
        
    </a>
    REINFORCE
</div>
</h4>
<ul>
<li>Initialize the policy parameter $\theta$</li>
<li>for episode $e$ from 1 to $K$, do:
<ul>
<li>Initialize the state $s$</li>
<li>for $t$ from 1 to $T$, do:
<ul>
<li>sample the trajectories $\{s_1, a_1, r_1, \dots, s_T, a_T, r_T\}$ under the policy $\pi_{\theta}$</li>
<li>For any $t \in [1, T]$, calculate the total reward since time $t$, $\psi_t = \sum_{t&rsquo;=t}^T \gamma^{t&rsquo;-t}r_{t&rsquo;}$</li>
<li>apply gradient ascent to update $\theta$, i.e. $\theta = \theta + \alpha \sum_t^T \psi_t \nabla_{\theta} \pi_{\theta}(a_t \mid s_t)$</li>
</ul>
</li>
<li>end for</li>
</ul>
</li>
<li>end for</li>
<li>return the policy $\pi_\theta$</li>
</ul>
<p>This is a &ldquo;policy version&rdquo; monte carlo approach. Like the Monte Carlo algorithm, it&rsquo;s a simple online algorithm but may suffer from issues like
high variance, delayed rewards, inefficient sampling, noisy updates and non-stationary returns. To tackle these issues, more dedicated tools
are needed.</p>
<h4 id="trust-region-policy-optimization-trpo" >
<div>
    <a href="#trust-region-policy-optimization-trpo">
        
    </a>
    Trust Region Policy Optimization (TRPO)
</div>
</h4>
<p>When using a deep neural network to fit the policy network, the policy gradient updates can be noisy with high variance. Thus resulting in
worse policy updates due to the fluctuation parameters. Is there any way to guarantee the monotonicity of the parameter updates?</p>
<p>Let&rsquo;s rephrase the problem as something like this: Suppose the current policy is $\pi_{\theta}$ with parameter $\theta$, we would like
to search for a new parameter $\theta&rsquo;$ such that $J(\theta&rsquo;) \geq J(\theta)$.</p>
<p align="center">
\begin{aligned}
J(\theta') - J(\theta) &= \mathbb{E}_{s_0}[V_{\pi_{\theta'}}(s_0)] - \mathbb{E}_{s_0}[V_{\pi_{\theta}}(s_0)] \\
&= \mathbb{E}_{\pi_{\theta}}[\sum_{t=0}^{\infty} \gamma^{t} r_t] - 
\end{aligned}
</p>
<h4 id="ppo" >
<div>
    <a href="#ppo">
        
    </a>
    PPO
</div>
</h4>
<h4 id="cross-entropy-method-gradient-free" >
<div>
    <a href="#cross-entropy-method-gradient-free">
        
    </a>
    Cross-Entropy Method [Gradient Free]
</div>
</h4>
<h4 id="evolution-strategy-gradient-free" >
<div>
    <a href="#evolution-strategy-gradient-free">
        
    </a>
    Evolution Strategy [Gradient Free]
</div>
</h4>
<h3 id="hybrid" >
<div>
    <a href="#hybrid">
        
    </a>
    Hybrid
</div>
</h3>
<h4 id="ddpq" >
<div>
    <a href="#ddpq">
        
    </a>
    DDPQ
</div>
</h4>
<h4 id="actor-critic-ac" >
<div>
    <a href="#actor-critic-ac">
        
    </a>
    Actor Critic (AC)
</div>
</h4>
<h4 id="sac" >
<div>
    <a href="#sac">
        
    </a>
    SAC
</div>
</h4>
<h2 id="reference" >
<div>
    <a href="#reference">
        
    </a>
    Reference
</div>
</h2>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Mao, Hongzi, et al. &ldquo;Resource management with deep reinforcement learning.&rdquo; Proceedings of the 15th ACM workshop on hot topics in networks. 2016&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Sebastianelli, Alessandro, et al. &ldquo;A Deep Q-Learning based approach applied to the Snake game.&rdquo; 2021 29th Mediterranean Conference on Control and Automation (MED). IEEE, 2021&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Muteba, K. F., Karim Djouani, and Thomas O. Olwal. &ldquo;Deep reinforcement learning based resource allocation for narrowband cognitive radio-IoT systems.&rdquo; Procedia Computer Science 175 (2020): 315-324&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Hessel, Matteo, et al. &ldquo;Rainbow: Combining improvements in deep reinforcement learning.&rdquo; Proceedings of the AAAI conference on artificial intelligence. Vol. 32. No. 1. 2018.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
        </div>

    </article>

    
    
        
    

    

    
        
    
    
    <div class="pagination post-pagination">
        <div class="left pagination-item disabled">
            
        </div>
        <div class="right pagination-item ">
            
                <a href="/posts/2024-07-05-rf-learning-tabular/">A Journey to Reinforcement Learning - Tabular Methods</a>
            
        </div>
    </div>





    

    
        








            

<script>
        function detectCurrentScheme2() {
                const defaultTheme = "auto";
                if (localStorage !== null && localStorage.getItem("user-color-scheme")) {
                        return localStorage.getItem("user-color-scheme");
                }
                if (defaultTheme === "dark" || defaultTheme === "light") {
                        return defaultTheme;
                }
                return window.matchMedia("(prefers-color-scheme: dark)").matches ? "dark" : "light";
        }

        let giscusTheme = detectCurrentScheme2();
        let giscusAttributes = {
                src: "https://giscus.app/client.js",
                "data-repo": "Bingzw\/bingzw.github.io",
                "data-repo-id": "R_kgDOMB8LQg",
                "data-category": "Announcements",
                "data-category-id": "DIC_kwDOMB8LQs4CfrMr",
                "data-mapping": "pathname",
                "data-strict": "0",
                "data-reactions-enabled": "1",
                "data-emit-metadata": "0",
                "data-input-position": "bottom",
                "data-theme": giscusTheme,
                "data-lang": "en",
                crossorigin: "anonymous",
                lazyload: "false",
                async: true,
        };
        let main = document.querySelector("main");
        let giscusScript = document.createElement("script");
        Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
        main.appendChild(giscusScript);
</script>




    

    

    

        </main>
        
            <footer class="common-footer noselect">
    
    

    <div class="common-footer-bottom">
        

        <div style="display: flex; align-items: center; gap:8px">
            © Bing Wang, 2024
            
        </div>
        <div style="display:flex;align-items: center">
            
            
            
            
            
            
        </div>
        <div>
            Powered by <a target="_blank" rel="noopener noreferrer" href="https://gohugo.io/">Hugo</a>, theme <a target="_blank" rel="noopener noreferrer" href="https://github.com/Junyi-99/hugo-theme-anubis2">Anubis2</a>.<br>
            

        </div>
    </div>

    <p class="h-card vcard">

    <a href=https://bingzw.github.io/ class="p-name u-url url fn" rel="me"></a>

    

    
</p>

</footer>

        
    </div>
</body>
</html>

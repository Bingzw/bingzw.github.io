<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on Bingz Learning Blog</title>
    <link>/tags/machine-learning/</link>
    <description>Bingz Learning Blog (Machine Learning)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <managingEditor>bingwang8878@gamil.com (Bing Wang)</managingEditor>
    <webMaster>bingwang8878@gamil.com (Bing Wang)</webMaster>
    <lastBuildDate>Thu, 06 Jun 2024 22:04:53 -0700</lastBuildDate>
    
    <atom:link href="/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Recommendation Models</title>
      <link>/posts/2024-06-06-recommendation-model/</link>
      <pubDate>Thu, 06 Jun 2024 22:04:53 -0700</pubDate>
      <author>bingwang8878@gamil.com (Bing Wang)</author>
      <guid>/posts/2024-06-06-recommendation-model/</guid>
      <description>&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://bingzw.github.io/recommendation_model/reco.jpeg&#34; width=&#34;600&#34; height=&#34;400&#34;&gt;&lt;br&gt;
&lt;p&gt;
&lt;p&gt;&lt;em&gt;Image cited from &lt;a href=&#34;https://www.vecteezy.com/&#34;&gt;vecteezy&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction-to-recommendation-models&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#introduction-to-recommendation-models&#34;&gt;
        
    &lt;/a&gt;
    Introduction to Recommendation Models
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;In the digital age, the sheer volume of available content and products can be overwhelming for users. Recommendation
systems play a crucial role in helping users discover relevant items by filtering through vast amounts of data. These
systems are integral to various industries, including e-commerce, streaming services, social media, and online
advertising.&lt;/p&gt;
&lt;p&gt;Recommendation models are algorithms designed to suggest items to users based on various factors such as past behavior,
preferences, and item characteristics. The goal is to enhance user experience by providing personalized recommendations,
thereby increasing engagement and satisfaction.&lt;/p&gt;
&lt;p&gt;In most cases, the goal is trying to find the most appealing item for the user given all user demographic, behavior and
interest information. A well-designed recommendation system usually results in better personalization, user engagement,
and revenue generation. Let&amp;rsquo;s take a look at the most popular modeling approaches.&lt;/p&gt;
&lt;h2 id=&#34;collaborative-filtering-cf&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#collaborative-filtering-cf&#34;&gt;
        
    &lt;/a&gt;
    Collaborative Filtering (CF)
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;Collaborative Filtering is one of the most famous recommendation models so far. It&amp;rsquo;s about to guess the user&amp;rsquo;s interest
based on the behaviors and preferences of other users with similar tastes/characteristics. This basic approach is
straightforward and only requires user-item interaction history.&lt;/p&gt;
&lt;h3 id=&#34;memory-based-cf&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#memory-based-cf&#34;&gt;
        
    &lt;/a&gt;
    Memory Based CF
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;Suppose the recommendation is purely derived from historical records and it does not involve any predictive modelings.
Let&amp;rsquo;s assume a table of all ratings of users on items. Let $r_{u,i}$ denotes the missing rating of user $u$ on item $i$.
$S_u$ be a set of users that share similar characteristics with user $u$. $P_j$ denotes a set of items that are close to
item $j$. The goal is to guess the missing rating $r_{u,i}$. Let&amp;rsquo;s start expressing the &amp;ldquo;predicted&amp;rdquo; rating from either user-user
filtering or item-item filtering.&lt;/p&gt;
&lt;h4 id=&#34;user-user-filtering&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#user-user-filtering&#34;&gt;
        
    &lt;/a&gt;
    User User Filtering
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;Assume that users with a similar profile share a similar taste. We are predicting the missing rating as the weighted
average from ratings of similar users:&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
$r_{u,i} = \bar{r}_u + \frac{\sum_{u&#39;\in S_u}sim(u, u&#39;) * (r_{u&#39;,i} - \bar{r}_{u&#39;})}{\sum_{u&#39;\in S_u}sim(u, u&#39;)}$
&lt;p&gt;
&lt;p&gt;where $\bar{r}_u$ is the average rating of user $u$ and $sim(u, u&amp;rsquo;)$ represents the similarity score between user $u$
and $u&amp;rsquo;$. A common choice is to calculate the cosine similarity between two user rating vectors.&lt;/p&gt;
&lt;h4 id=&#34;item-item-filtering&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#item-item-filtering&#34;&gt;
        
    &lt;/a&gt;
    Item Item Filtering
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;Assume that the customers will prefer products that share a high similarity with those already well appreciated. The
missing rating is thus predicted as the weighted average of a set of similar products:&lt;/p&gt;
&lt;p&gt;$$r_{u,i} = \frac{\sum_{j \in P_i}sim(j, i) * r_{u,j}}{\sum_{j \in P_i}sim(j, i)}$$&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s straightforward to observe that user-based filtering is to check what other users think of the same product, while
item-based filtering is aggregating what the user thinks of other items. Essentially, both are weighted linear
combination of observed ratings. Can we do better than weighted average?&lt;/p&gt;
&lt;h3 id=&#34;model-based-cf&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#model-based-cf&#34;&gt;
        
    &lt;/a&gt;
    Model Based CF
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;Unlike the memory based CF which is trying to fill in the missing cells in the rating matrix, model-based collaborative
filtering is to predict user preferences for items based on past interactions. It often relies on the concept of
latent factors. These are hidden features that influence user preferences and item characteristics. By uncovering these
latent factors, the model can predict the likelihood of a user liking an item.&lt;/p&gt;
&lt;p&gt;Let $R$ denotes the user item interaction rating matrix, $R \in \mathbb{R}^{m*n}$, where $m$ is the dimension of user
space and $n$ denotes the dimension of item space.&lt;/p&gt;
&lt;h4 id=&#34;clustering&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#clustering&#34;&gt;
        
    &lt;/a&gt;
    Clustering
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;Given user or item embeddings, a simple approach is to apply the K nearest neighbours to find the K closest users or
items depending on the similarity metrics used.&lt;/p&gt;
&lt;h4 id=&#34;matrix-factorization&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#matrix-factorization&#34;&gt;
        
    &lt;/a&gt;
    Matrix Factorization
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;The basic idea is to decompose the user-item interaction matrix into two lower-dimensional matrices:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;User Matrix (U)&lt;/strong&gt;: Represents users in terms of latent factors, $U \in \mathbb{R}^{m * p}$, where $p$ is the dimension
of the latent space.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Item Matrix (V)&lt;/strong&gt;: Represents items in terms of latent factors, $V \in \mathbb{R}^{n * p}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The interaction matrix $R$ is approximated as the product of these two matrices: $R \approx U \cdot V^T$. In practice, we
are usually optimizing for a weighted matrix factorization objective&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;$$L(\theta) = \sum_{(i, j)\in obs} \omega_{i, j} (R_{i, j} - U_i \cdot V_{j}^{T})^2 + \omega_{0} \sum_{(i, j) \notin obs}
(U_i \cdot V_{j}^{T})^2$$&lt;/p&gt;
&lt;p&gt;where $\omega_0$ is a hyper-parameter that weights the two terms so that the objective is not
dominated by one or the other, and $\omega_{i, j}$ is a function of the frequency of user $i$ and item $j$.&lt;/p&gt;
&lt;p&gt;Common optimization algorithms to minimize the above objective function includes &lt;strong&gt;stochastic gradient descent&lt;/strong&gt; and
&lt;strong&gt;weighted alternating least squares (WALS)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Recently, a few efforts on deep learning have also been proposed on matrix factorization. For example,&lt;/p&gt;
&lt;h5 id=&#34;deep-auto-encoders2&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#deep-auto-encoders2&#34;&gt;
        
    &lt;/a&gt;
    Deep Auto-Encoders&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h5&gt;
&lt;p&gt;It proposed a model called Collaborative Denoising Auto-Encoder (CDAE) that extends the traditional denoising autoencoder
architecture to the collaborative filtering domain. CDAE is designed to handle implicit feedback, where user preferences
are inferred from user behavior rather than explicit ratings. The model incorporates both user-specific and item-specific
factors, leveraging the rich user interaction data to learn better representations for recommendation tasks.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
    &lt;img src=&#34;https://bingzw.github.io/recommendation_model/autoencoder_mf.png&#34;&gt;&lt;br&gt;
    &lt;em&gt;Figure 1: deep auto-encoder&lt;/em&gt;
&lt;/p&gt;
&lt;h5 id=&#34;neural-collaborative-filtering3&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#neural-collaborative-filtering3&#34;&gt;
        
    &lt;/a&gt;
    Neural Collaborative Filtering&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h5&gt;
&lt;p&gt;NCF replaces these linear latent factor models with non-linear neural networks, allowing for a more expressive
representation of user-item interactions. By doing so, NCF can capture more complex patterns in the data that traditional
methods might miss. The core idea is to use multi-layer perceptrons (MLPs) to model the interaction function between
users and items, providing a more flexible and powerful framework for learning user preferences.&lt;/p&gt;
&lt;p&gt;The general architecture of NCF includes embedding layers for users and items, followed by one or more hidden layers
that learn the interaction between these embeddings. The final output layer predicts the user&amp;rsquo;s preference for a given
item. This approach not only improves the accuracy of recommendations but also enables the integration of additional
features, such as user demographics or item attributes, into the model. Refer to the DLRM or DeepFM for more details.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
    &lt;img src=&#34;https://bingzw.github.io/recommendation_model/ncf_mf.png&#34;&gt;&lt;br&gt;
    &lt;em&gt;Figure 2: neural collaborative filtering&lt;/em&gt;
&lt;/p&gt;
&lt;h2 id=&#34;content-based-models-ranking-as-recommendation&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#content-based-models-ranking-as-recommendation&#34;&gt;
        
    &lt;/a&gt;
    Content Based Models (Ranking As Recommendation)
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;Unlike the collaborative filtering that purely relies on user-item interaction data. Content-based recommendation models
focus on the attributes of items to suggest similar items to users based on their past interactions. These models analyze
the content (such as text, keywords, categories, or features) associated with the items and create a profile for each
user based on the features of the items they have shown interest in. Compared with collaborative filtering, it&amp;rsquo;s easier
to handle the cold start issue when item features are known. However, its recommendation may strictly adheres to the
user&amp;rsquo;s profile, potentially limiting diversity. There are quite a few popular model frameworks in this area and we would
focus on the models based on deep neural architecture.&lt;/p&gt;
&lt;h3 id=&#34;wide--deep4&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#wide--deep4&#34;&gt;
        
    &lt;/a&gt;
    Wide &amp;amp; Deep&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;Wide &amp;amp; Deep learning combined wide linear models and deep neural networks to achieve both memorization and generalization.
The model consists of two components:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Wide Component&lt;/strong&gt;: This part is a generalized linear model (GLM) that excels at memorization by capturing feature
interactions using cross-product feature transformations. The wide component is effective at handling sparse features
and explicitly memorizing frequent co-occurrence patterns. It can be represented as:&lt;/p&gt;
&lt;p&gt;$$y_{\text{wide}} = \mathbf{w}^T \mathbf{x} + b $$&lt;/p&gt;
&lt;p&gt;where $\mathbf{x}$ represents the input features, $\mathbf{w}$ represents the learned weights, and $b$ is
the bias term.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Component&lt;/strong&gt;: This part is a feed-forward neural network that excels at generalization by capturing high-level
and non-linear feature interactions. The deep component uses dense embeddings to represent categorical features, and it
learns implicit interactions through multiple hidden layers. The output of the deep component can be represented as:&lt;/p&gt;
&lt;p&gt;$$\mathbf{h}^L = f^L(\mathbf{W}^L \mathbf{h}^{L-1} + \mathbf{b}^L)$$&lt;/p&gt;
&lt;p&gt;where $\mathbf{h}^L$ is the activation of the $L$-th layer, $\mathbf{W}^L$ and $\mathbf{b}^L$ are the weights and biases
of the $L$-th layer, respectively, and $f^L$ is the activation function.
The final output of the deep component, $ y_{\text{deep}} $, is the output of the last layer of the deep neural network. It is given by:&lt;/p&gt;
&lt;p&gt;$$
y_{\text{deep}} = \mathbf{W}^O \mathbf{h}^{L} + \mathbf{b}^O
$$&lt;/p&gt;
&lt;p&gt;where $ \mathbf{W}^O $ and $ \mathbf{b}^O $ are the weights and bias of the output layer, and $ \mathbf{h}^L $ is the activation of the last hidden layer.&lt;/p&gt;
&lt;p&gt;The final prediction is a weighted sum of the outputs from the wide and deep components:&lt;/p&gt;
&lt;p&gt;$$\hat{y} = \sigma(y_{\text{wide}} + y_{\text{deep}})$$&lt;/p&gt;
&lt;p&gt;where $\sigma$ is the sigmoid function used to squash the output to a probability score.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p align=&#34;center&#34;&gt;
   &lt;img src=&#34;https://bingzw.github.io/recommendation_model/wide_deep.png&#34; width=&#34;800&#34; height=&#34;600&#34;&gt;&lt;br&gt;
   &lt;em&gt;Figure 3: wide &amp; deep model&lt;/em&gt;
&lt;/p&gt;
&lt;h3 id=&#34;deepfm5&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#deepfm5&#34;&gt;
        
    &lt;/a&gt;
    DeepFM&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;DeepFM addresses the challenge of capturing both low-order and high-order feature interactions in recommendation systems.
The model consists of two interconnected components:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;FM Component&lt;/strong&gt;: This part captures low-order feature interactions using Factorization Machines, which are effective
for handling sparse data and modeling pairwise feature interactions without manual feature engineering. The FM component
can be represented as:&lt;/p&gt;
&lt;p&gt;$$
y_{\text{FM}} = w_0 + \sum_{i=1}^n w_i x_i + \sum_{i=1}^n \sum_{j=i+1}^n \langle \mathbf{v}_i, \mathbf{v}_j \rangle x_i x_j
$$&lt;/p&gt;
&lt;p&gt;where $w_0$ is the global bias, $w_i$ is the weight of the $i$-th feature, $\mathbf{v}_i$ and $\mathbf{v}_j$ are
latent vectors for the $i$-th and $j$-th features, respectively, and $x_i$ and $x_j$ are input feature values.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Component&lt;/strong&gt;: This part captures high-order feature interactions through a deep neural network. The deep
component uses embeddings to represent input features and learns complex interactions through multiple hidden layers.
The output of the deep component can be represented as:&lt;/p&gt;
&lt;p&gt;$$\mathbf{h}^L = f^L(\mathbf{W}^L \mathbf{h}^{L-1} + \mathbf{b}^L)$$&lt;/p&gt;
&lt;p&gt;where $\mathbf{h}^L$ is the activation of the $L$-th layer, $\mathbf{W}^L$ and $\mathbf{b}^L$ are the weights and
biases of the $L$-th layer, respectively, and $f^L$ is the activation function.
The final output of the deep component, $ y_{\text{deep}} $, is the output of the last layer of the deep neural network. It is given by:&lt;/p&gt;
&lt;p&gt;$$
y_{\text{deep}} = \mathbf{W}^O \mathbf{h}^{L} + \mathbf{b}^O
$$&lt;/p&gt;
&lt;p&gt;where $ \mathbf{W}^O $ and $ \mathbf{b}^O $ are the weights and bias of the output layer, and $ \mathbf{h}^L $ is the
activation of the last hidden layer.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The final prediction is a combination of the outputs from the FM and deep components:&lt;/p&gt;
&lt;p&gt;$$\hat{y} = \sigma(y_{\text{FM}} + y_{\text{deep}})$$&lt;/p&gt;
&lt;p&gt;where $\sigma$ is the sigmoid function used to squash the output to a probability score.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
   &lt;img src=&#34;https://bingzw.github.io/recommendation_model/deepfm.png&#34; width=&#34;600&#34; height=&#34;400&#34;&gt;&lt;br&gt;
   &lt;em&gt;Figure 4: deepfm model&lt;/em&gt;
&lt;/p&gt;
&lt;h3 id=&#34;deep-learning-recommendation-model-dlrm6&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#deep-learning-recommendation-model-dlrm6&#34;&gt;
        
    &lt;/a&gt;
    Deep Learning Recommendation Model (DLRM)&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;Deep Learning Recommendation Model (DLRM) is an advanced machine learning framework designed by Facebook AI to tackle
the complex challenge of personalized recommendations at scale. It is particularly suited for large-scale recommendation
systems in environments such as social media platforms, e-commerce, and online advertising. The DLRM combines the
strengths of collaborative filtering and content-based methods by utilizing both dense and sparse features to provide
highly accurate and scalable recommendations.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
   &lt;img src=&#34;https://bingzw.github.io/recommendation_model/dlrm.png&#34; width=&#34;600&#34; height=&#34;400&#34;&gt;&lt;br&gt;
   &lt;em&gt;Figure 5: deep learning recommendation model&lt;/em&gt;
&lt;/p&gt;
&lt;p&gt;Key components of DLRM includes&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Sparse Features&lt;/strong&gt;: These are categorical variables (e.g., user ID, item ID) which are typically represented using
embeddings. Embeddings transform sparse categorical data into dense vectors of continuous numbers, making them suitable
for neural network processing, denoted as $\mathbf{x}_s[i]$.
The raw sparse features are transformed to sparse embeddings as follows. Let
$$
\mathbf{e}_i = \text{Embedding}(\mathbf{x}_s[i])
$$
where $\mathbf{e}_i$ is the embedding vector for the $i$-th sparse feature, $\text{Embedding}$ denotes an embedding lookup table&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dense Features&lt;/strong&gt;: These are numerical variables (e.g., user age, item price) that are used directly in their raw
form or normalized form, denoted as $\mathbf{x}_d$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Bottom MLP (Multilayer Perceptron)&lt;/strong&gt;: Processes the dense features to capture high-level representations.
$$
\mathbf{h}_d = \text{BottomMLP}(\mathbf{x}_d)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Interaction Layer&lt;/strong&gt;: This layer captures the interactions between different features (both sparse and dense). It
uses a dot product to compute the pairwise interactions among features.
$$
\mathbf{z} = \left[ \mathbf{h}_d, \mathbf{e}_1, \mathbf{e}_2, \ldots, \mathbf{e}_n \right]
$$
where $\mathbf{z}$ is the concatenated vector of dense feature representation and embeddings.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Top MLP&lt;/strong&gt;: Combines the processed dense features and interactions from the interaction layer to make the final prediction.
$$
\hat{y} = \sigma(\text{TopMLP}(\mathbf{z}))
$$
where $\sigma$ is an activation function, typically a sigmoid function for binary classification tasks.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;deep--cross-network-v2-dcn-v27&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#deep--cross-network-v2-dcn-v27&#34;&gt;
        
    &lt;/a&gt;
    Deep &amp;amp; Cross Network v2 (DCN v2)&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;DCN-V2 starts with an embedding layer, followed by a cross network containing multiple cross layers that models explicit
feature interactions, and then combines with a deep network that models implicit feature interactions. The function class
modeled by DCN-V2 is a strict superset of that modeled by DCN. The overall model architecture is depicted in Fig. 5, with
two ways to combine the cross network with the deep network: (1) stacked and (2) parallel.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cross Component&lt;/strong&gt;: The Cross Network V2 enhances the original cross network by introducing a more flexible mechanism
to capture feature interactions. The cross layer in DCN V2 can be represented as:&lt;/p&gt;
&lt;p&gt;$$
x_{l+1} = x_0 \odot (W_l \cdot x_l + b_l) + x_l
$$&lt;/p&gt;
&lt;p&gt;where $x_l$ is the input to the l-th cross layer. $W_l$ and $b_l$ are the weight matrix and bias vector of the l-th
cross layer. $x_{l+1}$ is the output of the $l+1$-th cross layer. $\odot$ is the Hadamard product.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Component&lt;/strong&gt;: The deep network in DCN V2 captures high-order feature interactions through a series of dense layers.
This part of the network learns abstract representations of the input features, enabling the model to generalize well.&lt;/p&gt;
&lt;p&gt;$$
h_{l} = f_{l}(W_{l} h_{l-1} + b_{l})
$$&lt;/p&gt;
&lt;p&gt;where $h_{l}$ is the activation of the $l$-th layer, $W_{l}$ and $b_{l}$ are the weights and
biases of the $l$-th layer, respectively, and $f_{l}$ is the activation function.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The final prediction is a combination of the outputs from either the hidden layer (stacked structure) or the  concatenation
of cross and deep network outputs.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
   &lt;img src=&#34;https://bingzw.github.io/recommendation_model/dcn_v2.png&#34; width=&#34;600&#34; height=&#34;400&#34;&gt;&lt;br&gt;
   &lt;em&gt;Figure 6: deep cross network v2&lt;/em&gt;
&lt;/p&gt;
&lt;h3 id=&#34;deep-interest-network-din8&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#deep-interest-network-din8&#34;&gt;
        
    &lt;/a&gt;
    Deep Interest Network (DIN)&lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;Deep Interest Network (DIN) is a neural network-based model designed for personalized recommendation systems, particularly
in the context of e-commerce and advertising. Unlike traditional recommendation models that primarily focus on user-item interactions,
DIN leverages a user&amp;rsquo;s historical behavior to make more accurate and contextually relevant recommendations. The key
innovation in DIN is its ability to capture user interests dynamically and use this information to influence the recommendation
process.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
   &lt;img src=&#34;https://bingzw.github.io/recommendation_model/din.png&#34; width=&#34;800&#34; height=&#34;600&#34;&gt;&lt;br&gt;
   &lt;em&gt;Figure 7: deep interest network&lt;/em&gt;
&lt;/p&gt;
&lt;p&gt;In particular, the DIN contains the following key components:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Embedding Layer&lt;/strong&gt;: User behaviors and items are represented as dense vectors through an embedding layer. Let
$({e_{1}, e_{2}, \ldots, e_{n}})$ be the sequence of embeddings for user behaviors, and $e_{target}$ be the embedding of the target item.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Local Activation Unit&lt;/strong&gt;: This unit applies the attention mechanism to compute the relevance of each user behavior
in the sequence with respect to the target item.
Let
$$
\alpha_{i} = \frac{\exp(\text{score}(e_{i}, e_{target}))}{\sum_{j=1}^{n} \exp(\text{score}(e_{j}, e_{target}))}
$$
where $\text{score}(e_{i}, e_{target})$ is the activation weight output from a feed-forward
network measuring the relevance of behavior $e_{i}$ to the target item.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Interest Extractor Layer&lt;/strong&gt;: Combines the weighted behavior embeddings to form a user interest representation.
$$
u = \sum_{i=1}^{n} \alpha_{i} e_{i}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Prediction Layer&lt;/strong&gt;: The user interest representation is concatenated with the target item embedding and fed into a neural network to predict the user&amp;rsquo;s interaction with the target item.
$$
\hat{y} = \sigma(W[u, e_{target}, e_{profile}, e_{context}] + b)
$$
where $[u, e_{target}, e_{profile}, e_{context}]$ denotes the concatenation of the user interest representation, profile
embedding, context embedding and the target item embedding, $W$ is a weight matrix, $b$ is a bias term, and $\sigma$ is an activation function (e.g., sigmoid).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;multi-gate-mixture-of-experts-mmoe9&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#multi-gate-mixture-of-experts-mmoe9&#34;&gt;
        
    &lt;/a&gt;
    Multi-gate Mixture of Experts (MMOE)&lt;sup id=&#34;fnref:9&#34;&gt;&lt;a href=&#34;#fn:9&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;Multi-gate Mixture-of-Experts (MMoE) is an advanced multi-task learning (MTL) framework designed to model and leverage
task relationships for improved performance across multiple tasks. The MMoE architecture combines the benefits of
mixture-of-experts models with the flexibility of multi-gate mechanisms, allowing the model to dynamically allocate
computational resources based on the specific needs of each task. This approach is particularly useful in scenarios
where tasks are interrelated and can benefit from shared learning while maintaining task-specific adaptations.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
   &lt;img src=&#34;https://bingzw.github.io/recommendation_model/mmoe.png&#34; width=&#34;800&#34; height=&#34;600&#34;&gt;&lt;br&gt;
   &lt;em&gt;Figure 8: multi-gate mixture of experts&lt;/em&gt;
&lt;/p&gt;
&lt;p&gt;Key components of the model architecture are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Experts&lt;/strong&gt;: A set of neural network sub-models that serve as specialized units for feature extraction. Each expert
is trained to capture different aspects of the input data. Let $\mathbf{x}$ represent the input data, $E$ represent
the number of experts, and $T$ represent the number of tasks. The output from expert is represented as
$$
\mathbf{h}^i = f_e^i(\mathbf{x}), \quad \text{for } i = 1, 2, \ldots, E
$$
where $f_e^i$ is the function of the $i$-th expert, and $\mathbf{h}^i$ is the output of the $i$-th expert.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Multi-gate Mechanism&lt;/strong&gt;: Separate gating networks for each task that dynamically select and weight the contributions
of different experts based on the input data and task requirements.
$$
\mathbf{g}^j = \sigma(\mathbf{W}^j \mathbf{x}), \quad \text{for } j = 1, 2, \ldots, T
$$
where $\mathbf{W}^j$ are the parameters of the gating network for task $j$, and $g^j$ is the gating output distribution
of gating weights to each expert assigned by task $j$. To combine the experts output and gating weights, we have
$$\mathbf{t}^j = \sum_{i=1}^{E} g_i^j \cdot \mathbf{h}^i$$
where $\mathbf{t}^j$ is the combined output for task $j$, and $g_i^j$ is the weight for the $i$-th expert assigned by
the gating network of task $j$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Task-specific Layers&lt;/strong&gt;: Layers that process the combined outputs from the experts, tailored to the specific requirements of each task.
$$
\hat{y}^j = f_o^j(\mathbf{t}^j)
$$
where $f_o^j$ is the task-specific output layer for task $j$, and $\hat{y}^j$ is the predicted output for task $j$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;behavior-sequence-transformer-bst10&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#behavior-sequence-transformer-bst10&#34;&gt;
        
    &lt;/a&gt;
    Behavior Sequence Transformer (BST)&lt;sup id=&#34;fnref:10&#34;&gt;&lt;a href=&#34;#fn:10&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;10&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;The Behavior Sequence Transformer (BST) is a novel neural network architecture designed for modeling user behavior
sequences in recommendation systems. BST leverages the power of Transformer models, which have achieved significant
success in natural language processing (NLP), to capture the sequential patterns and contextual dependencies in user
interactions over time. This approach enhances the ability to predict user preferences and improve recommendation accuracy.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
   &lt;img src=&#34;https://bingzw.github.io/recommendation_model/bst.png&#34; width=&#34;800&#34; height=&#34;600&#34;&gt;&lt;br&gt;
   &lt;em&gt;Figure 9: behavior sequence transformer&lt;/em&gt;
&lt;/p&gt;
&lt;p&gt;Key components of BST include:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Input Layer&lt;/strong&gt;:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;User Behavior Sequence&lt;/strong&gt;: A sequence of items or actions representing user interactions over time.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Contextual Features&lt;/strong&gt;: Additional information such as timestamps, device types, and other relevant context.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Embedding Layer&lt;/strong&gt;:
$$
\mathbf{E}_x = \mathbf{W}_e \cdot \mathbf{x}
$$
where $\mathbf{W}_e$ is the embedding matrix and $\mathbf{x}$ is the input feature vector.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Positional Encoding&lt;/strong&gt;:
$$
\mathbf{E}_p = \text{PE}(pos)
$$
where $\text{PE}(pos)$ is the positional encoding function that adds position-specific information to the embeddings.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Transformer Encoder&lt;/strong&gt;:
$$
\mathbf{H}_i = \text{LayerNorm}(\text{MultiHeadAttention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) + \mathbf{E}_i)
$$
where $\mathbf{Q}$, $\mathbf{K}$, and $\mathbf{V}$ are the query, key, and value matrices, and $\mathbf{E}_i$ is the
input embedding at layer $i$. A Feed-Forward Networks ($\text{FFN}$) is added on top of the self attention layer to further enhance the
model with non-linearity.
$$
\mathbf{O}_i = \text{LayerNorm}(\text{FFN}(\mathbf{H}_i) + \mathbf{H}_i)
$$
In practice, we usually stack multiple transformer layers. So the output of last layer is thus fed as the input of the
next layer.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Output Layer&lt;/strong&gt;:
$$
\hat{y} = \sigma(\mathbf{W}_o \cdot \mathbf{O}_L)
$$
where $\mathbf{W}_o$ is the weight matrix of the output layer, $\mathbf{O}_L$ is the output of the last Transformer encoder layer, and $\sigma$ is the activation function.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#summary&#34;&gt;
        
    &lt;/a&gt;
    Summary
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;So far, we have summarized the main popular recommendation models. These summaries highlight the unique strengths and
specific applications of each recommendation model, reflecting their advancements and contributions to the field. The
modern recommendation systems usually are built with multiple stages and are composed with both simple and complex models.
Users may choose the best model depending on the business requirements and system architecture design.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#citation&#34;&gt;
        
    &lt;/a&gt;
    Citation
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;If you find this post helpful, please consider citing it as:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bibtex&#34; data-lang=&#34;bibtex&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f3f99d&#34;&gt;@article&lt;/span&gt;{&lt;span style=&#34;color:#ff5c57&#34;&gt;wang2024recommendationmodel&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;author&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;Bing Wang&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;title&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;Recommendation Models&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;journal&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;bingzw.github.io&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;year&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;2024&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;month&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;June&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;url&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;https://bingzw.github.io/posts/2024-06-06-recommendation-model/&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;or&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Bing Wang. (2024, June). Recommendation Models. 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;https://bingzw.github.io/posts/2024-06-06-recommendation-model/
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;reference&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#reference&#34;&gt;
        
    &lt;/a&gt;
    Reference
&lt;/div&gt;
&lt;/h2&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://developers.google.com/machine-learning/recommendation/collaborative/matrix&#34;&gt;Collaborative Filtering and Matrix Factorization&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1708.01715&#34;&gt;Kuchaiev, Oleksii, and Boris Ginsburg. &amp;ldquo;Training deep autoencoders for collaborative filtering.&amp;rdquo; arXiv preprint arXiv:1708.01715 (2017)&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1708.05031&#34;&gt;He, Xiangnan, et al. &amp;ldquo;Neural collaborative filtering.&amp;rdquo; Proceedings of the 26th international conference on world wide web. 2017&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1606.07792&#34;&gt;Cheng, Heng-Tze, et al. &amp;ldquo;Wide &amp;amp; deep learning for recommender systems.&amp;rdquo; Proceedings of the 1st workshop on deep learning for recommender systems. 2016&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1703.04247&#34;&gt;Guo, Huifeng, et al. &amp;ldquo;DeepFM: a factorization-machine based neural network for CTR prediction.&amp;rdquo; arXiv preprint arXiv:1703.04247 (2017)&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1906.00091&#34;&gt;Naumov, Maxim, et al. &amp;ldquo;Deep learning recommendation model for personalization and recommendation systems.&amp;rdquo; arXiv preprint arXiv:1906.00091 (2019)&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2008.13535&#34;&gt;Wang, Ruoxi, et al. &amp;ldquo;Dcn v2: Improved deep &amp;amp; cross network and practical lessons for web-scale learning to rank systems.&amp;rdquo; Proceedings of the web conference 2021. 2021&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1706.06978&#34;&gt;Zhou, Guorui, et al. &amp;ldquo;Deep interest network for click-through rate prediction.&amp;rdquo; Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp;amp; data mining. 2018&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:9&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://dl.acm.org/doi/pdf/10.1145/3219819.3220007&#34;&gt;Ma, Jiaqi, et al. &amp;ldquo;Modeling task relationships in multi-task learning with multi-gate mixture-of-experts.&amp;rdquo; Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp;amp; data mining. 2018&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:9&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:10&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1905.06874&#34;&gt;Chen, Qiwei, et al. &amp;ldquo;Behavior sequence transformer for e-commerce recommendation in alibaba.&amp;rdquo; Proceedings of the 1st international workshop on deep learning practice for high-dimensional sparse data. 2019&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:10&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
    </item>
    
  </channel>
</rss>

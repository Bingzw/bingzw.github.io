<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on Bingz Learning Blog</title>
    <link>/tags/machine-learning/</link>
    <description>Bingz Learning Blog (Machine Learning)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <managingEditor>bingwang8878@gamil.com (Bing Wang)</managingEditor>
    <webMaster>bingwang8878@gamil.com (Bing Wang)</webMaster>
    <lastBuildDate>Mon, 15 Jul 2024 21:40:55 -0700</lastBuildDate>
    
    <atom:link href="/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>A Journey to Reinforcement Learning - Deep RL</title>
      <link>/posts/2024-07-15-rf-learning-deep/</link>
      <pubDate>Mon, 15 Jul 2024 21:40:55 -0700</pubDate>
      <author>bingwang8878@gamil.com (Bing Wang)</author>
      <guid>/posts/2024-07-15-rf-learning-deep/</guid>
      <description>&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;http://localhost:1313/rf/deeprl.png&#34; width=&#34;600&#34; height=&#34;400&#34;&gt;&lt;br&gt;
&lt;p&gt;
&lt;p&gt;&lt;em&gt;Image cited from &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This blog is a continuation of the last one: &lt;strong&gt;A Journey to Reinforcement Learning - Tabular Methods&lt;/strong&gt;. The
table of content structure and notations follow the same framework. Let&amp;rsquo;s continue the journey from the last value based
algorithm in model free setting.&lt;/p&gt;
&lt;h2 id=&#34;model-free---when-the-environment-is-unknown&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#model-free---when-the-environment-is-unknown&#34;&gt;
        
    &lt;/a&gt;
    Model Free - when the environment is unknown
&lt;/div&gt;
&lt;/h2&gt;
&lt;h3 id=&#34;value-based-continuing-&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#value-based-continuing-&#34;&gt;
        
    &lt;/a&gt;
    Value Based (Continuing &amp;hellip;)
&lt;/div&gt;
&lt;/h3&gt;
&lt;h4 id=&#34;deep-q-network-dqn&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#deep-q-network-dqn&#34;&gt;
        
    &lt;/a&gt;
    Deep Q Network (DQN)
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;Like what have been discussed in SARSA and $Q$ learning, they may not be a good fit when state or action space is too large
or even continuous. Maintaining a $Q$ table is not feasible in such cases. Instead, we can apply function approximation to
the $Q$ value. So far, the model powerful approach that can approximate almost any shape of function is &lt;strong&gt;Neural Network&lt;/strong&gt;.
Welcome to the realm of &lt;strong&gt;deep reinforcement learning&lt;/strong&gt; (short for &amp;ldquo;deep learning&amp;rdquo; + &amp;ldquo;reinforcement learning&amp;rdquo;).&lt;/p&gt;
&lt;p&gt;The idea seems to be natural. Let&amp;rsquo;s refine the DQN framework step by step. As an extension from $Q$ learning. Let&amp;rsquo;s frame the
problem with &lt;strong&gt;continuous states&lt;/strong&gt; and &lt;strong&gt;discrete actions&lt;/strong&gt; (the continuous actions case will be discussed later). The $Q$ value can
be approximated by fitting a neural network where the state is fed into the network and get output of $Q$ value at each action.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;http://localhost:1313/rf/q_dqn.png&#34; width=&#34;600&#34; height=&#34;400&#34;&gt;&lt;br&gt;
&lt;em&gt;Figure 1: Q learning vs DQN&lt;/em&gt;
&lt;p&gt;
&lt;p&gt;&lt;em&gt;Image cited from &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Given the network, what&amp;rsquo;s the loss function? From the basic $Q$ learning, the $Q$ value is updated
via $Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a&amp;rsquo;}Q(s&amp;rsquo;, a&amp;rsquo;) - Q(s, a)]$. This indicates that we are actually
&amp;ldquo;learning&amp;rdquo; the target $r + \gamma \max_{a&amp;rsquo;}Q(s&amp;rsquo;, a&amp;rsquo;)$. Therefore, the loss function can be defined as
$$
L(\theta, x) = \frac{1}{N}\sum_{i=1}^N[Q_{\theta}(s_i, a_i) - (r_i + \arg \max_{a&amp;rsquo;}Q_{\theta}(s&amp;rsquo;, a&amp;rsquo;))]^2 \tag{3.4}
$$
given a sample $x = (s_i, a_i, r_i, s&amp;rsquo;_i)$.&lt;/p&gt;
&lt;p&gt;When applying the backpropagation on (3.4), the target value is changing since it also depends on the parameter $\theta$
to be optimized. This differs from the typical deep learning where the label is given and fixed. Learning from a changing label
problem may be very unstable and loss may fluctuate. To tackle this, the target $Q$ network is introduced.&lt;/p&gt;
&lt;p&gt;The idea is to maintain two $Q$ neural nets: $Q_\theta$ and $Q_{\theta ^ *}$. The target net $Q_{\theta ^ *}$ is originally cloned from
the net $Q_\theta$ and the (3.4) is reformatted as&lt;/p&gt;
&lt;p&gt;$$
L(\theta, x) = \frac{1}{N}\sum_{i=1}^N[Q_{\theta}(s_i, a_i) - (r_i + \arg \max_{a&amp;rsquo;}Q_{\theta ^ *}(s&amp;rsquo;, a&amp;rsquo;))]^2 \tag{3.5}
$$&lt;/p&gt;
&lt;p&gt;The parameter $\theta ^ *$ is fixed in (3.5) and only copied from $Q_\theta$ periodically after a few gradient descent runs. In
this way, the loss function is firstly optimized towards a fixed label (only $\theta$ is updated), then labels are updated (copy
$\theta$ to $\theta ^ *$) and optimization continues.&lt;/p&gt;
&lt;p&gt;Gathering the training samples purely from interacting with the system results in dependent samples due to the Markov property. In DQN,
An &lt;strong&gt;experience replay&lt;/strong&gt; (ER) component introduce a &lt;em&gt;replay buffer&lt;/em&gt; that is sampling independent samples $x = (s_i, a_i, r_i, s&amp;rsquo;_i)$,
thus also improving the training efficiency.&lt;/p&gt;
&lt;p&gt;A general structure of DQN is&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;http://localhost:1313/rf/dqn_flow.png&#34; width=&#34;600&#34; height=&#34;400&#34;&gt;&lt;br&gt;
&lt;em&gt;Figure 2: DQN flow&lt;/em&gt;
&lt;p&gt;
&lt;p&gt;&lt;em&gt;Image cited from &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initialize the $Q_{\theta}$&lt;/li&gt;
&lt;li&gt;Initialize the target network $Q_{\theta ^ *}$ by cloning $\theta ^ *$ from $\theta$&lt;/li&gt;
&lt;li&gt;Initialize the replay buffer&lt;/li&gt;
&lt;li&gt;for episode $e$ from 1 to $K$, do:
&lt;ul&gt;
&lt;li&gt;Initialize the state $s$&lt;/li&gt;
&lt;li&gt;for $t$ from 1 to $T$, do:
&lt;ul&gt;
&lt;li&gt;apply the $\epsilon$-greedy strategy to $Q_{\theta}$ to choose the action $a$ based on $s$&lt;/li&gt;
&lt;li&gt;take the action and interact with the environment to get reward $r$ and $s&#39;$&lt;/li&gt;
&lt;li&gt;save the sample (s, a, r, s&amp;rsquo;) to the replay buffer&lt;/li&gt;
&lt;li&gt;if there are enough samples in the replay buffer, sample N data points $\{(s_i, s_i, r_i, s_{i+1})\}_{i=1,\dots,N}$&lt;/li&gt;
&lt;li&gt;for each sample, calculate the target $y_i = r_i + \gamma \arg\max_{a} Q_{\theta ^ *}(s_{i+1}, a)$&lt;/li&gt;
&lt;li&gt;minimize the loss function $L(\theta, x) = \frac{1}{N}\sum_{i=1}^N[y_i - Q_{\theta}(s_i, a_i)]^2$, update $Q_\theta$&lt;/li&gt;
&lt;li&gt;Update $Q_{\theta ^ *}$ every $M$ steps&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;end for&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;end for&lt;/li&gt;
&lt;li&gt;return $Q_{\theta}$ and derive the optimal policy&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;advanced-dqn&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#advanced-dqn&#34;&gt;
        
    &lt;/a&gt;
    Advanced DQN
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;Though DQN extends the problem solving capability from finite states to infinite state space, original DQN still suffers
from issues like overestimation. Below lists the efforts to further refine the DQN.&lt;/p&gt;
&lt;h5 id=&#34;double-dqn&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#double-dqn&#34;&gt;
        
    &lt;/a&gt;
    Double DQN
&lt;/div&gt;
&lt;/h5&gt;
&lt;p&gt;&lt;em&gt;Problem Addressed&lt;/em&gt;: DQN suffers from overestimation bias when updating Q-values because it uses the same network to
select and evaluate actions.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Approach&lt;/em&gt;: Double DQN separates action selection and action evaluation by using the training network to select actions and
the target network to evaluate them. This reduces overestimation bias and leads to more accurate Q-value predictions.&lt;/p&gt;
&lt;h5 id=&#34;duel-dqn&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#duel-dqn&#34;&gt;
        
    &lt;/a&gt;
    Duel DQN
&lt;/div&gt;
&lt;/h5&gt;
&lt;p&gt;&lt;em&gt;Problem Addressed&lt;/em&gt;: Traditional DQNs struggle to efficiently learn the value of states without requiring specific
action-value pairs.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Approach&lt;/em&gt;: Introduces a dueling network architecture that separates the estimation of state values and advantage functions.
This allows the model to learn which states are valuable without needing to learn the effect of each action in those states,
leading to improved learning efficiency.&lt;/p&gt;
&lt;h5 id=&#34;prioritized-experience-replay&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#prioritized-experience-replay&#34;&gt;
        
    &lt;/a&gt;
    Prioritized Experience Replay
&lt;/div&gt;
&lt;/h5&gt;
&lt;p&gt;&lt;em&gt;Problem Addressed&lt;/em&gt;: In standard experience replay, all transitions are sampled with equal probability, which can be
inefficient as some experiences are more valuable for learning.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Approach&lt;/em&gt;: Prioritizes sampling of experiences that have higher expected learning progress, meaning transitions with
a higher TD error are sampled more frequently. This focuses learning on more informative samples, speeding up
the learning process.&lt;/p&gt;
&lt;h5 id=&#34;noisy-nets&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#noisy-nets&#34;&gt;
        
    &lt;/a&gt;
    Noisy Nets
&lt;/div&gt;
&lt;/h5&gt;
&lt;p&gt;&lt;em&gt;Problem Addressed&lt;/em&gt;: Standard exploration methods (like epsilon-greedy) can be suboptimal or inefficient.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Approach&lt;/em&gt;: Replaces deterministic parameters in the neural network with parameterized noise, allowing the network to explore more
effectively by adding noise directly to the weights. This leads to more effective exploration strategies that adapt over time.&lt;/p&gt;
&lt;h5 id=&#34;distributional-dqn&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#distributional-dqn&#34;&gt;
        
    &lt;/a&gt;
    Distributional DQN
&lt;/div&gt;
&lt;/h5&gt;
&lt;p&gt;&lt;em&gt;Problem Addressed&lt;/em&gt;: Traditional DQNs focus on learning the expected return, which can overlook valuable distributional
information about the returns.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Approach&lt;/em&gt;: Instead of estimating the expected value, estimate the distribution of the $Q$ function. This is to capture
the uncertainty of the $Q$ function. The $Q$ function is then calculated as the expected value of the distribution.&lt;/p&gt;
&lt;h5 id=&#34;rainbow-dqn-4&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#rainbow-dqn-4&#34;&gt;
        
    &lt;/a&gt;
    Rainbow DQN &lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h5&gt;
&lt;p&gt;&lt;em&gt;Problem Addressed&lt;/em&gt;: Individual improvements to DQN (like those above) each address specific limitations, but combining
them could lead to even more robust performance.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Approach&lt;/em&gt;: Integrates several enhancements to DQN into a single framework: Double DQN, Duel DQN, Prioritized Experience Replay,
Noisy Nets, Distributional Q-learning, and a few others. By combining these techniques, Rainbow DQN leverages the strengths of
each approach to achieve superior performance in a unified model.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;http://localhost:1313/rf/rainbow.png&#34; width=&#34;500&#34; height=&#34;100&#34;&gt;&lt;br&gt;
&lt;em&gt;Figure 3: Rainbow DQN&lt;/em&gt;
&lt;p&gt;
&lt;p&gt;&lt;em&gt;Image cited from &lt;sup id=&#34;fnref1:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;policy-based&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#policy-based&#34;&gt;
        
    &lt;/a&gt;
    Policy Based
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;Value-based methods (like Q-learning and DQN) focus on estimating the value of state-action pairs to derive a policy.
However, these methods can struggle with high-dimensional or continuous &lt;strong&gt;action spaces&lt;/strong&gt;. Policy-based methods address
these limitations by directly learning the policy itself. In policy based approaches, policies are often represented
using parameterized functions, such as neural networks. These functions map states directly to probabilities over actions,
allowing the agent to choose actions based on the learned distribution.&lt;/p&gt;
&lt;h4 id=&#34;policy-gradient&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#policy-gradient&#34;&gt;
        
    &lt;/a&gt;
    Policy Gradient
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;Suppose $\pi_{\theta}$ is the parameterized policy that is to be optimized, our objective is to maximize the expected
cumulative reward from any starting state&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
$$
J(\theta) = \mathbb{E}_{s_0}[V_{\pi_{\theta}}(s_0)] \tag{4.1}
$$
&lt;/p&gt;
&lt;p&gt;where the expectation is taken over state. Let&amp;rsquo;s derive the derivative of (4.1).&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
\begin{aligned}
\nabla_{\theta} V_{\pi_{\theta}}(s) &amp;= \nabla_{\theta}  \sum_{a \in A} \pi_{\theta}(a \mid s)Q_{\pi_{\theta}}(s, a)\\
&amp;= \sum_{a \in A} (\nabla_{\theta} \pi_{\theta}(a \mid s)Q_{\pi_{\theta}}(s, a) + \pi_{\theta}(a \mid s) \nabla_{\theta} Q_{\pi_{\theta}}(s, a)) \\
&amp;= \sum_{a \in A} (\nabla_{\theta} \pi_{\theta}(a \mid s)Q_{\pi_{\theta}}(s, a) + \pi_{\theta}(a \mid s) \nabla_{\theta} \sum_{s&#39;, r} p(s&#39;, r \mid s, a)(r + \gamma V_{\pi_{\theta}}(s&#39;)) \\
&amp;= \sum_{a \in A} (\nabla_{\theta} \pi_{\theta}(a \mid s)Q_{\pi_{\theta}}(s, a) + \gamma\pi_{\theta}(a \mid s) \sum_{s&#39;, r} p(s&#39;, r \mid s, a)\nabla_{\theta} V_{\pi_{\theta}}(s&#39;) \\
&amp;= \sum_{a \in A} (\nabla_{\theta} \pi_{\theta}(a \mid s)Q_{\pi_{\theta}}(s, a) + \gamma\pi_{\theta}(a \mid s) \sum_{s&#39;} p(s&#39; \mid s, a)\nabla_{\theta} V_{\pi_{\theta}}(s&#39;) \hspace{2.4em} \text{(4.2)}
\end{aligned}
&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s define $\phi(s) = \sum_{a \in A} \nabla_{\theta} \pi_{\theta}(a \mid s)Q_{\pi_{\theta}}(s, a)$ and denote the probability
of getting state $s&amp;rsquo;$ after $k$ steps from state $s$ under the policy $\pi_{\theta}$ as $d_{\pi_{\theta}}(s \rightarrow s&amp;rsquo;, k)$, then continue
with equation (4.2) as&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
\begin{aligned}
\nabla_{\theta} V_{\pi_{\theta}}(s) &amp;= \phi(s) + \gamma\sum_{a}\pi_{\theta}(a \mid s) \sum_{s&#39;} p(s&#39; \mid s, a)\nabla_{\theta} V_{\pi_{\theta}}(s&#39;) \\
&amp;= \phi(s) + \gamma\sum_{a}\sum_{s&#39;}\pi_{\theta}(a \mid s)  p(s&#39; \mid s, a)\nabla_{\theta} V_{\pi_{\theta}}(s&#39;) \\
&amp;= \phi(s) + \gamma\sum_{s&#39;}d_{\pi_{\theta}}(s \rightarrow s&#39;, 1)\nabla_{\theta} V_{\pi_{\theta}}(s&#39;) \\
&amp;= \phi(s) + \gamma\sum_{s&#39;}d_{\pi_{\theta}}(s \rightarrow s&#39;, 1) [\phi(s&#39;) + \gamma\sum_{s&#39;&#39;}d_{\pi_{\theta}}(s&#39; \rightarrow s&#39;&#39;, 1)\nabla_{\theta} V_{\pi_{\theta}}(s&#39;&#39;)] \\
&amp;= \phi(s) + \gamma\sum_{s&#39;}d_{\pi_{\theta}}(s \rightarrow s&#39;, 1) \phi(s&#39;) + \gamma^2\sum_{s&#39;&#39;}d_{\pi_{\theta}}(s \rightarrow s&#39;&#39;, 2)\nabla_{\theta} V_{\pi_{\theta}}(s&#39;&#39;) \\
&amp;= \phi(s) + \gamma\sum_{s&#39;}d_{\pi_{\theta}}(s \rightarrow s&#39;, 1) \phi(s&#39;) + \gamma^2\sum_{s&#39;&#39;}d_{\pi_{\theta}}(s \rightarrow s&#39;&#39;, 2)\nabla_{\theta} V_{\pi_{\theta}}(s&#39;&#39;) + \gamma^3\sum_{s&#39;&#39;&#39;}d_{\pi_{\theta}}(s \rightarrow s&#39;&#39;&#39;, 3)\nabla_{\theta} V_{\pi_{\theta}}(s&#39;&#39;&#39;)] \\
&amp;= \dots \\
&amp;= \sum_{x \in S} \sum_{k=0}^{\infty} \gamma^k d_{\pi_{\theta}}(s \rightarrow x, k)\phi(x) \hspace{16 em} \text{(4.3)}
\end{aligned}
&lt;/p&gt;
&lt;p&gt;Now the gradient of the expected cumulative reward function is&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
\begin{aligned}
\nabla_{\theta} J_{\pi_{\theta}}(s) &amp;= \nabla_{\theta} \mathbb{E}_{s_0} [V_{\pi_{\theta}}(s_0)] \\
&amp;= \sum_{s \in S} \mathbb{E}_{s_0} [\sum_{k=0}^{\infty} \gamma^k d_{\pi_{\theta}}(s_0 \rightarrow s, k)\phi(s)] \\
&amp;= \sum_{s \in S} \eta(s)\phi(s), \hspace{1 em} (\eta(s) = \mathbb{E}_{s_0} [\sum_{k=0}^{\infty} \gamma^k d_{\pi_{\theta}}(s_0 \rightarrow s, k)]) \\
&amp;= (\sum_{s \in S} \eta(s)) \sum_{s \in S} \frac{\eta(s)}{\sum_{s \in S} \eta(s)} \phi(s) \\
&amp; \propto \sum_{s \in S} \frac{\eta(s)}{\sum_{s \in S} \eta(s)} \phi(s) \\
&amp;= \sum_{s \in S} \nu(s) \phi(s), \hspace{1 em} (\nu(s) = \frac{\eta(s)}{\sum_{s \in S} \eta(s)}) \\
&amp;= \sum_{s \in S} \nu(s) \sum_{a \in A} Q_{\pi_{\theta}}(s, a) \nabla_{\theta} \pi_{\theta}(a \mid s) \\
&amp;= \sum_{s \in S} \nu(s) \sum_{a \in A} \pi_{\theta}(a \mid s) Q_{\pi_{\theta}}(s, a) \frac{\nabla_{\theta} \pi_{\theta}(a \mid s)}{\pi_{\theta}(a \mid s)} \\
&amp;= \mathbb{E}_{\pi_{\theta}} [Q_{\pi_{\theta}}(s, a) \nabla_{\theta} \log(\pi_{\theta}(a \mid s))] \hspace{15 em} \text{(4.4)}
\end{aligned}
&lt;/p&gt;
&lt;p&gt;Thus, the expected cumulative reward function can be optimized by taking the gradient ascent from (4.4). Note that we may need to estimate
the $Q_{\pi_{\theta}}(s, a)$ when calculating the gradient, a simple way is to apply Monte Carlo here and thus results in &amp;ldquo;REINFORCE&amp;rdquo; algorithm.&lt;/p&gt;
&lt;h4 id=&#34;reinforce&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#reinforce&#34;&gt;
        
    &lt;/a&gt;
    REINFORCE
&lt;/div&gt;
&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Initialize the policy parameter $\theta$&lt;/li&gt;
&lt;li&gt;for episode $e$ from 1 to $K$, do:
&lt;ul&gt;
&lt;li&gt;Initialize the state $s$&lt;/li&gt;
&lt;li&gt;for $t$ from 1 to $T$, do:
&lt;ul&gt;
&lt;li&gt;sample the trajectories $\{s_1, a_1, r_1, \dots, s_T, a_T, r_T\}$ under the policy $\pi_{\theta}$&lt;/li&gt;
&lt;li&gt;For any $t \in [1, T]$, calculate the total reward since time $t$, $\psi_t = \sum_{t&amp;rsquo;=t}^T \gamma^{t&amp;rsquo;-t}r_{t&amp;rsquo;}$&lt;/li&gt;
&lt;li&gt;apply gradient ascent to update $\theta$, i.e. $\theta = \theta + \alpha \sum_t^T \psi_t \nabla_{\theta} \pi_{\theta}(a_t \mid s_t)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;end for&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;end for&lt;/li&gt;
&lt;li&gt;return the policy $\pi_\theta$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is a &amp;ldquo;policy version&amp;rdquo; monte carlo approach. Like the Monte Carlo algorithm, it&amp;rsquo;s a simple online algorithm but may suffer from issues like
high variance, delayed rewards, inefficient sampling, noisy updates and non-stationary returns. To tackle these issues, more dedicated tools
are needed.&lt;/p&gt;
&lt;h4 id=&#34;trust-region-policy-optimization-trpo-7&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#trust-region-policy-optimization-trpo-7&#34;&gt;
        
    &lt;/a&gt;
    Trust Region Policy Optimization (TRPO) &lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;When using a deep neural network to fit the policy network, the policy gradient updates can be noisy with high variance. Thus resulting in
worse policy updates due to the fluctuation parameters. Is there any way to guarantee the monotonicity of the parameter updates?&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s rephrase the problem as something like this: Suppose the current policy is $\pi_{\theta}$ with parameter $\theta$, we would like
to search for a new parameter $\theta&amp;rsquo;$ such that $J(\theta&amp;rsquo;) \geq J(\theta)$.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
\begin{aligned}
J(\theta&#39;) - J(\theta) &amp;= \mathbb{E}_{s_0}[V_{\pi_{\theta&#39;}}(s_0)] - \mathbb{E}_{s_0}[V_{\pi_{\theta}}(s_0)] \\
&amp;= \mathbb{E}_{\pi_{\theta&#39;}}[\sum_{t=0}^{\infty} \gamma^{t} r(s_{t}, a_{t})] - \mathbb{E}_{\pi_{\theta&#39;}}[\sum_{t=0}^{\infty} \gamma^{t} V_{\pi_{\theta}}(s_t) - \sum_{t=1}^{\infty} \gamma^{t} V_{\pi_{\theta}}(s_t)] \hspace{3 em} \text{(4.5)} \\
&amp;= \mathbb{E}_{\pi_{\theta&#39;}}[\sum_{t=0}^{\infty} \gamma^{t} r(s_{t}, a_{t})] + \mathbb{E}_{\pi_{\theta&#39;}}[\sum_{t=0}^{\infty} \gamma^{t} (\gamma V_{\pi_{\theta}}(s_{t+1}) - V_{\pi_{\theta}}(s_t))] \\
&amp;= \mathbb{E}_{\pi_{\theta&#39;}}[\sum_{t=0}^{\infty} \gamma^{t} [r(s_{t}, a_{t}) + \gamma V_{\pi_{\theta}}(s_{t+1}) - V_{\pi_{\theta}}(s_t)]] \\
&amp;= \mathbb{E}_{\pi_{\theta&#39;}}[\sum_{t=0}^{\infty} \gamma^{t} A_{\pi_{\theta}}(s_{t}, a_{t})], \hspace{1 em} (A_{\pi_{\theta}}(s_{t}, a_{t}) = Q_{\pi_{\theta}}(s_{t}, a_{t}) - V_{\pi_{\theta}}(s_{t}))\\ 
&amp;= \sum_{t=0}^{\infty} \gamma^{t} \mathbb{E}_{s_{t} \sim P_{t}^{\pi_{\theta&#39;}}} \mathbb{E}_{a_{t} \sim \pi_{\theta&#39;(\cdot \mid s_{t})}} [A_{\pi_{\theta}}(s_{t}, a_{t})] \\
&amp;= \frac{1}{1 - \gamma} \mathbb{E}_{s_{t} \sim \nu_{t}^{\pi_{\theta&#39;}}} \mathbb{E}_{a_{t} \sim \pi_{\theta&#39;(\cdot \mid s_{t})}} [A_{\pi_{\theta}}(s_{t}, a_{t})] \hspace{12 em} \text{(4.6)}
\end{aligned}
&lt;/p&gt;
where the equation (4.5) holds because the start state $s_0$ does not depend on policy $\pi_{\theta&#39;}$, thus the expectation can be 
rewritten under the policy $\pi_{\theta&#39;}$. In (4.6), we applied that $\nu_{t}^{\pi_{\theta}} = (1 - \gamma) \sum_{t=0}^{\infty} \gamma^{t} P_{t}^{\pi_{\theta}}$.
&lt;p&gt;Therefore, the goal is to find a new policy such that equation (4.6) is non-negative to guarantee the monotonicity property.&lt;/p&gt;
&lt;p&gt;However, it&amp;rsquo;s challenging to solve (4.6) directly as $\pi_{\theta&amp;rsquo;}$ are being used to update policy strategy and sampling states simultaneously. A simple trick is to apply importance sampling using
the old policy assuming new policy is kind of &amp;ldquo;similar&amp;rdquo; to the old one. Thus, the objective function in TPRO is&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
\begin{aligned}
L_{TPRO}(\theta&#39;) &amp;= \frac{1}{1 - \gamma} \mathbb{E}_{s_{t} \sim \nu_{t}^{\pi_{\theta}}} \mathbb{E}_{a_{t} \sim \pi_{\theta&#39;(\cdot \mid s_{t})}} [A_{\pi_{\theta}}(s_{t}, a_{t})] \\
&amp;= \frac{1}{1 - \gamma} \mathbb{E}_{s_{t} \sim \nu_{t}^{\pi_{\theta}}} \mathbb{E}_{a_{t} \sim \pi_{\theta(\cdot \mid s_{t})}} [\frac{\pi_{\theta&#39;(a \mid s_{t})}}{\pi_{\theta(a \mid s_{t})}} A_{\pi_{\theta}}(s_{t}, a_{t})]
\end{aligned}
&lt;/p&gt;
&lt;p&gt;TPRO is actually trying to solve the following optimization problem.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
$$
\max_{\theta&#39;} \mathbb{E}_{s_{t} \sim \nu_{t}^{\pi_{\theta}}} \mathbb{E}_{a_{t} \sim \pi_{\theta(\cdot \mid s_{t})}} [\frac{\pi_{\theta&#39;(a \mid s_{t})}}{\pi_{\theta(a \mid s_{t})}} A_{\pi_{\theta}}(s_{t}, a_{t})] \tag{4.7}
$$
&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
$$
s.t. \mathbb{E}_{s_{t} \sim \nu_{t}^{\pi_{\theta}}} [D_{KL}( \pi_{\theta(\cdot \mid s_{t})}, \pi_{\theta&#39;(\cdot \mid s_{t})})] \leq \delta \tag{4.8}
$$
&lt;/p&gt;
&lt;p&gt;To approximately solve the optimization problem, we can apply the Taylor approximation to the objective and its constraint like this.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
$$
\mathbb{E}_{s_{t} \sim \nu_{t}^{\pi_{\theta}}} \mathbb{E}_{a_{t} \sim \pi_{\theta(\cdot \mid s_{t})}} [\frac{\pi_{\theta&#39;(a \mid s_{t})}}{\pi_{\theta_k(a \mid s_{t})}} A_{\pi_{\theta_k}}(s_{t}, a_{t})] \approx g^T(\theta&#39; - \theta_k) \tag{4.9}
$$
&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
$$
\mathbb{E}_{s_{t} \sim \nu_{t}^{\pi_{\theta_k}}} [D_{KL}( \pi_{\theta_k(\cdot \mid s_{t})}, \pi_{\theta&#39;(\cdot \mid s_{t})})] \approx \frac{1}{2} (\theta&#39; - \theta_k)^T H(\theta&#39; - \theta_k) \tag{4.10}
$$
&lt;/p&gt;
where $g$ denotes the gradient of the left hand side of equation (4.9) and $H$ represents the Hessian matrix of the left hand side of equation (4.10). The optimization problem can be solved by the conjugate gradient method with
the following formula $\theta_{k+1} = \theta_k + \sqrt{\frac{2\delta}{x^T Hx}}x$.
&lt;p&gt;Therefore, the general TRPO algorithm is&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;initialize the policy network parameters $\theta$ and value network parameters $\omega$&lt;/li&gt;
&lt;li&gt;for episode $e$ from 1 to $E$, do:
&lt;ul&gt;
&lt;li&gt;sample the trajectories $\{s_1, a_1, r_1, \dots, \}$ under the policy $\pi_{\theta}$&lt;/li&gt;
&lt;li&gt;calculate the advantage $A(s_t, a_t)$ for each state, action pair. $A_t^{GAE} = \sum_{l=0}^{\infty} (\gamma \lambda)^l (r_{t+1} + \gamma V_{\omega}(s_{t+2}) - V_{\omega}(s_{t+1})), \lambda \in [0, 1]$&lt;/li&gt;
&lt;li&gt;calculate the gradient $g$ of the objective function&lt;/li&gt;
&lt;li&gt;calculate the $x = H^{-1}g$&lt;/li&gt;
&lt;li&gt;Find $i \in \{1,2,\dots,K\}$ and update the policy network parameter $\theta_{k+1} = \theta_k + \alpha^{i}\sqrt{\frac{2\delta}{x^T Hx}}x, \alpha \in (0, 1)$&lt;/li&gt;
&lt;li&gt;Update the value network parameters by minimizing the square error:&lt;/li&gt;
&lt;/ul&gt;
&lt;p align=&#34;center&#34;&gt;
$$ L(\omega) = \frac{1}{2} \mathbb{E}_t [G_t - V_{\omega} (s_t)]^2 $$
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;end for&lt;/li&gt;
&lt;li&gt;return the policy $\pi_\theta$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;proximal-policy-optimization-ppo8&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#proximal-policy-optimization-ppo8&#34;&gt;
        
    &lt;/a&gt;
    Proximal Policy Optimization (PPO)&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;Though TRPO is successful in many cases, the computation can be time consuming due to its complexity. To simplify the optimization process, PPO is taking the following two approaches for the
objective (4.7) with the constraint (4.8).&lt;/p&gt;
&lt;h5 id=&#34;ppo-penalty&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#ppo-penalty&#34;&gt;
        
    &lt;/a&gt;
    PPO-penalty
&lt;/div&gt;
&lt;/h5&gt;
&lt;p&gt;Instead of optimizing a constraint objective, we can transform the objective (4.7) into an un-constraint optimization problem by using Lagrange multipliers.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
$$
\max_{\theta} \mathbb{E}_{s \sim \nu_{t}^{\pi_{\theta_k}}} \mathbb{E}_{a \sim \pi_{\theta_k(\cdot \mid s)}} [\frac{\pi_{\theta(a \mid s)}}
{\pi_{\theta_k(a \mid s)}} A_{\pi_{\theta_k}}(s, a) - \beta D_{KL}( \pi_{\theta_k(a \mid s)}, \pi_{\theta(a \mid s)})] \tag{4.11}
$$
&lt;/p&gt;
where $d_k = D_{KL}^{\nu_{t}^{\pi_{\theta_k}}}(\pi_{\theta_k}, \pi_\theta)$ denotes the DL divergence between policies in two consecutive iterations. $\beta$ can be updated according to
&lt;pre&gt;&lt;code&gt;if d_k &amp;lt; delta / 1.5:
  beta_{k+1} = beta_k /2
elif d_k &amp;gt; delta * 1.5:
  beta_{k+1} = beta_k * 2
else:
  beta_{k+1} = beta_{k}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where $\delta$ is a hyper-parameter which is set in the beginning of learning.&lt;/p&gt;
&lt;h5 id=&#34;ppo-clip&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#ppo-clip&#34;&gt;
        
    &lt;/a&gt;
    PPO-Clip
&lt;/div&gt;
&lt;/h5&gt;
&lt;p&gt;The other way of joining the constraint into the objective is using clips, that is, to set boundaries for the objective function.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
$$
\max_{\theta} \mathbb{E}_{s \sim \nu_{t}^{\pi_{\theta_k}}} \mathbb{E}_{a \sim \pi_{\theta_k(\cdot \mid s)}} \left[min \\
\left(\frac{\pi_{\theta(a \mid s)}}{\pi_{\theta_k(a \mid s)}} A_{\pi_{\theta_k}}(s, a), clip \left(\frac{\pi_{\theta(a \mid s)}}
{\pi_{\theta_k(a \mid s)}}, 1-\epsilon, 1+\epsilon \right)A_{\pi_{\theta_k}}(s, a)\right) \right] \tag{4.12}
$$
&lt;/p&gt;
where $clip(x, a, b) = max(min(x, b), a)$ and $\epsilon$ is a hyper-parameter. This makes the policy updates to be within the $[1-\epsilon, 1+\epsilon]$.
&lt;p&gt;Therefore the sudo code for PPO is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;initialize the policy network parameters $\theta$ and value network parameters $\omega$&lt;/li&gt;
&lt;li&gt;for episode $e$ from 1 to $E$, do:
&lt;ul&gt;
&lt;li&gt;sample the trajectories $\{s_1, a_1, r_1, \dots, \}$ under the policy $\pi_{\theta}$&lt;/li&gt;
&lt;li&gt;calculate the advantage $A(s_t, a_t)$ for each state, action pair. $A_t^{GAE} = \sum_{l=0}^{\infty} (\gamma \lambda)^l (r_{t+1} + \gamma V_{\omega}(s_{t+2}) - V_{\omega}(s_{t+1})), \lambda \in [0, 1]$&lt;/li&gt;
&lt;li&gt;Compute discounted cumulative rewards:$ G_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \dots$&lt;/li&gt;
&lt;li&gt;calculate the gradient $g$ of the objective function&lt;/li&gt;
&lt;li&gt;update the policy network using stochastic gradient ascent&lt;/li&gt;
&lt;li&gt;Update the value network parameters by minimizing the square error:&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p align=&#34;center&#34;&gt;
$$ L(\omega) = \frac{1}{2} \mathbb{E}_t (G_t - V_{\omega} (s_t))^2 $$
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;end for&lt;/li&gt;
&lt;li&gt;return the policy $\pi_\theta$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hybrid&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#hybrid&#34;&gt;
        
    &lt;/a&gt;
    Hybrid
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;A hybrid approach in reinforcement learning combines elements from both value-based and policy-based methods to leverage
their respective strengths while mitigating their weaknesses.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Value-based methods (e.g., Q-Learning, DQN) focus on learning a value function (like $Q(s, a)$) to guide decision-making
but struggle in high-dimensional or continuous action spaces.&lt;/li&gt;
&lt;li&gt;Policy-based methods (e.g., REINFORCE) directly learn a policy $\pi(a|s)$, which works well for continuous actions but
suffers from high variance in gradient estimation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By integrating these approaches, a hybrid method learns both the value function (to stabilize learning and reduce variance)
and the policy (to directly optimize actions). Actor-Critic algorithms, like A2C, PPO, and SAC, are popular examples of
hybrid methods, combining a critic (value function) to evaluate actions and an actor (policy) to select actions. This synergy
improves learning efficiency, stability, and scalability to complex environments.&lt;/p&gt;
&lt;h4 id=&#34;actor-critic-ac9&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#actor-critic-ac9&#34;&gt;
        
    &lt;/a&gt;
    Actor Critic (AC)&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;Actor-Critic is a class of reinforcement learning (RL) algorithms that combines the benefits of policy-based methods
(like REINFORCE) and value-based methods (like Q-learning). It is a hybrid approach where two components — an actor and
a critic — work together to optimize the policy. There are two components: actor and critic.&lt;/p&gt;
&lt;p&gt;The actor is responsible for learning and outputting the policy, $\pi_\theta(a|s)$, which is a mapping from states to actions.
It is a parameterized function (e.g., a neural network) with parameters $\theta.$
Its goal is to directly improve the policy by maximizing the expected reward. The policy gradient (4.4) can be extended to&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
$$g = \mathbb{E}_{\pi_{\theta}} [A_{\pi_{\theta}}(s, a) \nabla_{\theta} \log(\pi_{\theta}(a \mid s))] \tag{4.13}$$
&lt;/p&gt;
&lt;p&gt;when introducing the value function $V_\pi(s)$ as baseline, and the advantage function $A_{\pi_{\theta}}(s, a)$ is usually
approximated by the temporal difference $\delta_t = r_t + \gamma V_\omega(s_{t+1}) - V_\omega(s_t)$.&lt;/p&gt;
&lt;p&gt;The critic evaluates how good the actions taken by the actor are, using a value function.
Common choices for the value function:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;State Value Function: $V_\pi(s) = \mathbb{E}_\pi [G_t | s_t = s]$&lt;/li&gt;
&lt;li&gt;Action-Value Function: $Q_\pi(s, a) = \mathbb{E}_\pi [G_t | s_t = s, a_t = a]$&lt;/li&gt;
&lt;li&gt;Advantage Function: $A_\pi(s, a) = Q_\pi(s, a) - V_\pi(s)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The critic guides the actor by providing feedback on its actions. When updating the critic net, the loss can be defined by temporal difference, i.e.
$L_\omega = \frac{1}{2}(r_t + \gamma V_\omega(s_{t+1}) - V_\omega(s_t))^2$. Therefore the gradient of critic loss
is $$\nabla_\omega L_\omega = -(r_t + \gamma V_\omega(s_{t+1}) - V_\omega(s_t))\nabla_\omega V_\omega(s_t) \tag{4.14}$$ Both the actor and critic loss
are thus optimized by gradient descent.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;http://localhost:1313/rf/ac.png&#34; width=&#34;300&#34; height=&#34;150&#34;&gt;&lt;br&gt;
&lt;em&gt;Figure 4: Actor Critic&lt;/em&gt;
&lt;p&gt;
&lt;p&gt;&lt;em&gt;Image cited from &lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In all, the sudo code for actor-critic is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;initialize the policy network parameters $\theta$ and value network parameters $\omega$&lt;/li&gt;
&lt;li&gt;for episode $e$ from 1 to $E$, do:
&lt;ul&gt;
&lt;li&gt;sample the trajectories $\{s_1, a_1, r_1, \dots, \}$ under the policy $\pi_{\theta}$&lt;/li&gt;
&lt;li&gt;calculate the temporal difference by $\delta_t = r_t + \gamma V_\omega(s_{t+1}) - V_\omega(s_t)$&lt;/li&gt;
&lt;li&gt;calculate the gradient (4.13) and update the policy net parameters by $\theta$&lt;/li&gt;
&lt;li&gt;calculate the gradient (4.14) and update the value net parameters by $\omega$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;end for&lt;/li&gt;
&lt;li&gt;return the policy $\pi_\theta$ and value function $V_\omega$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Actor-Critic serves as the foundation for many advanced RL algorithms and is widely used in solving complex decision-making problems.
For example, PPO (Proximal Policy Optimization) is an variant that improves stability by constraining the policy update step.
SAC (Soft Actor-Critic) extends actor-critic by incorporating entropy regularization to encourage exploration.&lt;/p&gt;
&lt;h4 id=&#34;ddpg11&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#ddpg11&#34;&gt;
        
    &lt;/a&gt;
    DDPG&lt;sup id=&#34;fnref:9&#34;&gt;&lt;a href=&#34;#fn:9&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;Reinforce, actor-critic, TRPO and PPO are all about on-policy learning algorithms, while DQN is off-policy algorithm, it suffered from the discrete action space.
Deep Deterministic Policy Gradient (DDPG) is a model-free, off-policy reinforcement learning algorithm designed for continuous action spaces.
It combines ideas from Deterministic Policy Gradient (DPG) and Deep Q-Learning (DQN), leveraging neural networks to approximate policies and value functions.&lt;/p&gt;
&lt;p&gt;DDPG leverages the Actor-Critic Architecture. However, some unique key features in DDPG include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Actor: Learns a deterministic policy $\mu_\theta(s)$ that maps states directly to actions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Critic: Estimates the action-value function $Q(s, a)$, which evaluates the quality of actions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Off-Policy Learning: Uses a replay buffer to store past experiences $(s, a, r, s&amp;rsquo;)$, enabling sample efficiency and breaking correlations between samples.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Target Networks: Employs target networks for both the actor and critic to stabilize learning by slowly updating their parameters towards the current networks.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Continuous Action Space:Unlike DQN, which handles discrete actions, DDPG can handle high-dimensional continuous actions, making it suitable for tasks like robotics control.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;DDPG sudo Algorithm:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initialize the actor network $\mu_\theta(s)$ and the critic network $Q_\phi(s, a)$ with random weights.&lt;/li&gt;
&lt;li&gt;Create target networks $\mu_{\theta&amp;rsquo;}(s)$ and $Q_{\phi&amp;rsquo;}(s, a)$ by copying the weights of the actor and critic networks.&lt;/li&gt;
&lt;li&gt;Initialize a replay buffer to store transitions $(s, a, r, s&amp;rsquo;)$.&lt;/li&gt;
&lt;li&gt;For each episode:
&lt;ul&gt;
&lt;li&gt;At each step:
&lt;ul&gt;
&lt;li&gt;Select an action using the actor network with added noise for exploration: $a_t = \mu_\theta(s_t) + \mathcal{N}_t$ (where $\mathcal{N}_t$ is a noise process like Ornstein-Uhlenbeck).&lt;/li&gt;
&lt;li&gt;Execute the action, observe reward $r_t$ and the next state $s_{t+1}$.&lt;/li&gt;
&lt;li&gt;Store the transition $(s_t, a_t, r_t, s_{t+1})$ in the replay buffer.&lt;/li&gt;
&lt;li&gt;Sample a minibatch of transitions from the replay buffer.&lt;/li&gt;
&lt;li&gt;Update the critic by minimizing the Bellman error: $\mathcal{L}(\phi) = \mathbb{E}\left[\left(Q_\phi(s, a) - \left(r + \gamma Q_{\phi&amp;rsquo;}(s&amp;rsquo;, \mu_{\theta&amp;rsquo;}(s&amp;rsquo;))\right)\right)^2\right]$&lt;/li&gt;
&lt;li&gt;Update the actor using the deterministic policy gradient: $\nabla_\theta J = \mathbb{E}\left[\nabla_\theta \mu_\theta(s) \nabla_a Q_\phi(s, a) \big|{a=\mu_\theta(s)} \right]$&lt;/li&gt;
&lt;li&gt;Update the target networks:
&lt;ul&gt;
&lt;li&gt;$\phi&amp;rsquo; \leftarrow \tau \phi + (1 - \tau) \phi&#39;$&lt;/li&gt;
&lt;li&gt;$\theta&amp;rsquo; \leftarrow \tau \theta + (1 - \tau) \theta&#39;$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Repeat until convergence.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;sac10&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#sac10&#34;&gt;
        
    &lt;/a&gt;
    SAC&lt;sup id=&#34;fnref:10&#34;&gt;&lt;a href=&#34;#fn:10&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;10&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;Though DDPG is able to learn an off-policy strategy, it&amp;rsquo;s not robust to converge and is usually very sensitive to hyper-parameters. Soft Actor-Critic (SAC)
is a &lt;strong&gt;model-free&lt;/strong&gt;, &lt;strong&gt;off-policy&lt;/strong&gt; reinforcement learning algorithm designed for &lt;strong&gt;continuous action spaces&lt;/strong&gt;. SAC improves traditional Actor-Critic methods by
incorporating &lt;strong&gt;entropy regularization&lt;/strong&gt; to encourage exploration and improve stability.&lt;/p&gt;
&lt;p&gt;Key Features of SAC&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Maximum Entropy Framework:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SAC maximizes a combination of reward and policy entropy:
$J(\pi) = \mathbb{E}\left[\sum_{t=0}^\infty \gamma^t \left(r(s_t, a_t) + \alpha \mathcal{H}(\pi(\cdot|s_t))\right)\right]$
where $\mathcal{H}(\pi(\cdot|s_t))$ is the entropy of the policy, and $\alpha$ is a temperature parameter balancing exploration and exploitation.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stochastic Policy:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SAC uses a stochastic policy $\pi_\phi(a|s)$ parameterized by a neural network, improving exploration compared to deterministic policies.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Off-Policy Learning:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SAC uses a replay buffer to store and reuse past experiences, enhancing sample efficiency.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Critic Networks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Two Q-value networks ($Q_{\theta_1}$ and $Q_{\theta_2}$) are used to reduce overestimation bias.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Target Networks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Slowly updated target Q-networks ($Q_{\theta_1&amp;rsquo;}$ and $Q_{\theta_2&amp;rsquo;}$) improve training stability.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;SAC sudo algorithm:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initialize:
&lt;ul&gt;
&lt;li&gt;Policy network $\pi_\phi(a|s)$.&lt;/li&gt;
&lt;li&gt;Two Q-value networks $Q_{\theta_1}(s, a)$ and $Q_{\theta_2}(s, a)$.&lt;/li&gt;
&lt;li&gt;Target Q-value networks $Q_{\theta_1&amp;rsquo;}$ and $Q_{\theta_2&amp;rsquo;}$.&lt;/li&gt;
&lt;li&gt;Replay buffer $\mathcal{D}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;For each episode:
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;At each time step:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sample an action $a_t \sim \pi_\phi(a|s_t)$.&lt;/li&gt;
&lt;li&gt;Execute the action, observe reward $r_t$ and next state $s_{t+1}$.&lt;/li&gt;
&lt;li&gt;Store the transition $(s_t, a_t, r_t, s_{t+1})$ in the replay buffer.&lt;/li&gt;
&lt;li&gt;Sample a minibatch of transitions $(s, a, r, s&amp;rsquo;)$ from the replay buffer.&lt;/li&gt;
&lt;li&gt;Compute the target Q-value:
$ y = r + \gamma \left[\min_{i=1,2} Q_{\theta_i&amp;rsquo;}(s&amp;rsquo;, a&amp;rsquo;) - \alpha \log \pi_\phi(a&amp;rsquo;|s&amp;rsquo;)\right]$
where $a&amp;rsquo; \sim \pi_\phi(\cdot|s&amp;rsquo;)$.&lt;/li&gt;
&lt;li&gt;Update the Q-value networks by minimizing:&lt;/li&gt;
&lt;/ul&gt;
 &lt;p align=&#34;center&#34;&gt;
 $\mathcal{L}_Q = \frac{1}{N} \sum \left(Q_{\theta_i}(s, a) - y\right)^2$
 &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Update the policy network using:&lt;/li&gt;
&lt;/ul&gt;
 &lt;p align=&#34;center&#34;&gt;
 $\mathcal{L}_\pi = \mathbb{E}_{s \sim \mathcal{D}, a \sim \pi_\phi} \left[\alpha \log \pi_\phi(a|s) - Q_{\theta_1}(s, a)\right]$
 &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Optionally, adjust the temperature parameter $\alpha$ to control entropy:&lt;/li&gt;
&lt;/ul&gt;
 &lt;p align=&#34;center&#34;&gt;
 $\mathcal{L}_\alpha = \mathbb{E}_{a \sim \pi_\phi} \left[-\alpha \left(\log \pi_\phi(a|s) + \mathcal{H}_\text{target}\right)\right]$
 &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Update the target Q-value networks:&lt;/li&gt;
&lt;/ul&gt;
 &lt;p align=&#34;center&#34;&gt;
 $\theta_i&#39; \leftarrow \tau \theta_i + (1 - \tau) \theta_i&#39;$
 &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Repeat until convergence.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In all, Reinforcement learning (RL) algorithms can be broadly categorized into value-based, policy-based, and actor-critic approaches.
Value-based methods, such as Q-learning and DQN (Deep Q-Network), focus on estimating value functions to derive optimal policies,
excelling in discrete action spaces. Policy-based methods, like REINFORCE, directly optimize the policy, making them suitable for
high-dimensional or continuous action spaces, though they can suffer from high variance. Actor-critic methods, such as A3C and PPO,
combine the strengths of value and policy-based approaches by using an actor to decide actions and a critic to evaluate them, leading
to better stability and efficiency. Advanced algorithms like DDPG and SAC extend RL to continuous control tasks by incorporating techniques
such as deterministic policies, entropy regularization, and off-policy training. Each algorithm is tailored to specific challenges,
balancing exploration, stability, and scalability in diverse RL environments. Below shows an overview of the RL algorithms.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;http://localhost:1313/rf/RL_algorithms.png&#34; width=&#34;800&#34; height=&#34;550&#34;&gt;&lt;br&gt;
&lt;em&gt;Figure 5: Taxonomy of Reinforcement Learning Algorithms&lt;/em&gt;
&lt;p&gt;
&lt;p&gt;&lt;em&gt;Image cited from &lt;sup id=&#34;fnref:11&#34;&gt;&lt;a href=&#34;#fn:11&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;11&lt;/a&gt;&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;citation&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#citation&#34;&gt;
        
    &lt;/a&gt;
    Citation
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;If you find this post helpful, please consider citing it as:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bibtex&#34; data-lang=&#34;bibtex&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f3f99d&#34;&gt;@article&lt;/span&gt;{&lt;span style=&#34;color:#ff5c57&#34;&gt;wang2024rflearningdeep&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;author&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;Bing Wang&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;title&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;A Journey to Reinforcement Learning - Deep RL&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;journal&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;bingzw.github.io&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;year&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;2024&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;month&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;July&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;url&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;https://bingzw.github.io/posts/2024-07-15-rf-learning-deep/&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;or&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Bing Wang. (2024, July). A Journey to Reinforcement Learning - Deep RL. 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;https://bingzw.github.io/posts/2024-07-15-rf-learning-deep/
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;reference&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#reference&#34;&gt;
        
    &lt;/a&gt;
    Reference
&lt;/div&gt;
&lt;/h2&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cl.cam.ac.uk/~ey204/teaching/ACS/R244_2018_2019/papers/mao_HOTNETS_2016.pdf&#34;&gt;Mao, Hongzi, et al. &amp;ldquo;Resource management with deep reinforcement learning.&amp;rdquo; Proceedings of the 15th ACM workshop on hot topics in networks. 2016&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.researchgate.net/profile/Silvia-Ullo/publication/351884746_A_Deep_Q-Learning_based_approach_applied_to_the_Snake_game/links/6140bf81dabce51cf451e5cd/A-Deep-Q-Learning-based-approach-applied-to-the-Snake-game.pdf&#34;&gt;Sebastianelli, Alessandro, et al. &amp;ldquo;A Deep Q-Learning based approach applied to the Snake game.&amp;rdquo; 2021 29th Mediterranean Conference on Control and Automation (MED). IEEE, 2021&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://pdf.sciencedirectassets.com/280203/1-s2.0-S1877050920X00135/1-s2.0-S1877050920317270/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEB8aCXVzLWVhc3QtMSJHMEUCIBh17YAsP3s4HO6Gu91yQrt0YzGZg43Bdw25VI2dHM3XAiEApEVuGJv4Mo%2Fk0EWe7Q49Q3v6TzM6q4uR1RnX8P80fJgqvAUI%2BP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAFGgwwNTkwMDM1NDY4NjUiDCKuNwv3%2BpSUgMzNZiqQBRwpKXgGARLCnqPuT15k0Smwr8r%2BQFH3S2wvseq9nLLFylSoZOfWzALE1Ln6c4WxsfjyqefZgDrdt%2FjcLiSwd%2BsiAYSZIt47t%2F%2Fu4jSgJ17To2MtSTHQOH9AH6IUMu93FLsvhnfHy0YfflxASGjijWMkldB9ogxAnquDkgaT1OqEa6QmZnvDlIaLSsLD5%2B8NPauGfzw4SAk0xEMG5YjqsKkJWD%2BhYjWAhEJreTUYMq0oKNz6Lg%2Bh8csxqOoHafYYLEbDXWcZxDAlcudjz0uCzi9PJlErsxmJUaUsOupBYG7O252KxbIQMZ1nVw9M3pp6Ihg0SiEQJYcm178Yq1k8MpndZO7HTLXRuQOpSCQi1YdEw8j8r84QJLcfME1EWbhrPezxXGMwSGHYJ3wF3hMSNNrcG1eQbmKSZpLZLHYx1ewiLwejT3WrKM0M7ogaOLmC854ng5izD0s3CCd%2BKos1TViNFQb1xvp%2BHKceFJ6QzPueqpvTCNOk7c9Wt6fosl4ZHsj%2FYyGKvLtnB3OOiEMQ52OSRYvmXICTlAK0PoNwWXayA%2BLhukKRrB4g%2FXPQRE%2FShS1O495GdUcukvQpbH4%2FcxgI6dw5lj3gwVWcj%2Fga9VBIHmx3RbWKexaupwqmBZWOY%2BOZCT%2FLpZla39lQ7xkIq9wSQV25%2BBzdt96va5ffpD%2F9sfm6KmISyWLjJcgCVkSwQVOJbSdPA%2BhMTfPreci3%2BBQ19dl1oHIw3HfiigntgdNIyfBH0vJ6Fg4hgR8MX73f743x%2BYSIfATowK9figEaH%2BpEXaWcESof%2FkAlaqx%2FTO59JzOJNB%2B9mFV0QFh2pfkg12mlqDNVBkGQBtdnNvXqgawcszshyL0kzrZJp94qE5%2F4MKPS4bsGOrEBRfodhQxsvkkZwea%2BobFI1jDgYDXWFe8EeYje3%2BHiPc%2BA8ZvWYaTgDjPdwJpcyFRkqvDD3doNPaItbGPKay55vEGE3dkbr%2BUjXw9fL2ONkyNGYz6MSfVrbe4O4HegdXeVKCjnZ4t9EMRAzv73PSWqPDTm3V758HVZU12yK652%2BV8Iy3g7gsMlm%2FgK7hjMz9lJ3UUs1jk9QbDkHSgP2VM%2F3JLDgeo%2BJ5Mhj56o%2FZDlYe4t&amp;amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;amp;X-Amz-Date=20250103T233634Z&amp;amp;X-Amz-SignedHeaders=host&amp;amp;X-Amz-Expires=300&amp;amp;X-Amz-Credential=ASIAQ3PHCVTY3KWOGMCD%2F20250103%2Fus-east-1%2Fs3%2Faws4_request&amp;amp;X-Amz-Signature=9ee9389298c9c7d315fb25d32c13e3e2ec7f2a5910a8707d8ae4f3a8f94e1a61&amp;amp;hash=38da7b42222fc0c6ab1ec2b34e73d1cec90f3979a0929597278c884df5f1a286&amp;amp;host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&amp;amp;pii=S1877050920317270&amp;amp;tid=spdf-19aa14af-b1d8-40de-91fb-4dab2f9971ca&amp;amp;sid=08c9e8946f89c14d13183e673e6b24f8836dgxrqa&amp;amp;type=client&amp;amp;tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&amp;amp;ua=0f155d0900510c08025f50&amp;amp;rr=8fc6cfd05824f642&amp;amp;cc=us&#34;&gt;Muteba, K. F., Karim Djouani, and Thomas O. Olwal. &amp;ldquo;Deep reinforcement learning based resource allocation for narrowband cognitive radio-IoT systems.&amp;rdquo; Procedia Computer Science 175 (2020): 315-324&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/11796&#34;&gt;Hessel, Matteo, et al. &amp;ldquo;Rainbow: Combining improvements in deep reinforcement learning.&amp;rdquo; Proceedings of the AAAI conference on artificial intelligence. Vol. 32. No. 1. 2018.&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref1:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://people.engr.tamu.edu/guni/csce642/files/trpo.pdf&#34;&gt;Schulman, John. &amp;ldquo;Trust Region Policy Optimization.&amp;rdquo; arXiv preprint arXiv:1502.05477 (2015).&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1707.06347&#34;&gt;Schulman, John, et al. &amp;ldquo;Proximal policy optimization algorithms.&amp;rdquo; arXiv preprint arXiv:1707.06347 (2017).&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/6392457&#34;&gt;Grondman, Ivo, et al. &amp;ldquo;A survey of actor-critic reinforcement learning: Standard and natural policy gradients.&amp;rdquo; IEEE Transactions on Systems, Man, and Cybernetics, part C (applications and reviews) 42.6 (2012): 1291-1307&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://medium.com/intro-to-artificial-intelligence/a-link-between-cross-entropy-and-policy-gradient-expression-b2b308511867&#34;&gt;The Actor-Critic Reinforcement Learning algorithm&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:9&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://proceedings.mlr.press/v32/silver14.html&#34;&gt;Silver, David, et al. &amp;ldquo;Deterministic policy gradient algorithms.&amp;rdquo; International conference on machine learning. Pmlr, 2014.&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:9&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:10&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://proceedings.mlr.press/v80/haarnoja18b&#34;&gt;Haarnoja, Tuomas, et al. &amp;ldquo;Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.&amp;rdquo; International conference on machine learning. PMLR, 2018.&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:10&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:11&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://doi.org/10.1007/978-981-15-4095-0_3&#34;&gt;Zhang, H., Yu, T. (2020). Taxonomy of Reinforcement Learning Algorithms. In: Dong, H., Ding, Z., Zhang, S. (eds) Deep Reinforcement Learning. Springer, Singapore.&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:11&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>A Journey to Reinforcement Learning - Tabular Methods</title>
      <link>/posts/2024-07-05-rf-learning-tabular/</link>
      <pubDate>Fri, 05 Jul 2024 20:59:39 -0700</pubDate>
      <author>bingwang8878@gamil.com (Bing Wang)</author>
      <guid>/posts/2024-07-05-rf-learning-tabular/</guid>
      <description>&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;http://localhost:1313/rf/rf.png&#34; width=&#34;600&#34; height=&#34;400&#34;&gt;&lt;br&gt;
&lt;p&gt;
&lt;p&gt;&lt;em&gt;Image cited from &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;basic-problem-statement-of-reinforcement-learning&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#basic-problem-statement-of-reinforcement-learning&#34;&gt;
        
    &lt;/a&gt;
    Basic Problem Statement of Reinforcement Learning
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in
an environment to maximize cumulative reward. Unlike supervised learning, where the model is trained on a fixed dataset,
RL involves learning through interaction with the environment. Let&amp;rsquo;s define some basic elements in RL domain.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Agent&lt;/strong&gt;: The learner or decision maker.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Environment&lt;/strong&gt;: The external system the agent interacts with.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;State&lt;/strong&gt; $S$: A representation of the current situation of the agent.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Action&lt;/strong&gt; $A$: The set of all possible moves the agent can make.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reward&lt;/strong&gt; $R$: A feedback signal from the environment to evaluate the agent&amp;rsquo;s action.
&lt;ul&gt;
&lt;li&gt;A factor $\gamma$ between 0 and 1 that represents the difference in importance between future rewards and immediate rewards is
usually used to prioritize short-term rewards over long-term rewards&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Policy $\pi$&lt;/strong&gt;: A strategy used by the agent to determine the next action based on the current state. It&amp;rsquo;s usually
a probability function $\pi: S\times A$ -&amp;gt; $[0, 1]$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Value Function $V_{\pi}(s)$&lt;/strong&gt;: A function that estimates the expected cumulative reward from a given state with the policy $\pi$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Q-Function $Q_{\pi}(s, a)$&lt;/strong&gt;: A function that estimates the expected cumulative reward from a given state-action pair with the policy $\pi$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As it can be seen in the above figure, the general workflow of RL involves&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Observe the state $s_t$ (and reward $r_t$), the state can be any representation of the current situation or context in which the agent operates.
Note that the state space $S$ can be either &lt;strong&gt;finite&lt;/strong&gt; or &lt;strong&gt;infinite&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Based on the current policy $\pi$, the agent selects an action $a_t \in A$ to perform. The selection can be &lt;strong&gt;deterministic&lt;/strong&gt;
or &lt;strong&gt;stochastic&lt;/strong&gt; depending on the policy.&lt;/li&gt;
&lt;li&gt;The agent performs the selected action $a_t$ in the environment (can also be either &lt;strong&gt;known&lt;/strong&gt; or &lt;strong&gt;unknown&lt;/strong&gt;).&lt;/li&gt;
&lt;li&gt;After performing the action, the agent receives a reward $r_{t+1}$ from the environment and observes the next state $s_{t+1}$.
The mechanism of moving from one state to another given a specific action is modeled by a probability function, denoted as $P(s_{t+1} \mid s_t, a_t)$.&lt;/li&gt;
&lt;li&gt;The agent updates its &lt;strong&gt;policy&lt;/strong&gt; $\pi(a_t \mid s_t)$ and &lt;strong&gt;value functions&lt;/strong&gt; $V(s_t)$ or $Q(s_t, a_t)$ based on the observed reward $r_t$
and next state $s_{t+1}$. The update rule varies depending on the RL algorithm used. (Note that the policy learned in step 5
can be either the &lt;strong&gt;same (on policy)&lt;/strong&gt; or &lt;strong&gt;different (off policy)&lt;/strong&gt; with the ones in step 2)&lt;/li&gt;
&lt;li&gt;The agent repeats again from step 1 and continues this iterative process until the policy converges, meaning it has
learned an optimal or near-optimal policy that maximizes cumulative rewards over time.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Reinforcement Learning (RL) derives its name from the concept of &amp;ldquo;reinforcement&amp;rdquo; in behavioral psychology, where learning
occurs through rewards and punishments. The agent learns to make decisions by receiving feedback in the form of rewards or
penalties. Positive outcomes reinforce the actions that led to them, strengthening the behavior. The learning process is kind
of a process of &lt;strong&gt;&amp;ldquo;Trial and Error&amp;rdquo;&lt;/strong&gt;, where the agent explores different actions to discover which ones yield the highest rewards.
Long-term beneficial actions are reinforced through repeated positive outcomes.&lt;/p&gt;
&lt;p&gt;Now let&amp;rsquo;s start with the most influential and fundamental RL model - Markov Decision Process (MDP). Our fantastic journey begins here.
Note that the below algorithms and equations are cited from &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;, &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; and &lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h3 id=&#34;markov-decision-process-mdp&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#markov-decision-process-mdp&#34;&gt;
        
    &lt;/a&gt;
    Markov Decision Process (MDP)
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s be more specific about the above settings to realize the MDP. Assume the state space $S$ and action space $A$ are finite,
the process is equipped with the markov property, that is, $P(s_{t+1} \mid h_t) = P(s_{t+1} \mid s_t, a_t)$, where $h_t = \{ s_1, a_1, &amp;hellip; , s_t, a_t \}$
denotes the history of states and actions.&lt;/p&gt;
&lt;p&gt;Suppose the agent is interacting with the environment for a total $T$ steps (horizon). Let&amp;rsquo;s define the total reward after time $t$ as
$$
G_t = r_{t+1} + \gamma r_{t+2} + &amp;hellip; + \gamma^{T-t-1} r_T \tag{1.1}
$$
The value function is defined as
$$
V_{\pi}(s) = \mathbb{E}_{\pi}[G_t \mid s_t=s] \tag{1.2}
$$&lt;/p&gt;
&lt;p&gt;The value action function $Q$ is defined as&lt;/p&gt;
&lt;p&gt;$$
Q_{\pi}(s, a) = \mathbb{E}_{\pi}[G_t \mid s_t=s, a_t=a] \tag{1.3}
$$&lt;/p&gt;
&lt;p&gt;From the above definition, it&amp;rsquo;s easy to derive the connection between $V$ and $Q$. In particular, when marginalizing the action,
we are able to convert $Q$ function to $V$ value function.&lt;/p&gt;
&lt;p&gt;$$
V_{\pi}(s) = \sum_{a \in A} \pi(a \mid s)Q_{\pi}(s, a) \tag{1.4}
$$&lt;/p&gt;
&lt;p&gt;when converting $V$ to $Q$, we can see that&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
\begin{aligned}
Q_{\pi}(s, a) &amp;= \mathbb{E}_{\pi}[G_t \mid s_t=s, a_t=a] \\
&amp;= \mathbb{E}_{\pi}[r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3}+ ... \mid s_t=s, a_t=a] \\
&amp;= \mathbb{E}_{\pi}[r_{t+1} \mid s_t=s, a_t=a] + \gamma \mathbb{E}_{\pi}[r_{t+2} + \gamma r_{t+3} + ... \mid s_t=s, a_t=a] \\
&amp;= R(s, a) + \gamma \mathbb{E}_{\pi}[G_{t+1} \mid s_t=s, a_t=a] \\
&amp;= R(s, a) + \gamma \mathbb{E}_{\pi}[\mathbb{E}_{\pi}[G_{t+1} \mid s_{t+1}] \mid s_t=s, a_t=a] \\
&amp;= R(s, a) + \gamma \mathbb{E}_{\pi}[V_{\pi}(s_{t+1}) \mid s_t=s, a_t=a] \\
&amp;= R(s, a) + \gamma \sum_{s&#39; \in S} p(s&#39; \mid s, a) V_{\pi}(s&#39;) \hspace{15.5em} \text{(1.5)}
\end{aligned}
&lt;/p&gt;
&lt;h4 id=&#34;bellman-expectation-equation&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#bellman-expectation-equation&#34;&gt;
        
    &lt;/a&gt;
    Bellman Expectation Equation
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;From (1.2), we can express the value function in an recursive way.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
\begin{aligned}
V_{\pi}(s) &amp;= \mathbb{E}_{\pi}[G_t \mid s_t=s] \\
&amp;= \mathbb{E}_{\pi}[r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + ... \mid s_t=s] \\
&amp;= \mathbb{E}_{\pi}[r_{t+1} \mid s_t=s] + \gamma\mathbb{E}_{\pi}[r_{t+2} + \gamma r_{t+3} + ... \mid s_t=s] \\
&amp;= \mathbb{E}_{\pi}[r_{t+1} \mid s_t=s] + \gamma\mathbb{E}_{\pi}[G_{t+1} \mid s_t=s] \\
&amp;= \mathbb{E}_{\pi}[r_{t+1} \mid s_t=s] + \gamma\mathbb{E}_{\pi}[\mathbb{E}_{\pi}[G_{t+1} \mid s_{t+1}] \mid s_t=s] \\
&amp;= \mathbb{E}_{\pi}[r_{t+1} \mid s_t=s] + \gamma\mathbb{E}_{\pi}[V_{\pi}(s_{t+1}) \mid s_t=s] \\
&amp;= \mathbb{E}_{\pi}[r_{t+1} + \gamma V_{\pi}(s_{t+1}) \mid s_t=s] \hspace{19em} \text{(1.6)}
\end{aligned}
&lt;/p&gt;
Similarly, the state action function can also be written recursively
$$
Q_{\pi}(s, a) = \mathbb{E}_{\pi}[r_{t+1} + \gamma Q_{\pi}(s_{t+1}, a_{t+1}) \mid s_t=s, a_t=a] \tag{1.7}
$$
Furthermore, equation (1.5) also expressed the current-future connection between $V$ and $Q$. So if we plug equation 
(1.5) in (1.4), then we would get 
$$
V_{\pi}(s) = \sum_{a \in A} \pi(a \mid s)(R(s, a) + \gamma \sum_{s&#39; \in S} p(s&#39; \mid s, a) V_{\pi}(s&#39;)) \tag{1.8}
$$
which denotes the connection of value function at current state and future state.
On the other hand, if (1.4) is plugged in (1.5), we can see
$$
Q_{\pi}(s, a) = R(s, a) + \gamma \sum_{s&#39; \in S} p(s&#39; \mid s, a) \sum_{a&#39; \in A} \pi(a&#39; \mid s&#39;)Q_{\pi}(s&#39;, a&#39;) \tag{1.9}
$$
which builds the connection of the action value function between the current and future state action pairs. By comparing
(1.6) and (1.8), (1.7) and (1.9), it&#39;s easy to observe that (1.8) and (1.9) actually implements the expectation expression explicitly. 
&lt;p&gt;A visualized interpretation of (1.8) and (1.9) are shown in the following backup diagram.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
    &lt;img src=&#34;http://localhost:1313/rf/backup.png&#34;&gt;&lt;br&gt;
    &lt;em&gt;Figure 1: Backup Diagram&lt;/em&gt;
&lt;/p&gt;
&lt;h4 id=&#34;bellman-optimal-equation&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#bellman-optimal-equation&#34;&gt;
        
    &lt;/a&gt;
    Bellman Optimal Equation
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;The goal of reinforcement learning is to find the optimal policy $\pi^*$ such that the value function is maximized.&lt;/p&gt;
&lt;p&gt;$$
\pi^*(s) = \arg \max_{\pi} V_{\pi}(s)
$$&lt;/p&gt;
&lt;p&gt;when this is achieved, the optimal value function is $V^*(s) = max_{\pi}V_{\pi}(s), \forall s \in S$. At this time,
the optimal value function can also be achieved by selecting the best action under the optimal policy&lt;/p&gt;
&lt;p&gt;$$
V^* (s) = \max_{a} Q^* (s, a) \tag{1.10}
$$&lt;/p&gt;
&lt;p&gt;where $Q^* (s, a) = \arg \max_{\pi} Q_{\pi}(s, a)$, $\forall s \in S, a \in A$.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s apply the optimal policy in (1.5) and we have&lt;/p&gt;
&lt;p&gt;$$
Q^* (s, a) = R(s, a) + \gamma \sum_{s&amp;rsquo; \in S} p(s&amp;rsquo; \mid s, a) V^*(s&amp;rsquo;) \tag{1.11}
$$&lt;/p&gt;
&lt;p&gt;When plugging (1.10) in (1.11), we get &lt;strong&gt;Bellman optimal equation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
Q^* (s, a) = R(s, a) + \gamma \sum_{s&amp;rsquo; \in S} p(s&amp;rsquo; \mid s, a) \max_{a&amp;rsquo;} Q^* (s&amp;rsquo;, a&amp;rsquo;) \tag{1.12}
$$&lt;/p&gt;
&lt;p&gt;Similarly, plugging (1.11) in (1.10), the Bellman optimal value equation is&lt;/p&gt;
&lt;p&gt;$$
V^* (s) = \max_{a} \left( R(s, a) + \gamma \sum_{s&amp;rsquo; \in S} p(s&amp;rsquo; \mid s, a) V^* (s&amp;rsquo;) \right) \tag{1.13}
$$&lt;/p&gt;
&lt;p&gt;So far, we have introduced the main math modeling framework in RL. How shall we fit the real-life cases into this framework
and get the best policy? What trade-offs have to be made in each scenario? Let&amp;rsquo;s take a deep dive.&lt;/p&gt;
&lt;h2 id=&#34;model-based---when-the-environment-is-given&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#model-based---when-the-environment-is-given&#34;&gt;
        
    &lt;/a&gt;
    Model Based - when the environment is given
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;Our journey begins with a typical MDP where the transition mechanism and the reward function are fully known. As seen in
the above Bellman expectation and optimal equations, the problem can be resolved by tackling a similar subproblem recursively.
This is usually known as dynamic programming.&lt;/p&gt;
&lt;h3 id=&#34;dynamic-programming&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#dynamic-programming&#34;&gt;
        
    &lt;/a&gt;
    Dynamic Programming
&lt;/div&gt;
&lt;/h3&gt;
&lt;h4 id=&#34;policy-iteration&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#policy-iteration&#34;&gt;
        
    &lt;/a&gt;
    policy iteration
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;The key idea is to iteratively process two alternative steps: policy evaluation and policy improvement. Policy evaluation
computes the value function $V^\pi$ for the current policy $\pi$. While policy improvement updates the policy $\pi$ using
the greedy approach.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;initialize the policy $\pi(s)$ and value function $V(s)$&lt;/li&gt;
&lt;li&gt;while not stop
&lt;ul&gt;
&lt;li&gt;while $\delta &amp;gt; \theta$, do
&lt;ul&gt;
&lt;li&gt;$\delta \leftarrow 0$&lt;/li&gt;
&lt;li&gt;for each $s \in S$
&lt;ul&gt;
&lt;li&gt;$v \leftarrow V(s)$&lt;/li&gt;
&lt;li&gt;$V(s) \leftarrow \sum_{a \in A} \pi(a \mid s)(R(s, a) + \gamma \sum_{s&amp;rsquo; \in S} p(s&amp;rsquo; \mid s, a) V_{\pi}(s&amp;rsquo;)) $ &lt;strong&gt;(policy evaluation)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;$\delta \leftarrow \max(\delta, |v - V(s)|)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;end while&lt;/li&gt;
&lt;li&gt;$\pi_{old} \leftarrow \pi$&lt;/li&gt;
&lt;li&gt;for each $s \in S$
&lt;ul&gt;
&lt;li&gt;$\pi(s) \leftarrow \arg \max_{a} R(s, a) + \gamma \sum_{s&amp;rsquo;} p(s&amp;rsquo; \mid s, a)V(s&amp;rsquo;)$ &lt;strong&gt;(policy improvement)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;if $\pi_{old} = \pi$
&lt;ul&gt;
&lt;li&gt;stop and return $\pi$ and $V$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;value-iteration&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#value-iteration&#34;&gt;
        
    &lt;/a&gt;
    value iteration
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;It usually takes quite a significant amount of time to run policy evaluation, especially when the state and action space are
large enough. Is there a way to avoid too many policy evaluation process? The answer is value iteration. It&amp;rsquo;s an iterative process
to update the Bellman optimal equation (1.13).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;initialize the value function $V(s)$&lt;/li&gt;
&lt;li&gt;while $\delta &amp;gt; \theta$, do
&lt;ul&gt;
&lt;li&gt;$\delta \leftarrow 0$&lt;/li&gt;
&lt;li&gt;for each $s \in S$
&lt;ul&gt;
&lt;li&gt;$v \leftarrow V(s)$&lt;/li&gt;
&lt;li&gt;$V(s) \leftarrow \max_{a \in A} (R(s, a) + \gamma \sum_{s&amp;rsquo; \in S} p(s&amp;rsquo; \mid s, a) V_{\pi}(s&amp;rsquo;)) $&lt;/li&gt;
&lt;li&gt;$\delta \leftarrow \max(\delta, |v - V(s)|)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;end while&lt;/li&gt;
&lt;li&gt;$\pi(s) \leftarrow \arg \max_{a} R(s, a) + \gamma \sum_{s&amp;rsquo;} p(s&amp;rsquo; \mid s, a)V(s&amp;rsquo;)$&lt;/li&gt;
&lt;li&gt;return $V$ and $\pi$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It can be seen that value iteration doesn&amp;rsquo;t own policy updates, it generates the optimal policy when the value function converges.&lt;/p&gt;
&lt;h2 id=&#34;model-free---when-the-environment-is-unknown&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#model-free---when-the-environment-is-unknown&#34;&gt;
        
    &lt;/a&gt;
    Model Free - when the environment is unknown
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;In practical, the environment is hardly fully known or it&amp;rsquo;s simply a blackbox most of the time. Thus dynamic programming (policy
iteration &amp;amp; value iteration) might not helpful. In this section, we will introduce solutions originated from various kinds ideas.&lt;/p&gt;
&lt;h3 id=&#34;value-based&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#value-based&#34;&gt;
        
    &lt;/a&gt;
    Value Based
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;This collection of algorithms aims optimizing the the value function $V$ or $Q$, which can then be used to derive the optimal
policy. The policy is typically derived indirectly by selecting actions that maximize the estimated value. Value based methods
are usually simple to implement and understand. They are effective in environments with discrete and finite action spaces.
Deriving optimal policies is clear and straightforward. However, they are usually struggling with high-dimensional or continuous
action spaces. Trying function approximation (e.g., neural networks) may not work well due to unstable and divergence. Besides,
value based methods usually require extensive exploration to accurately estimate value functions.&lt;/p&gt;
&lt;h4 id=&#34;model-free-policy-evaluation-monte-carlo--temporal-difference-td&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#model-free-policy-evaluation-monte-carlo--temporal-difference-td&#34;&gt;
        
    &lt;/a&gt;
    Model Free Policy Evaluation: Monte Carlo &amp;amp; Temporal Difference (TD)
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;Value based approaches inherits the idea from dynamic programming. The difference now is that the environment is unknown such that
both policy iteration and value iteration are not feasible (transition probably unavailable now). Let&amp;rsquo;s start with a simple question:
&lt;strong&gt;how can we estimate the value function given a policy when the environment is unknown&lt;/strong&gt;. The idea is to &lt;strong&gt;interact with the environment&lt;/strong&gt;
and update the value function/policy based on the returned rewards. Depending on how heavily we rely on interacting with the
environment, we have the Monte Carlo and Temporal difference method.&lt;/p&gt;
&lt;h5 id=&#34;monte-carlo&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#monte-carlo&#34;&gt;
        
    &lt;/a&gt;
    Monte Carlo
&lt;/div&gt;
&lt;/h5&gt;
&lt;p&gt;Monte Carlo methods rely on averaging returns of sampled episodes to estimate the expected value of states or state-action pairs.
Unlike temporal difference (TD) methods, Monte Carlo methods do not bootstrap and instead use complete episodes to update value estimates.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initialize $V_{\pi}(s)$ arbitrarily for all states $s \in S$.&lt;/li&gt;
&lt;li&gt;Initialize the total reward $S(s)$ and total visits $N(s)$&lt;/li&gt;
&lt;li&gt;for episode from 1 to $N$, do:
&lt;ul&gt;
&lt;li&gt;Generate an episode trajectory $(s_0, a_0, r_1, s_1, a_1, r_2, \ldots, s_T)$ following policy $\pi$.&lt;/li&gt;
&lt;li&gt;For each state $s$ that first appearing in the episode trajectory:
&lt;ul&gt;
&lt;li&gt;Compute the return $G_t$  from state $s$: $G_t = r_{t+1} + \gamma r_{t+2} + \dots + \gamma^{T-t-1} r_T$&lt;/li&gt;
&lt;li&gt;total reward at $s$ is $S(s) \leftarrow S(s) + G_t$&lt;/li&gt;
&lt;li&gt;total count visiting $s$ is $N(s) \leftarrow N(s) + 1$&lt;/li&gt;
&lt;li&gt;Update the value estimate $V_{\pi}(s)$ as the average of all observed returns for state $s$: $V_{\pi}(s) \leftarrow \frac{S(s)} {N(s)}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;end for&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;end for&lt;/li&gt;
&lt;li&gt;return the value function under policy $\pi$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It&amp;rsquo;s worth noting that the monte carlo update can also be reformated in an incremental way, that is,
$$
V_{\pi}(s) \leftarrow V_{\pi}(s) + \frac{1}{N(s)} (G_t - V_{\pi}(s)) \tag{3.1}
$$
It can be viewed to update the value function with an error term
$G_t - V_{\pi}(s)$. As $V_{\pi}(s) \rightarrow G_t$, then the error term does not bring too much value and as a result $V(s)$ converges to
$G_t$. So $G_t$ can somehow be interpreted as the target of the update. In Monte Carlo, the updates does not start until the
entire episode completes. This may not feasible in some cases where the interactive game never ends or more frequent updates
are expected. To tackle this, we are happy to introduce temporal difference.&lt;/p&gt;
&lt;h5 id=&#34;temporal-difference&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#temporal-difference&#34;&gt;
        
    &lt;/a&gt;
    Temporal Difference
&lt;/div&gt;
&lt;/h5&gt;
&lt;p&gt;From (1.2), we can see that the Monte Carlo is approximating the target $G_t$ using (3.1). If we only interact with the environment
one step instead of completing the full episode trajectory. It&amp;rsquo;s equivalent to reformat the (1.2) as (1.6), where the target is
thus $r_{t} + \gamma V_{\pi}(s_{t+1})$. So the formula (3.1) can be written as
$$
V_{\pi}(s) \leftarrow V_{\pi}(s) + \alpha (r_{t+1} + \gamma V_{\pi}(s_{t+1}) - V_{\pi}(s)) \tag{3.2}
$$
where the $r_{t+1} + \gamma V_{\pi}(s_{t+1}) - V_{\pi}(s)$ is called temporal difference error. Since only one step reward is retrieved
from the system interaction, it&amp;rsquo;s also noted as 1-step temporal difference, i.e. TD(1). What if we interact a few more steps with the environment?
The target $G_t$ would include more future steps rewards. A general representation of TD(k) is shown below.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
\begin{aligned}
&amp;TD(1) \hspace{1em} \rightarrow \hspace{1em} G^{1}_t = r_{t+1} + \gamma V(s_{t+1}) \\
&amp;TD(2) \hspace{1em} \rightarrow \hspace{1em} G^{2}_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 V(s_{t+2}) \\
&amp;TD(k) \hspace{1em} \rightarrow \hspace{1em} G^{k}_t = r_{t+1} + \gamma r_{t+2} + \dots + \gamma^{k-1} r_{t+k} + \gamma^k V(s_{t+k}) \\
&amp;TD(\infty) / MC \hspace{1em} \rightarrow \hspace{1em} G^{\infty}_t = r_{t+1} + \gamma r_{t+2}+ \dots + \gamma^{T-t-1} r_{T})
\end{aligned}
&lt;/p&gt;
&lt;p&gt;Compared with Monte Carlo, it&amp;rsquo;s possible to updating the value function in an online fashion, meaning that update happens after
every step of interaction. It&amp;rsquo;s more efficient than updating after completing an episode. This also indicates that TD learning can be
applied to any piece of episode, which is more flexible. The estimation variance is lower but bias can be higher due to bootstrapping
(updates based on estimated value of next state)&lt;/p&gt;
&lt;p&gt;Below compares the computation graph among MC, TD and DP. In MC, only the states on the episode trajectory are updated. On the
other hand, DP computes all the related next states when updating. TD compromise both MC and DP, it sampled only one or finite steps
for updating.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;http://localhost:1313/rf/rf_dp_mc_td.png&#34; width=&#34;900&#34; height=&#34;600&#34;&gt;&lt;br&gt;
&lt;em&gt;Figure 2: Visual Interpretation of DP, TD and MC&lt;/em&gt;
&lt;p&gt;
&lt;p&gt;&lt;em&gt;Image cited from &lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h4 id=&#34;sarsa&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#sarsa&#34;&gt;
        
    &lt;/a&gt;
    SARSA
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;Now given a policy $\pi$, we have learned TD and MC can help evaluate the value function when the environment is unknown.
The next natural question is How can we find the optimal policy? We can borrow the policy improvement idea from policy
iteration in DP. Get the next policy by using the greedy search, i.e. $\pi_{i+1}(s) = \arg \max_{a} Q_{\pi_i}(s, a)$.
Combining the generalized policy evaluation and the greedy policy improvement would allow us to build the generalized policy
iteration.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;http://localhost:1313/rf/policy_iteration.png&#34; width=&#34;250&#34; height=&#34;150&#34;&gt;&lt;br&gt;
&lt;em&gt;Figure 3: Generalized Policy Iteration&lt;/em&gt;
&lt;p&gt;
&lt;p&gt;Another difference from the policy iteration in DP is that we may never update some state-action pairs using the pure greedy
policy improvement strategy. This is because we are now interacting with the environment instead of updating all the states action
explicitly with known transition probabilities. So we may need to allow some extent of exploration to mitigate such issue. One
solution is to apply the $\epsilon$-greedy strategy.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
\begin{aligned}
\pi(a \mid s) &amp;= 
\begin{cases} 
      1 - \epsilon &amp; \text{if } a = \arg \max_a Q(s, a) \\
      \epsilon &amp;  \text{other actions in A}
\end{cases}
\end{aligned}
&lt;/p&gt;
&lt;p&gt;Thus the complete algorithm (TD policy evaluation on $Q$ + $\epsilon$-greedy policy improvement) is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initialize the $Q(s, a)$&lt;/li&gt;
&lt;li&gt;for iteration from 1 to $N$, do:
&lt;ul&gt;
&lt;li&gt;get initial state $s$&lt;/li&gt;
&lt;li&gt;apply the $\epsilon$-greedy strategy to choose the action $a$ based on $s$&lt;/li&gt;
&lt;li&gt;for $t$ from 1 to $T$, do:
&lt;ul&gt;
&lt;li&gt;interact with the environment and get reward $r$ and $s&#39;$&lt;/li&gt;
&lt;li&gt;apply the $\epsilon$-greedy strategy to choose the next action $a&amp;rsquo;$ based on $s&#39;$&lt;/li&gt;
&lt;li&gt;$Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma Q(s&amp;rsquo;, a&amp;rsquo;) - Q(s, a)]$ &lt;strong&gt;(TD policy evaluation)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;$s \leftarrow s&amp;rsquo;, a \leftarrow a&amp;rsquo;$ &lt;strong&gt;($\epsilon$-greedy policy improvement)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;end for&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;end for&lt;/li&gt;
&lt;li&gt;return the value action $Q$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It&amp;rsquo;s worth noting that once the state, action, reward, next state, next action $(s, a, r, s&amp;rsquo;, a&amp;rsquo;)$ is generated, the update is
conducted once and then repeats the iterations. Therefore, is called &lt;strong&gt;SARSA&lt;/strong&gt;. In a more general case, if we interact more steps
and update the $Q$ value using $Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha [r_t + \gamma r_{t+1} + \dots + \gamma^n Q(s_{t+n}, a_{t+n}) - Q(s_t, a_t)]$,
then it&amp;rsquo;s &lt;strong&gt;$n$ step SARSA&lt;/strong&gt;.&lt;/p&gt;
&lt;h4 id=&#34;q-learning&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#q-learning&#34;&gt;
        
    &lt;/a&gt;
    Q-Learning
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;If the way of updating the TD policy evaluation in SARSA is changed to
$$
Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a&amp;rsquo;} Q(s&amp;rsquo;, a&amp;rsquo;) - Q(s, a)] \tag{3.3}
$$
Then we would get the Q learning algorithm.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initialize the $Q(s, a)$&lt;/li&gt;
&lt;li&gt;for iteration from 1 to $N$, do:
&lt;ul&gt;
&lt;li&gt;get initial state $s$&lt;/li&gt;
&lt;li&gt;for $t$ from 1 to $T$, do:
&lt;ul&gt;
&lt;li&gt;apply the $\epsilon$-greedy strategy to choose the action $a$ based on $s$&lt;/li&gt;
&lt;li&gt;interact with the environment and get reward $r$ and $s&#39;$&lt;/li&gt;
&lt;li&gt;$Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a&amp;rsquo;} Q(s&amp;rsquo;, a&amp;rsquo;) - Q(s, a)]$&lt;/li&gt;
&lt;li&gt;$s \leftarrow s&#39;$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;end for&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;end for&lt;/li&gt;
&lt;li&gt;return the value action $Q$ and derive the optimal policy&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The major difference between Q learning and SARSA is about the way to update the $Q$ value. In (3.3), we are actually
approximating the target value $r + \gamma \max_{a&amp;rsquo;} Q(s&amp;rsquo;, a&amp;rsquo;)$. So the goal here is to learn
the optimal policy $\pi(a \mid s) = \max_{a&amp;rsquo;} Q(s, a&amp;rsquo;)$. Let&amp;rsquo;s call the policy used in (3.3) as the target policy.
The data used to learn the target policy $(s, a, r, s&amp;rsquo;)$ is however generated from another policy that interacts with the
environment, called the behavior policy ($\epsilon$-greedy). It&amp;rsquo;s easy to see that the target policy is different from the
behavior policy. This is also called &lt;strong&gt;off-policy&lt;/strong&gt; learning.&lt;/p&gt;
&lt;p&gt;On the other hand, the behavior policy fully aligns with the target policy in SARSA, which means $a&amp;rsquo;$ and $a$ are generated
from the same policy. This kind of learning is called &lt;strong&gt;on-policy&lt;/strong&gt; learning.&lt;/p&gt;
&lt;p&gt;One interesting common area among DP, SARSA, Q-learning is that they all assume finite or discrete state and action space.
Optimizing such problem can be seen as updating a table with an entry for each possible state or state-action pair. Thus,
these algorithms are also called &lt;strong&gt;tabular methods&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;To be continued &amp;hellip;&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;summary&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#summary&#34;&gt;
        
    &lt;/a&gt;
    Summary
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;So far, we have introduced the basic RL setting and basic tabular methods that are suitable to simple cases. However, most
real problems involve large quantity or infinite state or action space. More powerful tools are thus needed. In next blog,
we will start the journey to deep reinforcement learning, an area that utilize the fancy deep learning techniques to tackle
more complicated problems.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#citation&#34;&gt;
        
    &lt;/a&gt;
    Citation
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;If you find this post helpful, please consider citing it as:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bibtex&#34; data-lang=&#34;bibtex&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f3f99d&#34;&gt;@article&lt;/span&gt;{&lt;span style=&#34;color:#ff5c57&#34;&gt;wang2024rflearningtabular&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;author&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;Bing Wang&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;title&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;A Journey to Reinforcement Learning - Tabular Methods&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;journal&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;bingzw.github.io&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;year&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;2024&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;month&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;July&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;url&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;https://bingzw.github.io/posts/2024-07-05-rf-learning-tabular/&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;or&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Bing Wang. (2024, June). A Journey to Reinforcement Learning - Tabular Methods. 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;https://bingzw.github.io/posts/2024-07-05-rf-learning-tabular/
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;reference&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#reference&#34;&gt;
        
    &lt;/a&gt;
    Reference
&lt;/div&gt;
&lt;/h2&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.mdpi.com/2076-3417/13/4/2443&#34;&gt;Souchleris, Konstantinos, George K. Sidiropoulos, and George A. Papakostas. &amp;ldquo;Reinforcement learning in game industry—review, prospects and challenges.&amp;rdquo; Applied Sciences 13.4 (2023): 2443&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://datawhalechina.github.io/easy-rl/&#34;&gt;Easy RL&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://hrl.boyuai.com/&#34;&gt;Hands On RL&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf&#34;&gt;Reinforcement Learning: An Introduction&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://roboticseabass.com/2020/08/02/an-intuitive-guide-to-reinforcement-learning/&#34;&gt;An intuitive guide to reinforcement learning&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Recommendation Models</title>
      <link>/posts/2024-06-06-recommendation-model/</link>
      <pubDate>Thu, 06 Jun 2024 22:04:53 -0700</pubDate>
      <author>bingwang8878@gamil.com (Bing Wang)</author>
      <guid>/posts/2024-06-06-recommendation-model/</guid>
      <description>&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;http://localhost:1313/recommendation_model/reco.jpeg&#34; width=&#34;600&#34; height=&#34;400&#34;&gt;&lt;br&gt;
&lt;p&gt;
&lt;p&gt;&lt;em&gt;Image cited from &lt;a href=&#34;https://www.vecteezy.com/&#34;&gt;vecteezy&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction-to-recommendation-models&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#introduction-to-recommendation-models&#34;&gt;
        
    &lt;/a&gt;
    Introduction to Recommendation Models
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;In the digital age, the sheer volume of available content and products can be overwhelming for users. Recommendation
systems play a crucial role in helping users discover relevant items by filtering through vast amounts of data. These
systems are integral to various industries, including e-commerce, streaming services, social media, and online
advertising.&lt;/p&gt;
&lt;p&gt;Recommendation models are algorithms designed to suggest items to users based on various factors such as past behavior,
preferences, and item characteristics. The goal is to enhance user experience by providing personalized recommendations,
thereby increasing engagement and satisfaction.&lt;/p&gt;
&lt;p&gt;In most cases, the goal is trying to find the most appealing item for the user given all user demographic, behavior and
interest information. A well-designed recommendation system usually results in better personalization, user engagement,
and revenue generation. Let&amp;rsquo;s take a look at the most popular modeling approaches.&lt;/p&gt;
&lt;h2 id=&#34;collaborative-filtering-cf&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#collaborative-filtering-cf&#34;&gt;
        
    &lt;/a&gt;
    Collaborative Filtering (CF)
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;Collaborative Filtering is one of the most famous recommendation models so far. It&amp;rsquo;s about to guess the user&amp;rsquo;s interest
based on the behaviors and preferences of other users with similar tastes/characteristics. This basic approach is
straightforward and only requires user-item interaction history.&lt;/p&gt;
&lt;h3 id=&#34;memory-based-cf&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#memory-based-cf&#34;&gt;
        
    &lt;/a&gt;
    Memory Based CF
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;Suppose the recommendation is purely derived from historical records and it does not involve any predictive modelings.
Let&amp;rsquo;s assume a table of all ratings of users on items. Let $r_{u,i}$ denotes the missing rating of user $u$ on item $i$.
$S_u$ be a set of users that share similar characteristics with user $u$. $P_j$ denotes a set of items that are close to
item $j$. The goal is to guess the missing rating $r_{u,i}$. Let&amp;rsquo;s start expressing the &amp;ldquo;predicted&amp;rdquo; rating from either user-user
filtering or item-item filtering.&lt;/p&gt;
&lt;h4 id=&#34;user-user-filtering&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#user-user-filtering&#34;&gt;
        
    &lt;/a&gt;
    User User Filtering
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;Assume that users with a similar profile share a similar taste. We are predicting the missing rating as the weighted
average from ratings of similar users:&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
$r_{u,i} = \bar{r}_u + \frac{\sum_{u&#39;\in S_u}sim(u, u&#39;) * (r_{u&#39;,i} - \bar{r}_{u&#39;})}{\sum_{u&#39;\in S_u}sim(u, u&#39;)}$
&lt;p&gt;
&lt;p&gt;where $\bar{r}_u$ is the average rating of user $u$ and $sim(u, u&amp;rsquo;)$ represents the similarity score between user $u$
and $u&amp;rsquo;$. A common choice is to calculate the cosine similarity between two user rating vectors.&lt;/p&gt;
&lt;h4 id=&#34;item-item-filtering&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#item-item-filtering&#34;&gt;
        
    &lt;/a&gt;
    Item Item Filtering
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;Assume that the customers will prefer products that share a high similarity with those already well appreciated. The
missing rating is thus predicted as the weighted average of a set of similar products:&lt;/p&gt;
&lt;p&gt;$$r_{u,i} = \frac{\sum_{j \in P_i}sim(j, i) * r_{u,j}}{\sum_{j \in P_i}sim(j, i)}$$&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s straightforward to observe that user-based filtering is to check what other users think of the same product, while
item-based filtering is aggregating what the user thinks of other items. Essentially, both are weighted linear
combination of observed ratings. Can we do better than weighted average?&lt;/p&gt;
&lt;h3 id=&#34;model-based-cf&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#model-based-cf&#34;&gt;
        
    &lt;/a&gt;
    Model Based CF
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;Unlike the memory based CF which is trying to fill in the missing cells in the rating matrix, model-based collaborative
filtering is to predict user preferences for items based on past interactions. It often relies on the concept of
latent factors. These are hidden features that influence user preferences and item characteristics. By uncovering these
latent factors, the model can predict the likelihood of a user liking an item.&lt;/p&gt;
&lt;p&gt;Let $R$ denotes the user item interaction rating matrix, $R \in \mathbb{R}^{m*n}$, where $m$ is the dimension of user
space and $n$ denotes the dimension of item space.&lt;/p&gt;
&lt;h4 id=&#34;clustering&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#clustering&#34;&gt;
        
    &lt;/a&gt;
    Clustering
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;Given user or item embeddings, a simple approach is to apply the K nearest neighbours to find the K closest users or
items depending on the similarity metrics used.&lt;/p&gt;
&lt;h4 id=&#34;matrix-factorization&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#matrix-factorization&#34;&gt;
        
    &lt;/a&gt;
    Matrix Factorization
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;The basic idea is to decompose the user-item interaction matrix into two lower-dimensional matrices:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;User Matrix (U)&lt;/strong&gt;: Represents users in terms of latent factors, $U \in \mathbb{R}^{m * p}$, where $p$ is the dimension
of the latent space.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Item Matrix (V)&lt;/strong&gt;: Represents items in terms of latent factors, $V \in \mathbb{R}^{n * p}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The interaction matrix $R$ is approximated as the product of these two matrices: $R \approx U \cdot V^T$. In practice, we
are usually optimizing for a weighted matrix factorization objective&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;$$L(\theta) = \sum_{(i, j)\in obs} \omega_{i, j} (R_{i, j} - U_i \cdot V_{j}^{T})^2 + \omega_{0} \sum_{(i, j) \notin obs}
(U_i \cdot V_{j}^{T})^2$$&lt;/p&gt;
&lt;p&gt;where $\omega_0$ is a hyper-parameter that weights the two terms so that the objective is not
dominated by one or the other, and $\omega_{i, j}$ is a function of the frequency of user $i$ and item $j$.&lt;/p&gt;
&lt;p&gt;Common optimization algorithms to minimize the above objective function includes &lt;strong&gt;stochastic gradient descent&lt;/strong&gt; and
&lt;strong&gt;weighted alternating least squares (WALS)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Recently, a few efforts on deep learning have also been proposed on matrix factorization. For example,&lt;/p&gt;
&lt;h5 id=&#34;deep-auto-encoders2&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#deep-auto-encoders2&#34;&gt;
        
    &lt;/a&gt;
    Deep Auto-Encoders&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h5&gt;
&lt;p&gt;It proposed a model called Collaborative Denoising Auto-Encoder (CDAE) that extends the traditional denoising autoencoder
architecture to the collaborative filtering domain. CDAE is designed to handle implicit feedback, where user preferences
are inferred from user behavior rather than explicit ratings. The model incorporates both user-specific and item-specific
factors, leveraging the rich user interaction data to learn better representations for recommendation tasks.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
    &lt;img src=&#34;http://localhost:1313/recommendation_model/autoencoder_mf.png&#34;&gt;&lt;br&gt;
    &lt;em&gt;Figure 1: deep auto-encoder&lt;/em&gt;
&lt;/p&gt;
&lt;h5 id=&#34;neural-collaborative-filtering3&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#neural-collaborative-filtering3&#34;&gt;
        
    &lt;/a&gt;
    Neural Collaborative Filtering&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h5&gt;
&lt;p&gt;NCF replaces these linear latent factor models with non-linear neural networks, allowing for a more expressive
representation of user-item interactions. By doing so, NCF can capture more complex patterns in the data that traditional
methods might miss. The core idea is to use multi-layer perceptrons (MLPs) to model the interaction function between
users and items, providing a more flexible and powerful framework for learning user preferences.&lt;/p&gt;
&lt;p&gt;The general architecture of NCF includes embedding layers for users and items, followed by one or more hidden layers
that learn the interaction between these embeddings. The final output layer predicts the user&amp;rsquo;s preference for a given
item. This approach not only improves the accuracy of recommendations but also enables the integration of additional
features, such as user demographics or item attributes, into the model. Refer to the DLRM or DeepFM for more details.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
    &lt;img src=&#34;http://localhost:1313/recommendation_model/ncf_mf.png&#34;&gt;&lt;br&gt;
    &lt;em&gt;Figure 2: neural collaborative filtering&lt;/em&gt;
&lt;/p&gt;
&lt;h2 id=&#34;content-based-models-ranking-as-recommendation&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#content-based-models-ranking-as-recommendation&#34;&gt;
        
    &lt;/a&gt;
    Content Based Models (Ranking As Recommendation)
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;Unlike the collaborative filtering that purely relies on user-item interaction data. Content-based recommendation models
focus on the attributes of items to suggest similar items to users based on their past interactions. These models analyze
the content (such as text, keywords, categories, or features) associated with the items and create a profile for each
user based on the features of the items they have shown interest in. Compared with collaborative filtering, it&amp;rsquo;s easier
to handle the cold start issue when item features are known. However, its recommendation may strictly adheres to the
user&amp;rsquo;s profile, potentially limiting diversity. There are quite a few popular model frameworks in this area and we would
focus on the models based on deep neural architecture.&lt;/p&gt;
&lt;h3 id=&#34;wide--deep4&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#wide--deep4&#34;&gt;
        
    &lt;/a&gt;
    Wide &amp;amp; Deep&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;Wide &amp;amp; Deep learning combined wide linear models and deep neural networks to achieve both memorization and generalization.
The model consists of two components:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Wide Component&lt;/strong&gt;: This part is a generalized linear model (GLM) that excels at memorization by capturing feature
interactions using cross-product feature transformations. The wide component is effective at handling sparse features
and explicitly memorizing frequent co-occurrence patterns. It can be represented as:&lt;/p&gt;
&lt;p&gt;$$y_{\text{wide}} = \mathbf{w}^T \mathbf{x} + b $$&lt;/p&gt;
&lt;p&gt;where $\mathbf{x}$ represents the input features, $\mathbf{w}$ represents the learned weights, and $b$ is
the bias term.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Component&lt;/strong&gt;: This part is a feed-forward neural network that excels at generalization by capturing high-level
and non-linear feature interactions. The deep component uses dense embeddings to represent categorical features, and it
learns implicit interactions through multiple hidden layers. The output of the deep component can be represented as:&lt;/p&gt;
&lt;p&gt;$$\mathbf{h}^L = f^L(\mathbf{W}^L \mathbf{h}^{L-1} + \mathbf{b}^L)$$&lt;/p&gt;
&lt;p&gt;where $\mathbf{h}^L$ is the activation of the $L$-th layer, $\mathbf{W}^L$ and $\mathbf{b}^L$ are the weights and biases
of the $L$-th layer, respectively, and $f^L$ is the activation function.
The final output of the deep component, $ y_{\text{deep}} $, is the output of the last layer of the deep neural network. It is given by:&lt;/p&gt;
&lt;p&gt;$$
y_{\text{deep}} = \mathbf{W}^O \mathbf{h}^{L} + \mathbf{b}^O
$$&lt;/p&gt;
&lt;p&gt;where $ \mathbf{W}^O $ and $ \mathbf{b}^O $ are the weights and bias of the output layer, and $ \mathbf{h}^L $ is the activation of the last hidden layer.&lt;/p&gt;
&lt;p&gt;The final prediction is a weighted sum of the outputs from the wide and deep components:&lt;/p&gt;
&lt;p&gt;$$\hat{y} = \sigma(y_{\text{wide}} + y_{\text{deep}})$$&lt;/p&gt;
&lt;p&gt;where $\sigma$ is the sigmoid function used to squash the output to a probability score.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p align=&#34;center&#34;&gt;
   &lt;img src=&#34;http://localhost:1313/recommendation_model/wide_deep.png&#34; width=&#34;800&#34; height=&#34;600&#34;&gt;&lt;br&gt;
   &lt;em&gt;Figure 3: wide &amp; deep model&lt;/em&gt;
&lt;/p&gt;
&lt;h3 id=&#34;deepfm5&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#deepfm5&#34;&gt;
        
    &lt;/a&gt;
    DeepFM&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;DeepFM addresses the challenge of capturing both low-order and high-order feature interactions in recommendation systems.
The model consists of two interconnected components:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;FM Component&lt;/strong&gt;: This part captures low-order feature interactions using Factorization Machines, which are effective
for handling sparse data and modeling pairwise feature interactions without manual feature engineering. The FM component
can be represented as:&lt;/p&gt;
&lt;p&gt;$$
y_{\text{FM}} = w_0 + \sum_{i=1}^n w_i x_i + \sum_{i=1}^n \sum_{j=i+1}^n \langle \mathbf{v}_i, \mathbf{v}_j \rangle x_i x_j
$$&lt;/p&gt;
&lt;p&gt;where $w_0$ is the global bias, $w_i$ is the weight of the $i$-th feature, $\mathbf{v}_i$ and $\mathbf{v}_j$ are
latent vectors for the $i$-th and $j$-th features, respectively, and $x_i$ and $x_j$ are input feature values.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Component&lt;/strong&gt;: This part captures high-order feature interactions through a deep neural network. The deep
component uses embeddings to represent input features and learns complex interactions through multiple hidden layers.
The output of the deep component can be represented as:&lt;/p&gt;
&lt;p&gt;$$\mathbf{h}^L = f^L(\mathbf{W}^L \mathbf{h}^{L-1} + \mathbf{b}^L)$$&lt;/p&gt;
&lt;p&gt;where $\mathbf{h}^L$ is the activation of the $L$-th layer, $\mathbf{W}^L$ and $\mathbf{b}^L$ are the weights and
biases of the $L$-th layer, respectively, and $f^L$ is the activation function.
The final output of the deep component, $ y_{\text{deep}} $, is the output of the last layer of the deep neural network. It is given by:&lt;/p&gt;
&lt;p&gt;$$
y_{\text{deep}} = \mathbf{W}^O \mathbf{h}^{L} + \mathbf{b}^O
$$&lt;/p&gt;
&lt;p&gt;where $ \mathbf{W}^O $ and $ \mathbf{b}^O $ are the weights and bias of the output layer, and $ \mathbf{h}^L $ is the
activation of the last hidden layer.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The final prediction is a combination of the outputs from the FM and deep components:&lt;/p&gt;
&lt;p&gt;$$\hat{y} = \sigma(y_{\text{FM}} + y_{\text{deep}})$$&lt;/p&gt;
&lt;p&gt;where $\sigma$ is the sigmoid function used to squash the output to a probability score.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
   &lt;img src=&#34;http://localhost:1313/recommendation_model/deepfm.png&#34; width=&#34;600&#34; height=&#34;400&#34;&gt;&lt;br&gt;
   &lt;em&gt;Figure 4: deepfm model&lt;/em&gt;
&lt;/p&gt;
&lt;h3 id=&#34;deep-learning-recommendation-model-dlrm6&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#deep-learning-recommendation-model-dlrm6&#34;&gt;
        
    &lt;/a&gt;
    Deep Learning Recommendation Model (DLRM)&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;Deep Learning Recommendation Model (DLRM) is an advanced machine learning framework designed by Facebook AI to tackle
the complex challenge of personalized recommendations at scale. It is particularly suited for large-scale recommendation
systems in environments such as social media platforms, e-commerce, and online advertising. The DLRM combines the
strengths of collaborative filtering and content-based methods by utilizing both dense and sparse features to provide
highly accurate and scalable recommendations.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
   &lt;img src=&#34;http://localhost:1313/recommendation_model/dlrm.png&#34; width=&#34;600&#34; height=&#34;400&#34;&gt;&lt;br&gt;
   &lt;em&gt;Figure 5: deep learning recommendation model&lt;/em&gt;
&lt;/p&gt;
&lt;p&gt;Key components of DLRM includes&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Sparse Features&lt;/strong&gt;: These are categorical variables (e.g., user ID, item ID) which are typically represented using
embeddings. Embeddings transform sparse categorical data into dense vectors of continuous numbers, making them suitable
for neural network processing, denoted as $\mathbf{x}_s[i]$.
The raw sparse features are transformed to sparse embeddings as follows. Let
$$
\mathbf{e}_i = \text{Embedding}(\mathbf{x}_s[i])
$$
where $\mathbf{e}_i$ is the embedding vector for the $i$-th sparse feature, $\text{Embedding}$ denotes an embedding lookup table&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dense Features&lt;/strong&gt;: These are numerical variables (e.g., user age, item price) that are used directly in their raw
form or normalized form, denoted as $\mathbf{x}_d$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Bottom MLP (Multilayer Perceptron)&lt;/strong&gt;: Processes the dense features to capture high-level representations.
$$
\mathbf{h}_d = \text{BottomMLP}(\mathbf{x}_d)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Interaction Layer&lt;/strong&gt;: This layer captures the interactions between different features (both sparse and dense). It
uses a dot product to compute the pairwise interactions among features.
$$
\mathbf{z} = \left[ \mathbf{h}_d, \mathbf{e}_1, \mathbf{e}_2, \ldots, \mathbf{e}_n \right]
$$
where $\mathbf{z}$ is the concatenated vector of dense feature representation and embeddings.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Top MLP&lt;/strong&gt;: Combines the processed dense features and interactions from the interaction layer to make the final prediction.
$$
\hat{y} = \sigma(\text{TopMLP}(\mathbf{z}))
$$
where $\sigma$ is an activation function, typically a sigmoid function for binary classification tasks.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;deep--cross-network-v2-dcn-v27&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#deep--cross-network-v2-dcn-v27&#34;&gt;
        
    &lt;/a&gt;
    Deep &amp;amp; Cross Network v2 (DCN v2)&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;DCN-V2 starts with an embedding layer, followed by a cross network containing multiple cross layers that models explicit
feature interactions, and then combines with a deep network that models implicit feature interactions. The function class
modeled by DCN-V2 is a strict superset of that modeled by DCN. The overall model architecture is depicted in Fig. 5, with
two ways to combine the cross network with the deep network: (1) stacked and (2) parallel.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cross Component&lt;/strong&gt;: The Cross Network V2 enhances the original cross network by introducing a more flexible mechanism
to capture feature interactions. The cross layer in DCN V2 can be represented as:&lt;/p&gt;
&lt;p&gt;$$
x_{l+1} = x_0 \odot (W_l \cdot x_l + b_l) + x_l
$$&lt;/p&gt;
&lt;p&gt;where $x_l$ is the input to the l-th cross layer. $W_l$ and $b_l$ are the weight matrix and bias vector of the l-th
cross layer. $x_{l+1}$ is the output of the $l+1$-th cross layer. $\odot$ is the Hadamard product.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Component&lt;/strong&gt;: The deep network in DCN V2 captures high-order feature interactions through a series of dense layers.
This part of the network learns abstract representations of the input features, enabling the model to generalize well.&lt;/p&gt;
&lt;p&gt;$$
h_{l} = f_{l}(W_{l} h_{l-1} + b_{l})
$$&lt;/p&gt;
&lt;p&gt;where $h_{l}$ is the activation of the $l$-th layer, $W_{l}$ and $b_{l}$ are the weights and
biases of the $l$-th layer, respectively, and $f_{l}$ is the activation function.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The final prediction is a combination of the outputs from either the hidden layer (stacked structure) or the  concatenation
of cross and deep network outputs.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
   &lt;img src=&#34;http://localhost:1313/recommendation_model/dcn_v2.png&#34; width=&#34;600&#34; height=&#34;400&#34;&gt;&lt;br&gt;
   &lt;em&gt;Figure 6: deep cross network v2&lt;/em&gt;
&lt;/p&gt;
&lt;h3 id=&#34;deep-interest-network-din8&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#deep-interest-network-din8&#34;&gt;
        
    &lt;/a&gt;
    Deep Interest Network (DIN)&lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;Deep Interest Network (DIN) is a neural network-based model designed for personalized recommendation systems, particularly
in the context of e-commerce and advertising. Unlike traditional recommendation models that primarily focus on user-item interactions,
DIN leverages a user&amp;rsquo;s historical behavior to make more accurate and contextually relevant recommendations. The key
innovation in DIN is its ability to capture user interests dynamically and use this information to influence the recommendation
process.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
   &lt;img src=&#34;http://localhost:1313/recommendation_model/din.png&#34; width=&#34;800&#34; height=&#34;600&#34;&gt;&lt;br&gt;
   &lt;em&gt;Figure 7: deep interest network&lt;/em&gt;
&lt;/p&gt;
&lt;p&gt;In particular, the DIN contains the following key components:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Embedding Layer&lt;/strong&gt;: User behaviors and items are represented as dense vectors through an embedding layer. Let
$({e_{1}, e_{2}, \ldots, e_{n}})$ be the sequence of embeddings for user behaviors, and $e_{target}$ be the embedding of the target item.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Local Activation Unit&lt;/strong&gt;: This unit applies the attention mechanism to compute the relevance of each user behavior
in the sequence with respect to the target item.
Let
$$
\alpha_{i} = \frac{\exp(\text{score}(e_{i}, e_{target}))}{\sum_{j=1}^{n} \exp(\text{score}(e_{j}, e_{target}))}
$$
where $\text{score}(e_{i}, e_{target})$ is the activation weight output from a feed-forward
network measuring the relevance of behavior $e_{i}$ to the target item.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Interest Extractor Layer&lt;/strong&gt;: Combines the weighted behavior embeddings to form a user interest representation.
$$
u = \sum_{i=1}^{n} \alpha_{i} e_{i}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Prediction Layer&lt;/strong&gt;: The user interest representation is concatenated with the target item embedding and fed into a neural network to predict the user&amp;rsquo;s interaction with the target item.
$$
\hat{y} = \sigma(W[u, e_{target}, e_{profile}, e_{context}] + b)
$$
where $[u, e_{target}, e_{profile}, e_{context}]$ denotes the concatenation of the user interest representation, profile
embedding, context embedding and the target item embedding, $W$ is a weight matrix, $b$ is a bias term, and $\sigma$ is an activation function (e.g., sigmoid).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;multi-gate-mixture-of-experts-mmoe9&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#multi-gate-mixture-of-experts-mmoe9&#34;&gt;
        
    &lt;/a&gt;
    Multi-gate Mixture of Experts (MMOE)&lt;sup id=&#34;fnref:9&#34;&gt;&lt;a href=&#34;#fn:9&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;Multi-gate Mixture-of-Experts (MMoE) is an advanced multi-task learning (MTL) framework designed to model and leverage
task relationships for improved performance across multiple tasks. The MMoE architecture combines the benefits of
mixture-of-experts models with the flexibility of multi-gate mechanisms, allowing the model to dynamically allocate
computational resources based on the specific needs of each task. This approach is particularly useful in scenarios
where tasks are interrelated and can benefit from shared learning while maintaining task-specific adaptations.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
   &lt;img src=&#34;http://localhost:1313/recommendation_model/mmoe.png&#34; width=&#34;800&#34; height=&#34;600&#34;&gt;&lt;br&gt;
   &lt;em&gt;Figure 8: multi-gate mixture of experts&lt;/em&gt;
&lt;/p&gt;
&lt;p&gt;Key components of the model architecture are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Experts&lt;/strong&gt;: A set of neural network sub-models that serve as specialized units for feature extraction. Each expert
is trained to capture different aspects of the input data. Let $\mathbf{x}$ represent the input data, $E$ represent
the number of experts, and $T$ represent the number of tasks. The output from expert is represented as
$$
\mathbf{h}^i = f_e^i(\mathbf{x}), \quad \text{for } i = 1, 2, \ldots, E
$$
where $f_e^i$ is the function of the $i$-th expert, and $\mathbf{h}^i$ is the output of the $i$-th expert.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Multi-gate Mechanism&lt;/strong&gt;: Separate gating networks for each task that dynamically select and weight the contributions
of different experts based on the input data and task requirements.
$$
\mathbf{g}^j = \sigma(\mathbf{W}^j \mathbf{x}), \quad \text{for } j = 1, 2, \ldots, T
$$
where $\mathbf{W}^j$ are the parameters of the gating network for task $j$, and $g^j$ is the gating output distribution
of gating weights to each expert assigned by task $j$. To combine the experts output and gating weights, we have
$$\mathbf{t}^j = \sum_{i=1}^{E} g_i^j \cdot \mathbf{h}^i$$
where $\mathbf{t}^j$ is the combined output for task $j$, and $g_i^j$ is the weight for the $i$-th expert assigned by
the gating network of task $j$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Task-specific Layers&lt;/strong&gt;: Layers that process the combined outputs from the experts, tailored to the specific requirements of each task.
$$
\hat{y}^j = f_o^j(\mathbf{t}^j)
$$
where $f_o^j$ is the task-specific output layer for task $j$, and $\hat{y}^j$ is the predicted output for task $j$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;behavior-sequence-transformer-bst10&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#behavior-sequence-transformer-bst10&#34;&gt;
        
    &lt;/a&gt;
    Behavior Sequence Transformer (BST)&lt;sup id=&#34;fnref:10&#34;&gt;&lt;a href=&#34;#fn:10&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;10&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;The Behavior Sequence Transformer (BST) is a novel neural network architecture designed for modeling user behavior
sequences in recommendation systems. BST leverages the power of Transformer models, which have achieved significant
success in natural language processing (NLP), to capture the sequential patterns and contextual dependencies in user
interactions over time. This approach enhances the ability to predict user preferences and improve recommendation accuracy.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
   &lt;img src=&#34;http://localhost:1313/recommendation_model/bst.png&#34; width=&#34;800&#34; height=&#34;600&#34;&gt;&lt;br&gt;
   &lt;em&gt;Figure 9: behavior sequence transformer&lt;/em&gt;
&lt;/p&gt;
&lt;p&gt;Key components of BST include:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Input Layer&lt;/strong&gt;:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;User Behavior Sequence&lt;/strong&gt;: A sequence of items or actions representing user interactions over time.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Contextual Features&lt;/strong&gt;: Additional information such as timestamps, device types, and other relevant context.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Embedding Layer&lt;/strong&gt;:
$$
\mathbf{E}_x = \mathbf{W}_e \cdot \mathbf{x}
$$
where $\mathbf{W}_e$ is the embedding matrix and $\mathbf{x}$ is the input feature vector.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Positional Encoding&lt;/strong&gt;:
$$
\mathbf{E}_p = \text{PE}(pos)
$$
where $\text{PE}(pos)$ is the positional encoding function that adds position-specific information to the embeddings.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Transformer Encoder&lt;/strong&gt;:
$$
\mathbf{H}_i = \text{LayerNorm}(\text{MultiHeadAttention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) + \mathbf{E}_i)
$$
where $\mathbf{Q}$, $\mathbf{K}$, and $\mathbf{V}$ are the query, key, and value matrices, and $\mathbf{E}_i$ is the
input embedding at layer $i$. A Feed-Forward Networks ($\text{FFN}$) is added on top of the self attention layer to further enhance the
model with non-linearity.
$$
\mathbf{O}_i = \text{LayerNorm}(\text{FFN}(\mathbf{H}_i) + \mathbf{H}_i)
$$
In practice, we usually stack multiple transformer layers. So the output of last layer is thus fed as the input of the
next layer.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Output Layer&lt;/strong&gt;:
$$
\hat{y} = \sigma(\mathbf{W}_o \cdot \mathbf{O}_L)
$$
where $\mathbf{W}_o$ is the weight matrix of the output layer, $\mathbf{O}_L$ is the output of the last Transformer encoder layer, and $\sigma$ is the activation function.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#summary&#34;&gt;
        
    &lt;/a&gt;
    Summary
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;So far, we have summarized the main popular recommendation models. These summaries highlight the unique strengths and
specific applications of each recommendation model, reflecting their advancements and contributions to the field. The
modern recommendation systems usually are built with multiple stages and are composed with both simple and complex models.
Users may choose the best model depending on the business requirements and system architecture design.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#citation&#34;&gt;
        
    &lt;/a&gt;
    Citation
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;If you find this post helpful, please consider citing it as:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bibtex&#34; data-lang=&#34;bibtex&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f3f99d&#34;&gt;@article&lt;/span&gt;{&lt;span style=&#34;color:#ff5c57&#34;&gt;wang2024recommendationmodel&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;author&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;Bing Wang&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;title&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;Recommendation Models&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;journal&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;bingzw.github.io&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;year&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;2024&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;month&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;June&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#57c7ff&#34;&gt;url&lt;/span&gt; = &lt;span style=&#34;color:#5af78e&#34;&gt;&amp;#34;https://bingzw.github.io/posts/2024-06-06-recommendation-model/&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;or&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Bing Wang. (2024, June). Recommendation Models. 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;https://bingzw.github.io/posts/2024-06-06-recommendation-model/
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;reference&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#reference&#34;&gt;
        
    &lt;/a&gt;
    Reference
&lt;/div&gt;
&lt;/h2&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://developers.google.com/machine-learning/recommendation/collaborative/matrix&#34;&gt;Collaborative Filtering and Matrix Factorization&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1708.01715&#34;&gt;Kuchaiev, Oleksii, and Boris Ginsburg. &amp;ldquo;Training deep autoencoders for collaborative filtering.&amp;rdquo; arXiv preprint arXiv:1708.01715 (2017)&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1708.05031&#34;&gt;He, Xiangnan, et al. &amp;ldquo;Neural collaborative filtering.&amp;rdquo; Proceedings of the 26th international conference on world wide web. 2017&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1606.07792&#34;&gt;Cheng, Heng-Tze, et al. &amp;ldquo;Wide &amp;amp; deep learning for recommender systems.&amp;rdquo; Proceedings of the 1st workshop on deep learning for recommender systems. 2016&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1703.04247&#34;&gt;Guo, Huifeng, et al. &amp;ldquo;DeepFM: a factorization-machine based neural network for CTR prediction.&amp;rdquo; arXiv preprint arXiv:1703.04247 (2017)&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1906.00091&#34;&gt;Naumov, Maxim, et al. &amp;ldquo;Deep learning recommendation model for personalization and recommendation systems.&amp;rdquo; arXiv preprint arXiv:1906.00091 (2019)&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2008.13535&#34;&gt;Wang, Ruoxi, et al. &amp;ldquo;Dcn v2: Improved deep &amp;amp; cross network and practical lessons for web-scale learning to rank systems.&amp;rdquo; Proceedings of the web conference 2021. 2021&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1706.06978&#34;&gt;Zhou, Guorui, et al. &amp;ldquo;Deep interest network for click-through rate prediction.&amp;rdquo; Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp;amp; data mining. 2018&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:9&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://dl.acm.org/doi/pdf/10.1145/3219819.3220007&#34;&gt;Ma, Jiaqi, et al. &amp;ldquo;Modeling task relationships in multi-task learning with multi-gate mixture-of-experts.&amp;rdquo; Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp;amp; data mining. 2018&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:9&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:10&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1905.06874&#34;&gt;Chen, Qiwei, et al. &amp;ldquo;Behavior sequence transformer for e-commerce recommendation in alibaba.&amp;rdquo; Proceedings of the 1st international workshop on deep learning practice for high-dimensional sparse data. 2019&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:10&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
    </item>
    
  </channel>
</rss>

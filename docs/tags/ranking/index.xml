<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ranking on Bingz Learning Blog</title>
    <link>/tags/ranking/</link>
    <description>Bingz Learning Blog (Ranking)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <managingEditor>bingwang8878@gamil.com (Bing Wang)</managingEditor>
    <webMaster>bingwang8878@gamil.com (Bing Wang)</webMaster>
    <lastBuildDate>Thu, 06 Jun 2024 22:04:53 -0700</lastBuildDate>
    
    <atom:link href="/tags/ranking/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Recommendation Models</title>
      <link>/posts/2024-06-06-recommendation-model/</link>
      <pubDate>Thu, 06 Jun 2024 22:04:53 -0700</pubDate>
      <author>bingwang8878@gamil.com (Bing Wang)</author>
      <guid>/posts/2024-06-06-recommendation-model/</guid>
      <description>&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://bingzw.github.io/recommendation_model/reco.jpeg&#34; width=&#34;600&#34; height=&#34;400&#34;&gt;&lt;br&gt;
&lt;p&gt;
&lt;p&gt;&lt;em&gt;Image cited from &lt;a href=&#34;https://www.vecteezy.com/&#34;&gt;vecteezy&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction-to-recommendation-models&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#introduction-to-recommendation-models&#34;&gt;
        
    &lt;/a&gt;
    Introduction to Recommendation Models
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;In the digital age, the sheer volume of available content and products can be overwhelming for users. Recommendation
systems play a crucial role in helping users discover relevant items by filtering through vast amounts of data. These
systems are integral to various industries, including e-commerce, streaming services, social media, and online
advertising.&lt;/p&gt;
&lt;p&gt;Recommendation models are algorithms designed to suggest items to users based on various factors such as past behavior,
preferences, and item characteristics. The goal is to enhance user experience by providing personalized recommendations,
thereby increasing engagement and satisfaction.&lt;/p&gt;
&lt;p&gt;In most cases, the goal is trying to find the most appealing item for the user given all user demographic, behavior and
interest information. A well-designed recommendation system usually results in better personalization, user engagement,
and revenue generation. Let&amp;rsquo;s take a look at the most popular modeling approaches.&lt;/p&gt;
&lt;h2 id=&#34;collaborative-filtering-cf&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#collaborative-filtering-cf&#34;&gt;
        
    &lt;/a&gt;
    Collaborative Filtering (CF)
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;Collaborative Filtering is one of the most famous recommendation models so far. It&amp;rsquo;s about to guess the user&amp;rsquo;s interest
based on the behaviors and preferences of other users with similar tastes/characteristics. This basic approach is
straightforward and only requires user-item interaction history.&lt;/p&gt;
&lt;h3 id=&#34;memory-based-cf&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#memory-based-cf&#34;&gt;
        
    &lt;/a&gt;
    Memory Based CF
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;Suppose the recommendation is purely derived from historical records and it does not involve any predictive modelings.
Let&amp;rsquo;s assume a table of all ratings of users on items. Let $r_{u,i}$ denotes the missing rating of user $u$ on item $i$.
$S_u$ be a set of users that share similar characteristics with user $u$. $P_j$ denotes a set of items that are close to
item $j$. The goal is to guess the missing rating $r_{u,i}$. Let&amp;rsquo;s start expressing the &amp;ldquo;predicted&amp;rdquo; rating from either user-user
filtering or item-item filtering.&lt;/p&gt;
&lt;h4 id=&#34;user-user-filtering&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#user-user-filtering&#34;&gt;
        
    &lt;/a&gt;
    User User Filtering
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;Assume that users with a similar profile share a similar taste. We are predicting the missing rating as the weighted
average from ratings of similar users:&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
$r_{u,i} = \bar{r}_u + \frac{\sum_{u&#39;\in S_u}sim(u, u&#39;) * (r_{u&#39;,i} - \bar{r}_{u&#39;})}{\sum_{u&#39;\in S_u}sim(u, u&#39;)}$
&lt;p&gt;
&lt;p&gt;where $\bar{r}_u$ is the average rating of user $u$ and $sim(u, u&amp;rsquo;)$ represents the similarity score between user $u$
and $u&amp;rsquo;$. A common choice is to calculate the cosine similarity between two user rating vectors.&lt;/p&gt;
&lt;h4 id=&#34;item-item-filtering&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#item-item-filtering&#34;&gt;
        
    &lt;/a&gt;
    Item Item Filtering
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;Assume that the customers will prefer products that share a high similarity with those already well appreciated. The
missing rating is thus predicted as the weighted average of a set of similar products:&lt;/p&gt;
&lt;p&gt;$$r_{u,i} = \frac{\sum_{j \in P_i}sim(j, i) * r_{u,j}}{\sum_{j \in P_i}sim(j, i)}$$&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s straightforward to observe that user-based filtering is to check what other users think of the same product, while
item-based filtering is aggregating what the user thinks of other items. Essentially, both are weighted linear
combination of observed ratings. Can we do better than weighted average?&lt;/p&gt;
&lt;h3 id=&#34;model-based-cf&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#model-based-cf&#34;&gt;
        
    &lt;/a&gt;
    Model Based CF
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;Unlike the memory based CF which is trying to fill in the missing cells in the rating matrix, model-based collaborative
filtering is to predict user preferences for items based on past interactions. It often relies on the concept of
latent factors. These are hidden features that influence user preferences and item characteristics. By uncovering these
latent factors, the model can predict the likelihood of a user liking an item.&lt;/p&gt;
&lt;p&gt;Let $R$ denotes the user item interaction rating matrix, $R \in \mathbb{R}^{m*n}$, where $m$ is the dimension of user
space and $n$ denotes the dimension of item space.&lt;/p&gt;
&lt;h4 id=&#34;clustering&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#clustering&#34;&gt;
        
    &lt;/a&gt;
    Clustering
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;Given user or item embeddings, a simple approach is to apply the K nearest neighbours to find the K closest users or
items depending on the similarity metrics used.&lt;/p&gt;
&lt;h4 id=&#34;matrix-factorization&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#matrix-factorization&#34;&gt;
        
    &lt;/a&gt;
    Matrix Factorization
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;The basic idea is to decompose the user-item interaction matrix into two lower-dimensional matrices:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;User Matrix (U)&lt;/strong&gt;: Represents users in terms of latent factors, $U \in \mathbb{R}^{m * p}$, where $p$ is the dimension
of the latent space.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Item Matrix (V)&lt;/strong&gt;: Represents items in terms of latent factors, $V \in \mathbb{R}^{n * p}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The interaction matrix $R$ is approximated as the product of these two matrices: $R \approx U \cdot V^T$. In practice, we
are usually optimizing for a weighted matrix factorization objective&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;$$L(\theta) = \sum_{(i, j)\in obs} \omega_{i, j} (R_{i, j} - U_i \cdot V_{j}^{T})^2 + \omega_{0} \sum_{(i, j) \notin obs}
(U_i \cdot V_{j}^{T})^2$$&lt;/p&gt;
&lt;p&gt;where $\omega_0$ is a hyper-parameter that weights the two terms so that the objective is not
dominated by one or the other, and $\omega_{i, j}$ is a function of the frequency of user $i$ and item $j$.&lt;/p&gt;
&lt;p&gt;Common optimization algorithms to minimize the above objective function includes &lt;strong&gt;stochastic gradient descent&lt;/strong&gt; and
&lt;strong&gt;weighted alternating least squares (WALS)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Recently, a few efforts on deep learning have also been proposed on matrix factorization. For example,&lt;/p&gt;
&lt;h5 id=&#34;deep-auto-encoders2&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#deep-auto-encoders2&#34;&gt;
        
    &lt;/a&gt;
    Deep Auto-Encoders&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h5&gt;
&lt;p&gt;It proposed a model called Collaborative Denoising Auto-Encoder (CDAE) that extends the traditional denoising autoencoder
architecture to the collaborative filtering domain. CDAE is designed to handle implicit feedback, where user preferences
are inferred from user behavior rather than explicit ratings. The model incorporates both user-specific and item-specific
factors, leveraging the rich user interaction data to learn better representations for recommendation tasks.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
    &lt;img src=&#34;https://bingzw.github.io/recommendation_model/autoencoder_mf.png&#34;&gt;&lt;br&gt;
    &lt;em&gt;Figure 1: deep auto-encoder&lt;/em&gt;
&lt;/p&gt;
&lt;h5 id=&#34;neural-collaborative-filtering3&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#neural-collaborative-filtering3&#34;&gt;
        
    &lt;/a&gt;
    Neural Collaborative Filtering&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h5&gt;
&lt;p&gt;NCF replaces these linear latent factor models with non-linear neural networks, allowing for a more expressive
representation of user-item interactions. By doing so, NCF can capture more complex patterns in the data that traditional
methods might miss. The core idea is to use multi-layer perceptrons (MLPs) to model the interaction function between
users and items, providing a more flexible and powerful framework for learning user preferences.&lt;/p&gt;
&lt;p&gt;The general architecture of NCF includes embedding layers for users and items, followed by one or more hidden layers
that learn the interaction between these embeddings. The final output layer predicts the user&amp;rsquo;s preference for a given
item. This approach not only improves the accuracy of recommendations but also enables the integration of additional
features, such as user demographics or item attributes, into the model. Refer to the DLRM or DeepFM for more details.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
    &lt;img src=&#34;https://bingzw.github.io/recommendation_model/ncf_mf.png&#34;&gt;&lt;br&gt;
    &lt;em&gt;Figure 2: neural collaborative filtering&lt;/em&gt;
&lt;/p&gt;
&lt;h2 id=&#34;content-based-models-ranking-as-recommendation&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#content-based-models-ranking-as-recommendation&#34;&gt;
        
    &lt;/a&gt;
    Content Based Models (Ranking As Recommendation)
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;Unlike the collaborative filtering that purely relies on user-item interaction data. Content-based recommendation models
focus on the attributes of items to suggest similar items to users based on their past interactions. These models analyze
the content (such as text, keywords, categories, or features) associated with the items and create a profile for each
user based on the features of the items they have shown interest in. Compared with collaborative filtering, it&amp;rsquo;s easier
to handle the cold start issue when item features are known. However, its recommendation may strictly adheres to the
user&amp;rsquo;s profile, potentially limiting diversity. There are quite a few popular model frameworks in this area and we would
focus on the models based on deep neural architecture.&lt;/p&gt;
&lt;h3 id=&#34;wide--deep4&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#wide--deep4&#34;&gt;
        
    &lt;/a&gt;
    Wide &amp;amp; Deep&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;Wide &amp;amp; Deep learning combined wide linear models and deep neural networks to achieve both memorization and generalization.
The model consists of two components:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Wide Component&lt;/strong&gt;: This part is a generalized linear model (GLM) that excels at memorization by capturing feature
interactions using cross-product feature transformations. The wide component is effective at handling sparse features
and explicitly memorizing frequent co-occurrence patterns. It can be represented as:&lt;/p&gt;
&lt;p&gt;$$y_{\text{wide}} = \mathbf{w}^T \mathbf{x} + b $$&lt;/p&gt;
&lt;p&gt;where $\mathbf{x}$ represents the input features, $\mathbf{w}$ represents the learned weights, and $b$ is
the bias term.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Component&lt;/strong&gt;: This part is a feed-forward neural network that excels at generalization by capturing high-level
and non-linear feature interactions. The deep component uses dense embeddings to represent categorical features, and it
learns implicit interactions through multiple hidden layers. The output of the deep component can be represented as:&lt;/p&gt;
&lt;p&gt;$$\mathbf{h}^L = f^L(\mathbf{W}^L \mathbf{h}^{L-1} + \mathbf{b}^L)$$&lt;/p&gt;
&lt;p&gt;where $\mathbf{h}^L$ is the activation of the $L$-th layer, $\mathbf{W}^L$ and $\mathbf{b}^L$ are the weights and biases
of the $L$-th layer, respectively, and $f^L$ is the activation function.
The final output of the deep component, $ y_{\text{deep}} $, is the output of the last layer of the deep neural network. It is given by:&lt;/p&gt;
&lt;p&gt;$$
y_{\text{deep}} = \mathbf{W}^O \mathbf{h}^{L} + \mathbf{b}^O
$$&lt;/p&gt;
&lt;p&gt;where $ \mathbf{W}^O $ and $ \mathbf{b}^O $ are the weights and bias of the output layer, and $ \mathbf{h}^L $ is the activation of the last hidden layer.&lt;/p&gt;
&lt;p&gt;The final prediction is a weighted sum of the outputs from the wide and deep components:&lt;/p&gt;
&lt;p&gt;$$\hat{y} = \sigma(y_{\text{wide}} + y_{\text{deep}})$$&lt;/p&gt;
&lt;p&gt;where $\sigma$ is the sigmoid function used to squash the output to a probability score.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p align=&#34;center&#34;&gt;
   &lt;img src=&#34;https://bingzw.github.io/recommendation_model/wide_deep.png&#34; width=&#34;800&#34; height=&#34;600&#34;&gt;&lt;br&gt;
   &lt;em&gt;Figure 3: wide &amp; deep model&lt;/em&gt;
&lt;/p&gt;
&lt;h3 id=&#34;deepfm5&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#deepfm5&#34;&gt;
        
    &lt;/a&gt;
    DeepFM&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;DeepFM addresses the challenge of capturing both low-order and high-order feature interactions in recommendation systems.
The model consists of two interconnected components:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;FM Component&lt;/strong&gt;: This part captures low-order feature interactions using Factorization Machines, which are effective
for handling sparse data and modeling pairwise feature interactions without manual feature engineering. The FM component
can be represented as:&lt;/p&gt;
&lt;p&gt;$$
y_{\text{FM}} = w_0 + \sum_{i=1}^n w_i x_i + \sum_{i=1}^n \sum_{j=i+1}^n \langle \mathbf{v}_i, \mathbf{v}_j \rangle x_i x_j
$$&lt;/p&gt;
&lt;p&gt;where $w_0$ is the global bias, $w_i$ is the weight of the $i$-th feature, $\mathbf{v}_i$ and $\mathbf{v}_j$ are
latent vectors for the $i$-th and $j$-th features, respectively, and $x_i$ and $x_j$ are input feature values.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Component&lt;/strong&gt;: This part captures high-order feature interactions through a deep neural network. The deep
component uses embeddings to represent input features and learns complex interactions through multiple hidden layers.
The output of the deep component can be represented as:&lt;/p&gt;
&lt;p&gt;$$\mathbf{h}^L = f^L(\mathbf{W}^L \mathbf{h}^{L-1} + \mathbf{b}^L)$$&lt;/p&gt;
&lt;p&gt;where $\mathbf{h}^L$ is the activation of the $L$-th layer, $\mathbf{W}^L$ and $\mathbf{b}^L$ are the weights and
biases of the $L$-th layer, respectively, and $f^L$ is the activation function.
The final output of the deep component, $ y_{\text{deep}} $, is the output of the last layer of the deep neural network. It is given by:&lt;/p&gt;
&lt;p&gt;$$
y_{\text{deep}} = \mathbf{W}^O \mathbf{h}^{L} + \mathbf{b}^O
$$&lt;/p&gt;
&lt;p&gt;where $ \mathbf{W}^O $ and $ \mathbf{b}^O $ are the weights and bias of the output layer, and $ \mathbf{h}^L $ is the
activation of the last hidden layer.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The final prediction is a combination of the outputs from the FM and deep components:&lt;/p&gt;
&lt;p&gt;$$\hat{y} = \sigma(y_{\text{FM}} + y_{\text{deep}})$$&lt;/p&gt;
&lt;p&gt;where $\sigma$ is the sigmoid function used to squash the output to a probability score.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
   &lt;img src=&#34;https://bingzw.github.io/recommendation_model/deepfm.png&#34; width=&#34;600&#34; height=&#34;400&#34;&gt;&lt;br&gt;
   &lt;em&gt;Figure 4: deepfm model&lt;/em&gt;
&lt;/p&gt;
&lt;h3 id=&#34;deep--cross-network-v2-dcn-v26&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#deep--cross-network-v2-dcn-v26&#34;&gt;
        
    &lt;/a&gt;
    Deep &amp;amp; Cross Network v2 (DCN v2)&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;
&lt;/div&gt;
&lt;/h3&gt;
&lt;p&gt;DCN-V2 starts with an embedding layer, followed by a cross network containing multiple cross layers that models explicit
feature interactions, and then combines with a deep network that models implicit feature interactions. The function class
modeled by DCN-V2 is a strict superset of that modeled by DCN. The overall model architecture is depicted in Fig. 5, with
two ways to combine the cross network with the deep network: (1) stacked and (2) parallel.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cross Component&lt;/strong&gt;: The Cross Network V2 enhances the original cross network by introducing a more flexible mechanism
to capture feature interactions. The cross layer in DCN V2 can be represented as:&lt;/p&gt;
&lt;p&gt;$$
x_{l+1} = x_0 \odot (W_l \cdot x_l + b_l) + x_l
$$&lt;/p&gt;
&lt;p&gt;where $x_l$ is the input to the l-th cross layer. $W_l$ and $b_l$ are the weight matrix and bias vector of the l-th
cross layer. $x_{l+1}$ is the output of the $l+1$-th cross layer. $\odot$ is the Hadamard product.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Component&lt;/strong&gt;: The deep network in DCN V2 captures high-order feature interactions through a series of dense layers.
This part of the network learns abstract representations of the input features, enabling the model to generalize well.&lt;/p&gt;
&lt;p&gt;$$
h_{l} = f_{l}(W_{l} h_{l-1} + b_{l})
$$&lt;/p&gt;
&lt;p&gt;where $h_{l}$ is the activation of the $l$-th layer, $W_{l}$ and $b_{l}$ are the weights and
biases of the $l$-th layer, respectively, and $f_{l}$ is the activation function.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The final prediction is a combination of the outputs from either the hidden layer (stacked structure) or the  concatenation
of cross and deep network outputs.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
   &lt;img src=&#34;https://bingzw.github.io/recommendation_model/dcn_v2.png&#34; width=&#34;600&#34; height=&#34;400&#34;&gt;&lt;br&gt;
   &lt;em&gt;Figure 5: deep cross network v2&lt;/em&gt;
&lt;/p&gt;
&lt;h3 id=&#34;deep-interest-network-din&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#deep-interest-network-din&#34;&gt;
        
    &lt;/a&gt;
    Deep Interest Network (DIN)
&lt;/div&gt;
&lt;/h3&gt;
&lt;h3 id=&#34;deep-learning-recommendation-model-dlrm&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#deep-learning-recommendation-model-dlrm&#34;&gt;
        
    &lt;/a&gt;
    Deep Learning Recommendation Model (DLRM)
&lt;/div&gt;
&lt;/h3&gt;
&lt;h3 id=&#34;entire-space-multi-task-model-esmm&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#entire-space-multi-task-model-esmm&#34;&gt;
        
    &lt;/a&gt;
    Entire Space Multi-Task Model (ESMM)
&lt;/div&gt;
&lt;/h3&gt;
&lt;h3 id=&#34;multi-gate-mixture-of-experts-mmoe&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#multi-gate-mixture-of-experts-mmoe&#34;&gt;
        
    &lt;/a&gt;
    Multi-gate Mixture of Experts (MMOE)
&lt;/div&gt;
&lt;/h3&gt;
&lt;h3 id=&#34;progressive-layered-extraction-ple&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#progressive-layered-extraction-ple&#34;&gt;
        
    &lt;/a&gt;
    Progressive Layered Extraction (PLE)
&lt;/div&gt;
&lt;/h3&gt;
&lt;h3 id=&#34;behavior-sequence-transformer-bst&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#behavior-sequence-transformer-bst&#34;&gt;
        
    &lt;/a&gt;
    Behavior Sequence Transformer (BST)
&lt;/div&gt;
&lt;/h3&gt;
&lt;h2 id=&#34;reference&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#reference&#34;&gt;
        
    &lt;/a&gt;
    Reference
&lt;/div&gt;
&lt;/h2&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://developers.google.com/machine-learning/recommendation/collaborative/matrix&#34;&gt;Collaborative Filtering and Matrix Factorization&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1708.01715&#34;&gt;Kuchaiev, Oleksii, and Boris Ginsburg. &amp;ldquo;Training deep autoencoders for collaborative filtering.&amp;rdquo; arXiv preprint arXiv:1708.01715 (2017)&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1708.05031&#34;&gt;He, Xiangnan, et al. &amp;ldquo;Neural collaborative filtering.&amp;rdquo; Proceedings of the 26th international conference on world wide web. 2017&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1606.07792&#34;&gt;Cheng, Heng-Tze, et al. &amp;ldquo;Wide &amp;amp; deep learning for recommender systems.&amp;rdquo; Proceedings of the 1st workshop on deep learning for recommender systems. 2016&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1703.04247&#34;&gt;Guo, Huifeng, et al. &amp;ldquo;DeepFM: a factorization-machine based neural network for CTR prediction.&amp;rdquo; arXiv preprint arXiv:1703.04247 (2017)&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2008.13535&#34;&gt;Wang, Ruoxi, et al. &amp;ldquo;Dcn v2: Improved deep &amp;amp; cross network and practical lessons for web-scale learning to rank systems.&amp;rdquo; Proceedings of the web conference 2021. 2021&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
    </item>
    
  </channel>
</rss>

<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Reinforcement Learning on Bingz Learning Blog</title>
    <link>/tags/deep-reinforcement-learning/</link>
    <description>Bingz Learning Blog (Deep Reinforcement Learning)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <managingEditor>bingwang8878@gamil.com (Bing Wang)</managingEditor>
    <webMaster>bingwang8878@gamil.com (Bing Wang)</webMaster>
    <lastBuildDate>Mon, 15 Jul 2024 21:40:55 -0700</lastBuildDate>
    
    <atom:link href="/tags/deep-reinforcement-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>A Journey to Reinforcement Learning - Deep RL</title>
      <link>/posts/2024-07-15-rf-learning-deep/</link>
      <pubDate>Mon, 15 Jul 2024 21:40:55 -0700</pubDate>
      <author>bingwang8878@gamil.com (Bing Wang)</author>
      <guid>/posts/2024-07-15-rf-learning-deep/</guid>
      <description>&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;http://localhost:1313/rf/deeprl.png&#34; width=&#34;600&#34; height=&#34;400&#34;&gt;&lt;br&gt;
&lt;p&gt;
&lt;p&gt;This blog is a continuation of the last one: &lt;strong&gt;A Journey to Reinforcement Learning - Tabular Methods&lt;/strong&gt;. The
table of content structure and notations follow the same framework. Let&amp;rsquo;s continue the journey from the last value based
algorithm in model free setting.&lt;/p&gt;
&lt;h2 id=&#34;model-free---when-the-environment-is-unknown&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#model-free---when-the-environment-is-unknown&#34;&gt;
        
    &lt;/a&gt;
    Model Free - when the environment is unknown
&lt;/div&gt;
&lt;/h2&gt;
&lt;h3 id=&#34;value-based-continuing-&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#value-based-continuing-&#34;&gt;
        
    &lt;/a&gt;
    Value Based (Continuing &amp;hellip;)
&lt;/div&gt;
&lt;/h3&gt;
&lt;h4 id=&#34;deep-q-network-dqn&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#deep-q-network-dqn&#34;&gt;
        
    &lt;/a&gt;
    Deep Q Network (DQN)
&lt;/div&gt;
&lt;/h4&gt;
&lt;p&gt;Like what have been discussed in SARSA and $Q$ learning, they may not be a good fit when state or action space is too large
or even continuous. Maintaining a $Q$ table is not feasible in such cases. Instead, we can apply function approximation to
the $Q$ value. So far, the model powerful approach that can approximate almost any shape of function is &lt;strong&gt;Neural Network&lt;/strong&gt;.
Welcome to the realm of &lt;strong&gt;deep reinforcement learning&lt;/strong&gt; (short for &amp;ldquo;deep learning&amp;rdquo; + &amp;ldquo;reinforcement learning&amp;rdquo;).&lt;/p&gt;
&lt;p&gt;The idea seems to be natural. Let&amp;rsquo;s refine the DQN framework step by step. As an extension from $Q$ learning. Let&amp;rsquo;s frame the
problem with &lt;strong&gt;continuous states&lt;/strong&gt; and &lt;strong&gt;discrete actions&lt;/strong&gt; (the continuous actions case will be discussed later). The $Q$ value can
be approximated by fitting a neural network where the state is fed into the network and get output of $Q$ value at each action.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;http://localhost:1313/rf/q_dqn.png&#34; width=&#34;600&#34; height=&#34;400&#34;&gt;&lt;br&gt;
&lt;em&gt;Figure 1: Q learning vs DQN&lt;/em&gt;
&lt;p&gt;
&lt;h3 id=&#34;policy-based&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#policy-based&#34;&gt;
        
    &lt;/a&gt;
    Policy Based
&lt;/div&gt;
&lt;/h3&gt;
&lt;h4 id=&#34;policy-gradient&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#policy-gradient&#34;&gt;
        
    &lt;/a&gt;
    Policy Gradient
&lt;/div&gt;
&lt;/h4&gt;
&lt;h4 id=&#34;trpoppo&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#trpoppo&#34;&gt;
        
    &lt;/a&gt;
    TRPO/PPO
&lt;/div&gt;
&lt;/h4&gt;
&lt;h4 id=&#34;cross-entropy-method-gradient-free&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#cross-entropy-method-gradient-free&#34;&gt;
        
    &lt;/a&gt;
    Cross-Entropy Method [Gradient Free]
&lt;/div&gt;
&lt;/h4&gt;
&lt;h4 id=&#34;evolution-strategy-gradient-free&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#evolution-strategy-gradient-free&#34;&gt;
        
    &lt;/a&gt;
    Evolution Strategy [Gradient Free]
&lt;/div&gt;
&lt;/h4&gt;
&lt;h3 id=&#34;hybrid&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#hybrid&#34;&gt;
        
    &lt;/a&gt;
    Hybrid
&lt;/div&gt;
&lt;/h3&gt;
&lt;h4 id=&#34;ddpq&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#ddpq&#34;&gt;
        
    &lt;/a&gt;
    DDPQ
&lt;/div&gt;
&lt;/h4&gt;
&lt;h4 id=&#34;actor-critic-ac&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#actor-critic-ac&#34;&gt;
        
    &lt;/a&gt;
    Actor Critic (AC)
&lt;/div&gt;
&lt;/h4&gt;
&lt;h4 id=&#34;sac&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#sac&#34;&gt;
        
    &lt;/a&gt;
    SAC
&lt;/div&gt;
&lt;/h4&gt;</description>
    </item>
    
  </channel>
</rss>

<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Reinforcement Learning on Bingz Learning Blog</title>
    <link>/tags/reinforcement-learning/</link>
    <description>Bingz Learning Blog (Reinforcement Learning)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <managingEditor>bingwang8878@gamil.com (Bing Wang)</managingEditor>
    <webMaster>bingwang8878@gamil.com (Bing Wang)</webMaster>
    <lastBuildDate>Fri, 05 Jul 2024 20:59:39 -0700</lastBuildDate>
    
    <atom:link href="/tags/reinforcement-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>A Journey to Reinforcement Learning</title>
      <link>/posts/2024-07-05-rf-learning/</link>
      <pubDate>Fri, 05 Jul 2024 20:59:39 -0700</pubDate>
      <author>bingwang8878@gamil.com (Bing Wang)</author>
      <guid>/posts/2024-07-05-rf-learning/</guid>
      <description>&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://bingzw.github.io/rf/rf.png&#34; width=&#34;600&#34; height=&#34;400&#34;&gt;&lt;br&gt;
&lt;em&gt;Figure 1: basic RL model&lt;/em&gt;
&lt;p&gt;
&lt;p&gt;&lt;em&gt;Image cited from &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;basic-problem-statement-of-reinforcement-learning&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#basic-problem-statement-of-reinforcement-learning&#34;&gt;
        
    &lt;/a&gt;
    Basic Problem Statement of Reinforcement Learning
&lt;/div&gt;
&lt;/h2&gt;
&lt;p&gt;Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in
an environment to maximize cumulative reward. Unlike supervised learning, where the model is trained on a fixed dataset,
RL involves learning through interaction with the environment. Let&amp;rsquo;s define some basic elements in RL domain.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Agent&lt;/strong&gt;: The learner or decision maker.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Environment&lt;/strong&gt;: The external system the agent interacts with.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;State&lt;/strong&gt; $S$: A representation of the current situation of the agent.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Action&lt;/strong&gt; $A$: The set of all possible moves the agent can make.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reward&lt;/strong&gt; $R$: A feedback signal from the environment to evaluate the agent&amp;rsquo;s action.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Policy $\pi$&lt;/strong&gt;: A strategy used by the agent to determine the next action based on the current state. It&amp;rsquo;s usually
a probability function $\pi: S\times A$ -&amp;gt; $[0, 1]$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Value Function $V(s)$&lt;/strong&gt;: A function that estimates the expected cumulative reward from a given state.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Q-Function $Q(s, a)$&lt;/strong&gt;: A function that estimates the expected cumulative reward from a given state-action pair.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As it can be seen in &lt;em&gt;Figure 1&lt;/em&gt;, the general workflow of RL involves&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Observe the state $s_t$ (and reward $r_t$), the state can be any representation of the current situation or context in which the agent operates.
Note that the state space $S$ can be either &lt;strong&gt;finite&lt;/strong&gt; or &lt;strong&gt;infinite&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Based on the current policy $\pi$, the agent selects an action $a_t \in A$ to perform. The selection can be &lt;strong&gt;deterministic&lt;/strong&gt;
or &lt;strong&gt;stochastic&lt;/strong&gt; depending on the policy.&lt;/li&gt;
&lt;li&gt;The agent performs the selected action $a_t$ in the environment (can also be either &lt;strong&gt;known&lt;/strong&gt; or &lt;strong&gt;unknown&lt;/strong&gt;).&lt;/li&gt;
&lt;li&gt;After performing the action, the agent receives a reward $r_{t+1}$ from the environment and observes the next state $s_{t+1}$.&lt;/li&gt;
&lt;li&gt;The agent updates its &lt;strong&gt;policy&lt;/strong&gt; $\pi(a_t|s_t)$ and &lt;strong&gt;value functions&lt;/strong&gt; $V(s_t)$ or $Q(s_t, a_t)$ based on the observed reward $r_t$
and next state $s_{t+1}$. The update rule varies depending on the RL algorithm used. (Note that the policy learned in step 5
can be either the &lt;strong&gt;same (on policy)&lt;/strong&gt; or &lt;strong&gt;different (off policy)&lt;/strong&gt; with the ones in step 2)&lt;/li&gt;
&lt;li&gt;The agent repeats again from step 1 and continues this iterative process until the policy converges, meaning it has
learned an optimal or near-optimal policy that maximizes cumulative rewards over time.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Reinforcement Learning (RL) derives its name from the concept of &amp;ldquo;reinforcement&amp;rdquo; in behavioral psychology, where learning
occurs through rewards and punishments. The agent learns to make decisions by receiving feedback in the form of rewards or
penalties. Positive outcomes reinforce the actions that led to them, strengthening the behavior. The learning process is kind
of a process of &lt;strong&gt;&amp;ldquo;Trial and Error&amp;rdquo;&lt;/strong&gt;, where the agent explores different actions to discover which ones yield the highest rewards.
Long-term beneficial actions are reinforced through repeated positive outcomes.&lt;/p&gt;
&lt;p&gt;Now let&amp;rsquo;s start with the most influential and fundamental RL model - Markov Decision Process (MDP). Our fantastic journey begins here.&lt;/p&gt;
&lt;h2 id=&#34;model-based---when-the-environment-is-given&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#model-based---when-the-environment-is-given&#34;&gt;
        
    &lt;/a&gt;
    Model Based - When the environment is given
&lt;/div&gt;
&lt;/h2&gt;
&lt;h3 id=&#34;dynamic-programming&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#dynamic-programming&#34;&gt;
        
    &lt;/a&gt;
    Dynamic Programming
&lt;/div&gt;
&lt;/h3&gt;
&lt;h2 id=&#34;model-free---when-the-environment-is-unknown&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#model-free---when-the-environment-is-unknown&#34;&gt;
        
    &lt;/a&gt;
    Model Free - When the environment is unknown
&lt;/div&gt;
&lt;/h2&gt;
&lt;h2 id=&#34;value-based&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#value-based&#34;&gt;
        
    &lt;/a&gt;
    Value Based
&lt;/div&gt;
&lt;/h2&gt;
&lt;h3 id=&#34;on-policy&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#on-policy&#34;&gt;
        
    &lt;/a&gt;
    On-Policy
&lt;/div&gt;
&lt;/h3&gt;
&lt;h4 id=&#34;sarsa&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#sarsa&#34;&gt;
        
    &lt;/a&gt;
    SARSA
&lt;/div&gt;
&lt;/h4&gt;
&lt;h3 id=&#34;off-policy&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#off-policy&#34;&gt;
        
    &lt;/a&gt;
    Off-Policy
&lt;/div&gt;
&lt;/h3&gt;
&lt;h4 id=&#34;q-learning&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#q-learning&#34;&gt;
        
    &lt;/a&gt;
    Q-Learning
&lt;/div&gt;
&lt;/h4&gt;
&lt;h4 id=&#34;deep-q-network-dqn&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#deep-q-network-dqn&#34;&gt;
        
    &lt;/a&gt;
    Deep Q Network (DQN)
&lt;/div&gt;
&lt;/h4&gt;
&lt;h2 id=&#34;policy-based&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#policy-based&#34;&gt;
        
    &lt;/a&gt;
    Policy Based
&lt;/div&gt;
&lt;/h2&gt;
&lt;h3 id=&#34;gradient-based&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#gradient-based&#34;&gt;
        
    &lt;/a&gt;
    Gradient Based
&lt;/div&gt;
&lt;/h3&gt;
&lt;h4 id=&#34;policy-gradient&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#policy-gradient&#34;&gt;
        
    &lt;/a&gt;
    Policy Gradient
&lt;/div&gt;
&lt;/h4&gt;
&lt;h4 id=&#34;trpoppo&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#trpoppo&#34;&gt;
        
    &lt;/a&gt;
    TRPO/PPO
&lt;/div&gt;
&lt;/h4&gt;
&lt;h4 id=&#34;actor-critic-ac&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#actor-critic-ac&#34;&gt;
        
    &lt;/a&gt;
    Actor Critic (AC)
&lt;/div&gt;
&lt;/h4&gt;
&lt;h3 id=&#34;gradient-free&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#gradient-free&#34;&gt;
        
    &lt;/a&gt;
    Gradient Free
&lt;/div&gt;
&lt;/h3&gt;
&lt;h4 id=&#34;cross-entropy-method&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#cross-entropy-method&#34;&gt;
        
    &lt;/a&gt;
    Cross-Entropy Method
&lt;/div&gt;
&lt;/h4&gt;
&lt;h4 id=&#34;evolution-strategy&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#evolution-strategy&#34;&gt;
        
    &lt;/a&gt;
    Evolution Strategy
&lt;/div&gt;
&lt;/h4&gt;
&lt;h2 id=&#34;summary&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#summary&#34;&gt;
        
    &lt;/a&gt;
    Summary
&lt;/div&gt;
&lt;/h2&gt;
&lt;h2 id=&#34;citation&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#citation&#34;&gt;
        
    &lt;/a&gt;
    Citation
&lt;/div&gt;
&lt;/h2&gt;
&lt;h2 id=&#34;reference&#34; &gt;
&lt;div&gt;
    &lt;a href=&#34;#reference&#34;&gt;
        
    &lt;/a&gt;
    Reference
&lt;/div&gt;
&lt;/h2&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.mdpi.com/2076-3417/13/4/2443&#34;&gt;Souchleris, Konstantinos, George K. Sidiropoulos, and George A. Papakostas. &amp;ldquo;Reinforcement learning in game industryâ€”review, prospects and challenges.&amp;rdquo; Applied Sciences 13.4 (2023): 2443&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
    </item>
    
  </channel>
</rss>
